
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import itertools

import torchvision
import torchvision.transforms as transforms
import seaborn as sns
import matplotlib.pyplot as plt
import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch.backends.cudnn as cudnn
from sklearn.metrics import f1_score

#Training set

file_path_1 = '/content/drive/MyDrive/Ν124/music_genre_data_di/train/mfccs/X.npy'
file_path_2 = '/content/drive/MyDrive/Ν124/music_genre_data_di/train/mfccs/labels.npy'
x_tr = np.load(file_path_1)
y_tr = np.load(file_path_2)

#Validation set

file_path_3 = '/content/drive/MyDrive/Ν124/music_genre_data_di/val/mfccs/X.npy'
file_path_4 = '/content/drive/MyDrive/Ν124/music_genre_data_di/val/mfccs/labels.npy'
x_val = np.load(file_path_3)
y_val= np.load(file_path_4)

#Test set
file_path_5 = '/content/drive/MyDrive/Ν124/music_genre_data_di/test/mfccs/X.npy'
file_path_6 = '/content/drive/MyDrive/Ν124/music_genre_data_di/test/mfccs/labels.npy'
x_test = np.load(file_path_5)
y_test = np.load(file_path_6)

from sklearn.preprocessing import LabelEncoder

#encoding
label_encoder = LabelEncoder()

y_tr_en = label_encoder.fit_transform(y_tr)
y_val_en = label_encoder.transform(y_val)
y_test_en = label_encoder.transform(y_test)

#checking if there is something miss labeled in the original set
print("Original labels:", np.unique(y_tr))

#see the matching labels to their encodings
print("Mapping of classes to integers:")
for class_name, class_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):
    print(f"{class_name} : {class_label}")
Original labels: ['blues' 'classical' 'hiphop' 'rock_metal_hardrock']
Mapping of classes to integers:
blues : 0
classical : 1
hiphop : 2
rock_metal_hardrock : 3

from torch.utils.data import TensorDataset, DataLoader


# np.arrays to PyTorch tensors
x_tr_tensor = torch.Tensor(x_tr)
y_tr_tensor = torch.LongTensor(y_tr_en)
x_val_tensor = torch.Tensor(x_val)
y_val_tensor = torch.LongTensor(y_val_en)
x_test_tensor = torch.Tensor(x_test)
y_test_tensor = torch.LongTensor(y_test_en)


# Create TensorDataset for each data group
train_dataset = TensorDataset(x_tr_tensor, y_tr_tensor)
val_dataset = TensorDataset(x_val_tensor, y_val_tensor)
test_dataset = TensorDataset(x_test_tensor, y_test_tensor)

#Data loading
# Create DataLoader for each data group
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)




class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(26, 128),
            nn.ReLU(),
            nn.Linear(128, 32),
            nn.ReLU(),
            nn.Linear(32, 4),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

# utilize the GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = NeuralNetwork().to(device)

# print the model structure
print(model)

NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=26, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=32, bias=True)
    (3): ReLU()
    (4): Linear(in_features=32, out_features=4, bias=True)
  )
)


def train_loop(dataloader, model, loss_fn, optimizer, num_epochs):
    for epoch in range(num_epochs):
        for batch, (X, y) in enumerate(dataloader):

            # in case of using gpu
            X = X.to(device)
            y = y.to(device)

            # Compute prediction and loss
            pred = model(X)
            loss = loss_fn(pred, y)

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Print loss for every 100 batches
            if batch % 100 == 0:
                loss_value = loss.item()
                current = batch * len(X)
                print(f"loss: {loss_value:>7f}  [{current:>5d}/{len(dataloader.dataset):>5d}]")




def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    test_loss, correct = 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for X, y in dataloader:
            X = X.to(device)
            y = y.to(device)

            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

            all_preds.extend(pred.argmax(1).tolist())
            all_labels.extend(y.tolist())

    test_loss /= size
    correct /= size
    accuracy = accuracy_score(all_labels, all_preds)
    f1_macro = f1_score(all_labels, all_preds, average='macro')
    confusion_mat = confusion_matrix(all_labels, all_preds)

    return test_loss, f1_macro, accuracy, confusion_mat
Βήμα 5: Εκπαίδευση δικτύου

Εκπαιδεύστε το νευρωνικό δίκτυο στο training set χρησιμοποιώντας τα εξής:

• optimizer: stochastic gradient descent

• learning rate: 0.002

• loss function: cross-entropy loss

• αριθμός εποχών: 30


import time
# Optimizer, learning rate, loss function, and number of epochs
optimizer = torch.optim.SGD(model.parameters(), lr=0.002)
loss_fn = nn.CrossEntropyLoss()
num_epochs = 30

loss_values = []
accuracy_values = []
f1_values = []


# Start the timer
start_time = time.time()

# Train the model
for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}\n-------------------------------")



    train_loop(train_loader, model, loss_fn, optimizer, num_epochs)

    # Evaluate the model on the test set
    test_loss, test_f1, test_acc, test_cm = test_loop(test_loader, model, loss_fn)

    # Append the values to the respective lists
    loss_values.append(test_loss)
    accuracy_values.append(test_acc)
    f1_values.append(test_f1)



    # Print the evaluation scores
    print("Test Set Evaluation:")
    print(f"Loss: {test_loss:.4f}")
    print(f"F1 Score (macro-averaged): {test_f1:.4f}")
    print(f"Accuracy: {100*test_acc:.4f}")
    print("Confusion Matrix:")
    print(test_cm)

# Stop the timer
end_time = time.time()
epoch_time = end_time - start_time
print(f"Time taken for epoch: {epoch_time:.2f} seconds")
Epoch 1
-------------------------------
loss: 1.383125  [    0/ 3200]
loss: 1.393098  [ 1600/ 3200]
loss: 1.411556  [    0/ 3200]
loss: 1.374131  [ 1600/ 3200]
loss: 1.359613  [    0/ 3200]
loss: 1.371377  [ 1600/ 3200]
loss: 1.358528  [    0/ 3200]
loss: 1.363453  [ 1600/ 3200]
loss: 1.315819  [    0/ 3200]
loss: 1.382683  [ 1600/ 3200]
loss: 1.408198  [    0/ 3200]
loss: 1.359016  [ 1600/ 3200]
loss: 1.316400  [    0/ 3200]
loss: 1.377565  [ 1600/ 3200]
loss: 1.387827  [    0/ 3200]
loss: 1.350775  [ 1600/ 3200]
loss: 1.279317  [    0/ 3200]
loss: 1.294737  [ 1600/ 3200]
loss: 1.382262  [    0/ 3200]
loss: 1.277583  [ 1600/ 3200]
loss: 1.260538  [    0/ 3200]
loss: 1.311363  [ 1600/ 3200]
loss: 1.307195  [    0/ 3200]
loss: 1.261264  [ 1600/ 3200]
loss: 1.308394  [    0/ 3200]
loss: 1.173208  [ 1600/ 3200]
loss: 1.294776  [    0/ 3200]
loss: 1.232319  [ 1600/ 3200]
loss: 1.208732  [    0/ 3200]
loss: 1.096234  [ 1600/ 3200]
loss: 1.119831  [    0/ 3200]
loss: 1.200876  [ 1600/ 3200]
loss: 1.167629  [    0/ 3200]
loss: 1.143468  [ 1600/ 3200]
loss: 1.069971  [    0/ 3200]
loss: 1.172985  [ 1600/ 3200]
loss: 1.305326  [    0/ 3200]
loss: 1.067416  [ 1600/ 3200]
loss: 1.160119  [    0/ 3200]
loss: 1.089487  [ 1600/ 3200]
loss: 0.988998  [    0/ 3200]
loss: 1.144742  [ 1600/ 3200]
loss: 1.198303  [    0/ 3200]
loss: 1.130144  [ 1600/ 3200]
loss: 1.122703  [    0/ 3200]
loss: 1.053468  [ 1600/ 3200]
loss: 1.067148  [    0/ 3200]
loss: 1.068746  [ 1600/ 3200]
loss: 1.161429  [    0/ 3200]
loss: 1.280578  [ 1600/ 3200]
loss: 0.972330  [    0/ 3200]
loss: 1.080833  [ 1600/ 3200]
loss: 0.848568  [    0/ 3200]
loss: 1.073638  [ 1600/ 3200]
loss: 1.077413  [    0/ 3200]
loss: 0.930756  [ 1600/ 3200]
loss: 0.989890  [    0/ 3200]
loss: 0.998338  [ 1600/ 3200]
loss: 0.803622  [    0/ 3200]
loss: 1.025630  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0653
F1 Score (macro-averaged): 0.5789
Accuracy: 59.4477
Confusion Matrix:
[[ 95  60  84  85]
 [ 23 261   6   7]
 [ 75  35 221  25]
 [ 34  77  47 241]]
Epoch 2
-------------------------------
loss: 0.808068  [    0/ 3200]
loss: 0.883683  [ 1600/ 3200]
loss: 0.875904  [    0/ 3200]
loss: 1.137445  [ 1600/ 3200]
loss: 0.886430  [    0/ 3200]
loss: 1.074494  [ 1600/ 3200]
loss: 1.066512  [    0/ 3200]
loss: 0.842722  [ 1600/ 3200]
loss: 0.877314  [    0/ 3200]
loss: 0.768358  [ 1600/ 3200]
loss: 1.009653  [    0/ 3200]
loss: 0.984065  [ 1600/ 3200]
loss: 0.972271  [    0/ 3200]
loss: 1.006457  [ 1600/ 3200]
loss: 1.040008  [    0/ 3200]
loss: 0.941885  [ 1600/ 3200]
loss: 0.993942  [    0/ 3200]
loss: 0.835099  [ 1600/ 3200]
loss: 0.741414  [    0/ 3200]
loss: 0.911585  [ 1600/ 3200]
loss: 1.013776  [    0/ 3200]
loss: 0.950149  [ 1600/ 3200]
loss: 1.113071  [    0/ 3200]
loss: 0.970279  [ 1600/ 3200]
loss: 1.350189  [    0/ 3200]
loss: 0.686789  [ 1600/ 3200]
loss: 1.009153  [    0/ 3200]
loss: 0.582782  [ 1600/ 3200]
loss: 0.919257  [    0/ 3200]
loss: 0.915049  [ 1600/ 3200]
loss: 0.812287  [    0/ 3200]
loss: 0.950367  [ 1600/ 3200]
loss: 0.743068  [    0/ 3200]
loss: 1.009746  [ 1600/ 3200]
loss: 1.103834  [    0/ 3200]
loss: 0.966611  [ 1600/ 3200]
loss: 0.903586  [    0/ 3200]
loss: 0.876377  [ 1600/ 3200]
loss: 0.992524  [    0/ 3200]
loss: 1.040653  [ 1600/ 3200]
loss: 0.973698  [    0/ 3200]
loss: 1.106553  [ 1600/ 3200]
loss: 0.663537  [    0/ 3200]
loss: 0.853239  [ 1600/ 3200]
loss: 0.818150  [    0/ 3200]
loss: 0.812904  [ 1600/ 3200]
loss: 1.042703  [    0/ 3200]
loss: 1.062415  [ 1600/ 3200]
loss: 1.243046  [    0/ 3200]
loss: 0.988786  [ 1600/ 3200]
loss: 0.662417  [    0/ 3200]
loss: 0.734598  [ 1600/ 3200]
loss: 1.110810  [    0/ 3200]
loss: 0.729913  [ 1600/ 3200]
loss: 0.926621  [    0/ 3200]
loss: 1.127610  [ 1600/ 3200]
loss: 1.012651  [    0/ 3200]
loss: 0.942887  [ 1600/ 3200]
loss: 1.070140  [    0/ 3200]
loss: 0.560756  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0591
F1 Score (macro-averaged): 0.5984
Accuracy: 61.4826
Confusion Matrix:
[[ 92  56  59 117]
 [ 15 263   5  14]
 [ 84  22 217  33]
 [ 35  59  31 274]]
Epoch 3
-------------------------------
loss: 1.063620  [    0/ 3200]
loss: 0.664568  [ 1600/ 3200]
loss: 0.990107  [    0/ 3200]
loss: 1.139882  [ 1600/ 3200]
loss: 0.798069  [    0/ 3200]
loss: 0.965406  [ 1600/ 3200]
loss: 0.868238  [    0/ 3200]
loss: 1.064452  [ 1600/ 3200]
loss: 0.911248  [    0/ 3200]
loss: 0.672157  [ 1600/ 3200]
loss: 0.661824  [    0/ 3200]
loss: 0.946544  [ 1600/ 3200]
loss: 1.171396  [    0/ 3200]
loss: 0.835306  [ 1600/ 3200]
loss: 0.825472  [    0/ 3200]
loss: 1.062556  [ 1600/ 3200]
loss: 0.541383  [    0/ 3200]
loss: 0.899426  [ 1600/ 3200]
loss: 0.879047  [    0/ 3200]
loss: 0.877302  [ 1600/ 3200]
loss: 1.276610  [    0/ 3200]
loss: 0.774946  [ 1600/ 3200]
loss: 0.891121  [    0/ 3200]
loss: 1.066384  [ 1600/ 3200]
loss: 1.071261  [    0/ 3200]
loss: 0.897994  [ 1600/ 3200]
loss: 1.020653  [    0/ 3200]
loss: 0.810748  [ 1600/ 3200]
loss: 0.740102  [    0/ 3200]
loss: 0.661816  [ 1600/ 3200]
loss: 0.840421  [    0/ 3200]
loss: 0.826027  [ 1600/ 3200]
loss: 0.639224  [    0/ 3200]
loss: 0.912766  [ 1600/ 3200]
loss: 0.592317  [    0/ 3200]
loss: 0.977866  [ 1600/ 3200]
loss: 0.902799  [    0/ 3200]
loss: 1.104066  [ 1600/ 3200]
loss: 0.871212  [    0/ 3200]
loss: 0.744346  [ 1600/ 3200]
loss: 1.130876  [    0/ 3200]
loss: 0.685683  [ 1600/ 3200]
loss: 0.460365  [    0/ 3200]
loss: 0.850860  [ 1600/ 3200]
loss: 0.992683  [    0/ 3200]
loss: 0.850154  [ 1600/ 3200]
loss: 1.230797  [    0/ 3200]
loss: 0.857332  [ 1600/ 3200]
loss: 0.716442  [    0/ 3200]
loss: 1.014480  [ 1600/ 3200]
loss: 0.657108  [    0/ 3200]
loss: 1.091167  [ 1600/ 3200]
loss: 0.305570  [    0/ 3200]
loss: 0.762380  [ 1600/ 3200]
loss: 0.821110  [    0/ 3200]
loss: 0.569183  [ 1600/ 3200]
loss: 1.099923  [    0/ 3200]
loss: 0.899228  [ 1600/ 3200]
loss: 1.008973  [    0/ 3200]
loss: 0.851956  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0577
F1 Score (macro-averaged): 0.6158
Accuracy: 62.3547
Confusion Matrix:
[[114  45  47 118]
 [ 18 257   2  20]
 [100  14 206  36]
 [ 44  44  30 281]]
Epoch 4
-------------------------------
loss: 0.906236  [    0/ 3200]
loss: 0.769918  [ 1600/ 3200]
loss: 0.769631  [    0/ 3200]
loss: 0.904640  [ 1600/ 3200]
loss: 0.796764  [    0/ 3200]
loss: 0.721893  [ 1600/ 3200]
loss: 0.752278  [    0/ 3200]
loss: 0.903727  [ 1600/ 3200]
loss: 0.617745  [    0/ 3200]
loss: 0.706982  [ 1600/ 3200]
loss: 1.208054  [    0/ 3200]
loss: 1.026088  [ 1600/ 3200]
loss: 0.666943  [    0/ 3200]
loss: 0.885563  [ 1600/ 3200]
loss: 1.500613  [    0/ 3200]
loss: 0.773120  [ 1600/ 3200]
loss: 0.865992  [    0/ 3200]
loss: 0.972429  [ 1600/ 3200]
loss: 0.968785  [    0/ 3200]
loss: 0.877006  [ 1600/ 3200]
loss: 0.684203  [    0/ 3200]
loss: 0.918843  [ 1600/ 3200]
loss: 0.718297  [    0/ 3200]
loss: 0.536285  [ 1600/ 3200]
loss: 0.787097  [    0/ 3200]
loss: 0.975350  [ 1600/ 3200]
loss: 1.052860  [    0/ 3200]
loss: 0.653413  [ 1600/ 3200]
loss: 0.729350  [    0/ 3200]
loss: 0.799620  [ 1600/ 3200]
loss: 0.728089  [    0/ 3200]
loss: 0.880521  [ 1600/ 3200]
loss: 1.035154  [    0/ 3200]
loss: 0.992520  [ 1600/ 3200]
loss: 0.734080  [    0/ 3200]
loss: 0.841159  [ 1600/ 3200]
loss: 0.785794  [    0/ 3200]
loss: 1.006286  [ 1600/ 3200]
loss: 0.928323  [    0/ 3200]
loss: 0.849605  [ 1600/ 3200]
loss: 0.734887  [    0/ 3200]
loss: 0.627653  [ 1600/ 3200]
loss: 0.920456  [    0/ 3200]
loss: 0.844564  [ 1600/ 3200]
loss: 0.571608  [    0/ 3200]
loss: 1.055310  [ 1600/ 3200]
loss: 0.720558  [    0/ 3200]
loss: 0.723001  [ 1600/ 3200]
loss: 1.030585  [    0/ 3200]
loss: 0.971473  [ 1600/ 3200]
loss: 0.513499  [    0/ 3200]
loss: 1.145059  [ 1600/ 3200]
loss: 0.688217  [    0/ 3200]
loss: 0.741312  [ 1600/ 3200]
loss: 0.905982  [    0/ 3200]
loss: 1.017064  [ 1600/ 3200]
loss: 0.833985  [    0/ 3200]
loss: 0.623581  [ 1600/ 3200]
loss: 0.900907  [    0/ 3200]
loss: 0.697094  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0571
F1 Score (macro-averaged): 0.5978
Accuracy: 63.5174
Confusion Matrix:
[[ 51  10 112 151]
 [ 37 195  13  52]
 [ 17   7 304  28]
 [ 18   7  50 324]]
Epoch 5
-------------------------------
loss: 0.924389  [    0/ 3200]
loss: 0.773535  [ 1600/ 3200]
loss: 0.687500  [    0/ 3200]
loss: 0.894106  [ 1600/ 3200]
loss: 0.857707  [    0/ 3200]
loss: 0.808457  [ 1600/ 3200]
loss: 0.414004  [    0/ 3200]
loss: 0.746580  [ 1600/ 3200]
loss: 0.805955  [    0/ 3200]
loss: 0.806139  [ 1600/ 3200]
loss: 1.201345  [    0/ 3200]
loss: 1.038397  [ 1600/ 3200]
loss: 0.873929  [    0/ 3200]
loss: 0.586553  [ 1600/ 3200]
loss: 0.892407  [    0/ 3200]
loss: 0.537198  [ 1600/ 3200]
loss: 0.860572  [    0/ 3200]
loss: 0.576410  [ 1600/ 3200]
loss: 0.803432  [    0/ 3200]
loss: 0.770828  [ 1600/ 3200]
loss: 0.821457  [    0/ 3200]
loss: 0.858323  [ 1600/ 3200]
loss: 0.939448  [    0/ 3200]
loss: 1.133952  [ 1600/ 3200]
loss: 0.749101  [    0/ 3200]
loss: 1.060753  [ 1600/ 3200]
loss: 0.966901  [    0/ 3200]
loss: 1.024330  [ 1600/ 3200]
loss: 0.553999  [    0/ 3200]
loss: 0.712376  [ 1600/ 3200]
loss: 0.646043  [    0/ 3200]
loss: 0.523389  [ 1600/ 3200]
loss: 1.107428  [    0/ 3200]
loss: 0.702020  [ 1600/ 3200]
loss: 0.807009  [    0/ 3200]
loss: 0.726346  [ 1600/ 3200]
loss: 0.969370  [    0/ 3200]
loss: 0.897613  [ 1600/ 3200]
loss: 0.824162  [    0/ 3200]
loss: 0.927424  [ 1600/ 3200]
loss: 0.879712  [    0/ 3200]
loss: 0.827271  [ 1600/ 3200]
loss: 0.841405  [    0/ 3200]
loss: 0.732904  [ 1600/ 3200]
loss: 0.968186  [    0/ 3200]
loss: 0.883515  [ 1600/ 3200]
loss: 0.541903  [    0/ 3200]
loss: 0.759917  [ 1600/ 3200]
loss: 0.628093  [    0/ 3200]
loss: 0.869987  [ 1600/ 3200]
loss: 0.835267  [    0/ 3200]
loss: 0.665896  [ 1600/ 3200]
loss: 0.975204  [    0/ 3200]
loss: 0.919417  [ 1600/ 3200]
loss: 0.777671  [    0/ 3200]
loss: 0.901848  [ 1600/ 3200]
loss: 1.113423  [    0/ 3200]
loss: 0.652971  [ 1600/ 3200]
loss: 0.948223  [    0/ 3200]
loss: 0.743751  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0573
F1 Score (macro-averaged): 0.6107
Accuracy: 63.5174
Confusion Matrix:
[[ 70  10 153  91]
 [ 40 210  19  28]
 [  9   7 332   8]
 [ 42  10  85 262]]
Epoch 6
-------------------------------
loss: 0.596726  [    0/ 3200]
loss: 0.783186  [ 1600/ 3200]
loss: 0.709228  [    0/ 3200]
loss: 1.098461  [ 1600/ 3200]
loss: 0.867610  [    0/ 3200]
loss: 0.775213  [ 1600/ 3200]
loss: 0.708693  [    0/ 3200]
loss: 0.656598  [ 1600/ 3200]
loss: 1.067891  [    0/ 3200]
loss: 0.977493  [ 1600/ 3200]
loss: 1.126882  [    0/ 3200]
loss: 0.853929  [ 1600/ 3200]
loss: 0.829117  [    0/ 3200]
loss: 0.776089  [ 1600/ 3200]
loss: 0.664802  [    0/ 3200]
loss: 0.800232  [ 1600/ 3200]
loss: 0.909715  [    0/ 3200]
loss: 1.116849  [ 1600/ 3200]
loss: 0.867921  [    0/ 3200]
loss: 0.710734  [ 1600/ 3200]
loss: 0.761111  [    0/ 3200]
loss: 0.967619  [ 1600/ 3200]
loss: 0.513726  [    0/ 3200]
loss: 0.756121  [ 1600/ 3200]
loss: 0.842520  [    0/ 3200]
loss: 0.687325  [ 1600/ 3200]
loss: 0.840765  [    0/ 3200]
loss: 0.914244  [ 1600/ 3200]
loss: 0.779194  [    0/ 3200]
loss: 1.288895  [ 1600/ 3200]
loss: 1.132390  [    0/ 3200]
loss: 0.926337  [ 1600/ 3200]
loss: 0.903292  [    0/ 3200]
loss: 0.703724  [ 1600/ 3200]
loss: 0.546295  [    0/ 3200]
loss: 0.667783  [ 1600/ 3200]
loss: 0.944510  [    0/ 3200]
loss: 0.753680  [ 1600/ 3200]
loss: 0.907950  [    0/ 3200]
loss: 1.053157  [ 1600/ 3200]
loss: 0.695031  [    0/ 3200]
loss: 0.911908  [ 1600/ 3200]
loss: 0.789507  [    0/ 3200]
loss: 0.744683  [ 1600/ 3200]
loss: 0.720211  [    0/ 3200]
loss: 0.935518  [ 1600/ 3200]
loss: 0.893800  [    0/ 3200]
loss: 0.644951  [ 1600/ 3200]
loss: 1.390955  [    0/ 3200]
loss: 0.639591  [ 1600/ 3200]
loss: 1.055425  [    0/ 3200]
loss: 0.621878  [ 1600/ 3200]
loss: 0.681124  [    0/ 3200]
loss: 1.037113  [ 1600/ 3200]
loss: 0.835963  [    0/ 3200]
loss: 0.831846  [ 1600/ 3200]
loss: 1.311149  [    0/ 3200]
loss: 0.659934  [ 1600/ 3200]
loss: 0.458466  [    0/ 3200]
loss: 1.169190  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0534
F1 Score (macro-averaged): 0.6619
Accuracy: 66.7878
Confusion Matrix:
[[127  23  69 105]
 [ 34 236   3  24]
 [ 52  12 271  21]
 [ 53  24  37 285]]
Epoch 7
-------------------------------
loss: 0.769207  [    0/ 3200]
loss: 0.593006  [ 1600/ 3200]
loss: 0.626599  [    0/ 3200]
loss: 0.748659  [ 1600/ 3200]
loss: 0.559193  [    0/ 3200]
loss: 0.756455  [ 1600/ 3200]
loss: 1.184229  [    0/ 3200]
loss: 1.298266  [ 1600/ 3200]
loss: 0.990450  [    0/ 3200]
loss: 1.156283  [ 1600/ 3200]
loss: 0.559417  [    0/ 3200]
loss: 0.986231  [ 1600/ 3200]
loss: 0.874903  [    0/ 3200]
loss: 1.094612  [ 1600/ 3200]
loss: 0.916144  [    0/ 3200]
loss: 0.829457  [ 1600/ 3200]
loss: 0.925995  [    0/ 3200]
loss: 0.572347  [ 1600/ 3200]
loss: 0.936871  [    0/ 3200]
loss: 0.674423  [ 1600/ 3200]
loss: 0.657430  [    0/ 3200]
loss: 0.976045  [ 1600/ 3200]
loss: 0.911291  [    0/ 3200]
loss: 0.990532  [ 1600/ 3200]
loss: 0.458751  [    0/ 3200]
loss: 0.946639  [ 1600/ 3200]
loss: 0.912206  [    0/ 3200]
loss: 0.774671  [ 1600/ 3200]
loss: 0.759656  [    0/ 3200]
loss: 0.992934  [ 1600/ 3200]
loss: 1.051511  [    0/ 3200]
loss: 0.511995  [ 1600/ 3200]
loss: 0.801922  [    0/ 3200]
loss: 0.690118  [ 1600/ 3200]
loss: 1.027908  [    0/ 3200]
loss: 0.836223  [ 1600/ 3200]
loss: 0.655734  [    0/ 3200]
loss: 0.299865  [ 1600/ 3200]
loss: 0.839823  [    0/ 3200]
loss: 1.188106  [ 1600/ 3200]
loss: 0.731710  [    0/ 3200]
loss: 0.712624  [ 1600/ 3200]
loss: 0.626256  [    0/ 3200]
loss: 1.006416  [ 1600/ 3200]
loss: 0.709451  [    0/ 3200]
loss: 1.177601  [ 1600/ 3200]
loss: 0.715177  [    0/ 3200]
loss: 0.806588  [ 1600/ 3200]
loss: 0.972402  [    0/ 3200]
loss: 0.978295  [ 1600/ 3200]
loss: 0.908135  [    0/ 3200]
loss: 1.153203  [ 1600/ 3200]
loss: 0.606325  [    0/ 3200]
loss: 1.091861  [ 1600/ 3200]
loss: 0.959005  [    0/ 3200]
loss: 0.967131  [ 1600/ 3200]
loss: 0.952181  [    0/ 3200]
loss: 0.863816  [ 1600/ 3200]
loss: 0.650888  [    0/ 3200]
loss: 0.987425  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0522
F1 Score (macro-averaged): 0.6417
Accuracy: 66.4244
Confusion Matrix:
[[ 85  25  85 129]
 [ 24 236   5  32]
 [ 28  11 286  31]
 [ 24  25  43 307]]
Epoch 8
-------------------------------
loss: 1.259276  [    0/ 3200]
loss: 0.819408  [ 1600/ 3200]
loss: 0.748234  [    0/ 3200]
loss: 0.893385  [ 1600/ 3200]
loss: 0.652385  [    0/ 3200]
loss: 1.176283  [ 1600/ 3200]
loss: 0.960584  [    0/ 3200]
loss: 0.405609  [ 1600/ 3200]
loss: 0.665060  [    0/ 3200]
loss: 1.196995  [ 1600/ 3200]
loss: 0.846247  [    0/ 3200]
loss: 0.938096  [ 1600/ 3200]
loss: 0.590269  [    0/ 3200]
loss: 0.836380  [ 1600/ 3200]
loss: 0.747793  [    0/ 3200]
loss: 0.819063  [ 1600/ 3200]
loss: 0.630182  [    0/ 3200]
loss: 1.131154  [ 1600/ 3200]
loss: 0.664219  [    0/ 3200]
loss: 1.033052  [ 1600/ 3200]
loss: 0.632663  [    0/ 3200]
loss: 0.503690  [ 1600/ 3200]
loss: 0.807590  [    0/ 3200]
loss: 0.589764  [ 1600/ 3200]
loss: 0.987649  [    0/ 3200]
loss: 0.877708  [ 1600/ 3200]
loss: 0.979158  [    0/ 3200]
loss: 0.663110  [ 1600/ 3200]
loss: 0.542073  [    0/ 3200]
loss: 0.836240  [ 1600/ 3200]
loss: 0.705437  [    0/ 3200]
loss: 0.694462  [ 1600/ 3200]
loss: 1.132255  [    0/ 3200]
loss: 0.938781  [ 1600/ 3200]
loss: 0.434129  [    0/ 3200]
loss: 1.133702  [ 1600/ 3200]
loss: 0.700007  [    0/ 3200]
loss: 0.675923  [ 1600/ 3200]
loss: 0.723895  [    0/ 3200]
loss: 1.212181  [ 1600/ 3200]
loss: 0.607057  [    0/ 3200]
loss: 0.771953  [ 1600/ 3200]
loss: 1.062359  [    0/ 3200]
loss: 0.755361  [ 1600/ 3200]
loss: 1.169309  [    0/ 3200]
loss: 0.760206  [ 1600/ 3200]
loss: 0.485540  [    0/ 3200]
loss: 0.829718  [ 1600/ 3200]
loss: 0.712353  [    0/ 3200]
loss: 0.768168  [ 1600/ 3200]
loss: 0.893785  [    0/ 3200]
loss: 0.768312  [ 1600/ 3200]
loss: 1.111923  [    0/ 3200]
loss: 0.533316  [ 1600/ 3200]
loss: 1.083751  [    0/ 3200]
loss: 0.642343  [ 1600/ 3200]
loss: 0.425668  [    0/ 3200]
loss: 0.774878  [ 1600/ 3200]
loss: 0.602190  [    0/ 3200]
loss: 0.973647  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0550
F1 Score (macro-averaged): 0.6306
Accuracy: 64.0988
Confusion Matrix:
[[105  36  34 149]
 [ 19 250   1  27]
 [ 87  11 210  48]
 [ 30  27  25 317]]
Epoch 9
-------------------------------
loss: 1.056988  [    0/ 3200]
loss: 0.952436  [ 1600/ 3200]
loss: 0.654800  [    0/ 3200]
loss: 0.736544  [ 1600/ 3200]
loss: 0.820785  [    0/ 3200]
loss: 1.112724  [ 1600/ 3200]
loss: 0.648142  [    0/ 3200]
loss: 0.685305  [ 1600/ 3200]
loss: 0.598179  [    0/ 3200]
loss: 0.643340  [ 1600/ 3200]
loss: 0.694204  [    0/ 3200]
loss: 1.031585  [ 1600/ 3200]
loss: 0.926935  [    0/ 3200]
loss: 0.712589  [ 1600/ 3200]
loss: 0.809470  [    0/ 3200]
loss: 0.478622  [ 1600/ 3200]
loss: 0.688771  [    0/ 3200]
loss: 0.812501  [ 1600/ 3200]
loss: 0.959316  [    0/ 3200]
loss: 0.868241  [ 1600/ 3200]
loss: 0.638138  [    0/ 3200]
loss: 0.620617  [ 1600/ 3200]
loss: 0.851977  [    0/ 3200]
loss: 0.689014  [ 1600/ 3200]
loss: 0.784822  [    0/ 3200]
loss: 0.602248  [ 1600/ 3200]
loss: 0.729841  [    0/ 3200]
loss: 0.786448  [ 1600/ 3200]
loss: 0.857778  [    0/ 3200]
loss: 1.016857  [ 1600/ 3200]
loss: 0.661252  [    0/ 3200]
loss: 0.707624  [ 1600/ 3200]
loss: 0.395301  [    0/ 3200]
loss: 0.923559  [ 1600/ 3200]
loss: 0.525624  [    0/ 3200]
loss: 0.992060  [ 1600/ 3200]
loss: 1.075236  [    0/ 3200]
loss: 0.886059  [ 1600/ 3200]
loss: 0.521467  [    0/ 3200]
loss: 0.795411  [ 1600/ 3200]
loss: 0.485447  [    0/ 3200]
loss: 0.773567  [ 1600/ 3200]
loss: 0.972723  [    0/ 3200]
loss: 0.705209  [ 1600/ 3200]
loss: 0.416988  [    0/ 3200]
loss: 0.944847  [ 1600/ 3200]
loss: 0.763213  [    0/ 3200]
loss: 0.608433  [ 1600/ 3200]
loss: 1.114474  [    0/ 3200]
loss: 0.468389  [ 1600/ 3200]
loss: 0.680712  [    0/ 3200]
loss: 0.617954  [ 1600/ 3200]
loss: 0.759009  [    0/ 3200]
loss: 1.289193  [ 1600/ 3200]
loss: 0.557458  [    0/ 3200]
loss: 0.934708  [ 1600/ 3200]
loss: 0.526406  [    0/ 3200]
loss: 0.567436  [ 1600/ 3200]
loss: 0.645755  [    0/ 3200]
loss: 0.965966  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0527
F1 Score (macro-averaged): 0.6643
Accuracy: 65.9157
Confusion Matrix:
[[173  30  32  89]
 [ 31 244   1  21]
 [109  11 216  20]
 [ 75  28  22 274]]
Epoch 10
-------------------------------
loss: 0.683799  [    0/ 3200]
loss: 0.775573  [ 1600/ 3200]
loss: 0.832655  [    0/ 3200]
loss: 0.816124  [ 1600/ 3200]
loss: 1.004059  [    0/ 3200]
loss: 0.933252  [ 1600/ 3200]
loss: 0.805942  [    0/ 3200]
loss: 0.835857  [ 1600/ 3200]
loss: 0.636106  [    0/ 3200]
loss: 1.087972  [ 1600/ 3200]
loss: 0.506051  [    0/ 3200]
loss: 0.964492  [ 1600/ 3200]
loss: 0.712118  [    0/ 3200]
loss: 0.617238  [ 1600/ 3200]
loss: 0.617762  [    0/ 3200]
loss: 1.096386  [ 1600/ 3200]
loss: 0.622058  [    0/ 3200]
loss: 1.067820  [ 1600/ 3200]
loss: 0.681737  [    0/ 3200]
loss: 0.619070  [ 1600/ 3200]
loss: 0.829176  [    0/ 3200]
loss: 0.704014  [ 1600/ 3200]
loss: 0.601583  [    0/ 3200]
loss: 1.196050  [ 1600/ 3200]
loss: 0.762190  [    0/ 3200]
loss: 0.722787  [ 1600/ 3200]
loss: 0.613939  [    0/ 3200]
loss: 0.853839  [ 1600/ 3200]
loss: 0.942362  [    0/ 3200]
loss: 0.903387  [ 1600/ 3200]
loss: 0.916644  [    0/ 3200]
loss: 0.743964  [ 1600/ 3200]
loss: 1.009867  [    0/ 3200]
loss: 1.564302  [ 1600/ 3200]
loss: 1.075657  [    0/ 3200]
loss: 0.624724  [ 1600/ 3200]
loss: 0.528056  [    0/ 3200]
loss: 0.968560  [ 1600/ 3200]
loss: 0.783799  [    0/ 3200]
loss: 1.023221  [ 1600/ 3200]
loss: 0.633247  [    0/ 3200]
loss: 0.560057  [ 1600/ 3200]
loss: 1.213120  [    0/ 3200]
loss: 0.858673  [ 1600/ 3200]
loss: 0.834119  [    0/ 3200]
loss: 0.542203  [ 1600/ 3200]
loss: 0.893830  [    0/ 3200]
loss: 0.758110  [ 1600/ 3200]
loss: 0.714267  [    0/ 3200]
loss: 1.219205  [ 1600/ 3200]
loss: 0.664467  [    0/ 3200]
loss: 0.990088  [ 1600/ 3200]
loss: 0.641415  [    0/ 3200]
loss: 0.877524  [ 1600/ 3200]
loss: 0.811485  [    0/ 3200]
loss: 0.746573  [ 1600/ 3200]
loss: 0.730813  [    0/ 3200]
loss: 0.733713  [ 1600/ 3200]
loss: 0.746954  [    0/ 3200]
loss: 1.139969  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0504
F1 Score (macro-averaged): 0.6770
Accuracy: 68.3866
Confusion Matrix:
[[137  22  80  85]
 [ 34 232   5  26]
 [ 33  10 298  15]
 [ 48  22  55 274]]
Epoch 11
-------------------------------
loss: 0.832154  [    0/ 3200]
loss: 0.703505  [ 1600/ 3200]
loss: 1.282497  [    0/ 3200]
loss: 0.650174  [ 1600/ 3200]
loss: 1.251795  [    0/ 3200]
loss: 0.688652  [ 1600/ 3200]
loss: 0.676978  [    0/ 3200]
loss: 0.599323  [ 1600/ 3200]
loss: 0.659368  [    0/ 3200]
loss: 0.656943  [ 1600/ 3200]
loss: 0.409681  [    0/ 3200]
loss: 0.756358  [ 1600/ 3200]
loss: 0.480702  [    0/ 3200]
loss: 1.023318  [ 1600/ 3200]
loss: 0.809632  [    0/ 3200]
loss: 1.033096  [ 1600/ 3200]
loss: 0.567138  [    0/ 3200]
loss: 0.703977  [ 1600/ 3200]
loss: 0.685516  [    0/ 3200]
loss: 0.455627  [ 1600/ 3200]
loss: 0.689548  [    0/ 3200]
loss: 0.812144  [ 1600/ 3200]
loss: 1.136506  [    0/ 3200]
loss: 0.685995  [ 1600/ 3200]
loss: 0.646045  [    0/ 3200]
loss: 0.786334  [ 1600/ 3200]
loss: 0.560043  [    0/ 3200]
loss: 0.966009  [ 1600/ 3200]
loss: 1.123758  [    0/ 3200]
loss: 0.817723  [ 1600/ 3200]
loss: 0.769619  [    0/ 3200]
loss: 1.248856  [ 1600/ 3200]
loss: 0.725478  [    0/ 3200]
loss: 0.820249  [ 1600/ 3200]
loss: 0.611121  [    0/ 3200]
loss: 0.704902  [ 1600/ 3200]
loss: 1.186662  [    0/ 3200]
loss: 0.762245  [ 1600/ 3200]
loss: 0.687492  [    0/ 3200]
loss: 0.404667  [ 1600/ 3200]
loss: 0.749923  [    0/ 3200]
loss: 0.903146  [ 1600/ 3200]
loss: 0.875791  [    0/ 3200]
loss: 0.704746  [ 1600/ 3200]
loss: 0.772603  [    0/ 3200]
loss: 0.793719  [ 1600/ 3200]
loss: 0.892591  [    0/ 3200]
loss: 0.900858  [ 1600/ 3200]
loss: 0.708537  [    0/ 3200]
loss: 1.093578  [ 1600/ 3200]
loss: 0.354454  [    0/ 3200]
loss: 1.353156  [ 1600/ 3200]
loss: 0.830251  [    0/ 3200]
loss: 0.531964  [ 1600/ 3200]
loss: 0.824995  [    0/ 3200]
loss: 0.824269  [ 1600/ 3200]
loss: 0.773730  [    0/ 3200]
loss: 1.118459  [ 1600/ 3200]
loss: 0.664282  [    0/ 3200]
loss: 0.626779  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0505
F1 Score (macro-averaged): 0.6787
Accuracy: 68.6773
Confusion Matrix:
[[132  19  70 103]
 [ 34 228   5  30]
 [ 36  11 288  21]
 [ 38  20  44 297]]
Epoch 12
-------------------------------
loss: 0.618862  [    0/ 3200]
loss: 0.900749  [ 1600/ 3200]
loss: 0.921998  [    0/ 3200]
loss: 0.765860  [ 1600/ 3200]
loss: 0.956051  [    0/ 3200]
loss: 0.541866  [ 1600/ 3200]
loss: 0.803694  [    0/ 3200]
loss: 0.710758  [ 1600/ 3200]
loss: 1.147606  [    0/ 3200]
loss: 0.741467  [ 1600/ 3200]
loss: 0.881588  [    0/ 3200]
loss: 1.070590  [ 1600/ 3200]
loss: 0.700359  [    0/ 3200]
loss: 0.848978  [ 1600/ 3200]
loss: 0.899748  [    0/ 3200]
loss: 0.921573  [ 1600/ 3200]
loss: 0.616108  [    0/ 3200]
loss: 0.944046  [ 1600/ 3200]
loss: 1.088640  [    0/ 3200]
loss: 1.076060  [ 1600/ 3200]
loss: 0.780988  [    0/ 3200]
loss: 0.967443  [ 1600/ 3200]
loss: 0.732884  [    0/ 3200]
loss: 0.989752  [ 1600/ 3200]
loss: 0.815372  [    0/ 3200]
loss: 0.958885  [ 1600/ 3200]
loss: 0.678228  [    0/ 3200]
loss: 0.868165  [ 1600/ 3200]
loss: 1.005220  [    0/ 3200]
loss: 0.766017  [ 1600/ 3200]
loss: 0.821326  [    0/ 3200]
loss: 0.602254  [ 1600/ 3200]
loss: 0.898142  [    0/ 3200]
loss: 0.643446  [ 1600/ 3200]
loss: 0.606442  [    0/ 3200]
loss: 1.172039  [ 1600/ 3200]
loss: 0.717127  [    0/ 3200]
loss: 0.677328  [ 1600/ 3200]
loss: 0.678368  [    0/ 3200]
loss: 0.997389  [ 1600/ 3200]
loss: 0.611914  [    0/ 3200]
loss: 0.530493  [ 1600/ 3200]
loss: 0.840822  [    0/ 3200]
loss: 1.096862  [ 1600/ 3200]
loss: 0.867956  [    0/ 3200]
loss: 0.945408  [ 1600/ 3200]
loss: 0.604222  [    0/ 3200]
loss: 0.568201  [ 1600/ 3200]
loss: 0.610307  [    0/ 3200]
loss: 0.858321  [ 1600/ 3200]
loss: 0.552266  [    0/ 3200]
loss: 0.634441  [ 1600/ 3200]
loss: 0.475926  [    0/ 3200]
loss: 1.517738  [ 1600/ 3200]
loss: 0.677739  [    0/ 3200]
loss: 0.723578  [ 1600/ 3200]
loss: 0.708170  [    0/ 3200]
loss: 0.737007  [ 1600/ 3200]
loss: 0.800142  [    0/ 3200]
loss: 0.450316  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0502
F1 Score (macro-averaged): 0.6652
Accuracy: 68.0233
Confusion Matrix:
[[107  19  75 123]
 [ 30 231   6  30]
 [ 27  11 289  29]
 [ 25  19  46 309]]
Epoch 13
-------------------------------
loss: 0.684164  [    0/ 3200]
loss: 0.677145  [ 1600/ 3200]
loss: 0.812792  [    0/ 3200]
loss: 0.655169  [ 1600/ 3200]
loss: 0.637049  [    0/ 3200]
loss: 0.490295  [ 1600/ 3200]
loss: 0.478182  [    0/ 3200]
loss: 0.555215  [ 1600/ 3200]
loss: 0.775988  [    0/ 3200]
loss: 0.629056  [ 1600/ 3200]
loss: 0.844295  [    0/ 3200]
loss: 0.726433  [ 1600/ 3200]
loss: 1.266338  [    0/ 3200]
loss: 0.541748  [ 1600/ 3200]
loss: 0.775599  [    0/ 3200]
loss: 1.218294  [ 1600/ 3200]
loss: 0.795388  [    0/ 3200]
loss: 0.467763  [ 1600/ 3200]
loss: 0.520139  [    0/ 3200]
loss: 0.395177  [ 1600/ 3200]
loss: 0.691790  [    0/ 3200]
loss: 0.709253  [ 1600/ 3200]
loss: 0.398039  [    0/ 3200]
loss: 0.737655  [ 1600/ 3200]
loss: 1.034140  [    0/ 3200]
loss: 0.676014  [ 1600/ 3200]
loss: 0.616439  [    0/ 3200]
loss: 0.565498  [ 1600/ 3200]
loss: 0.515601  [    0/ 3200]
loss: 0.969588  [ 1600/ 3200]
loss: 0.651973  [    0/ 3200]
loss: 0.748811  [ 1600/ 3200]
loss: 0.796358  [    0/ 3200]
loss: 0.966663  [ 1600/ 3200]
loss: 0.729852  [    0/ 3200]
loss: 0.784391  [ 1600/ 3200]
loss: 0.837180  [    0/ 3200]
loss: 0.633513  [ 1600/ 3200]
loss: 0.776205  [    0/ 3200]
loss: 0.764359  [ 1600/ 3200]
loss: 0.784615  [    0/ 3200]
loss: 0.838877  [ 1600/ 3200]
loss: 0.719202  [    0/ 3200]
loss: 0.938615  [ 1600/ 3200]
loss: 1.086538  [    0/ 3200]
loss: 0.861997  [ 1600/ 3200]
loss: 0.612966  [    0/ 3200]
loss: 0.455289  [ 1600/ 3200]
loss: 0.735626  [    0/ 3200]
loss: 0.552898  [ 1600/ 3200]
loss: 0.498674  [    0/ 3200]
loss: 0.793795  [ 1600/ 3200]
loss: 0.953696  [    0/ 3200]
loss: 1.110780  [ 1600/ 3200]
loss: 0.632689  [    0/ 3200]
loss: 0.528343  [ 1600/ 3200]
loss: 1.035797  [    0/ 3200]
loss: 0.982705  [ 1600/ 3200]
loss: 0.923378  [    0/ 3200]
loss: 0.829547  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0518
F1 Score (macro-averaged): 0.6838
Accuracy: 68.5320
Confusion Matrix:
[[165  43  42  74]
 [ 17 265   0  15]
 [ 71  16 254  15]
 [ 68  45  27 259]]
Epoch 14
-------------------------------
loss: 0.658618  [    0/ 3200]
loss: 1.036756  [ 1600/ 3200]
loss: 0.894417  [    0/ 3200]
loss: 0.894053  [ 1600/ 3200]
loss: 0.797932  [    0/ 3200]
loss: 0.910779  [ 1600/ 3200]
loss: 0.701629  [    0/ 3200]
loss: 0.712235  [ 1600/ 3200]
loss: 1.111310  [    0/ 3200]
loss: 0.678698  [ 1600/ 3200]
loss: 0.579079  [    0/ 3200]
loss: 0.445446  [ 1600/ 3200]
loss: 0.876562  [    0/ 3200]
loss: 0.423267  [ 1600/ 3200]
loss: 0.847545  [    0/ 3200]
loss: 0.407097  [ 1600/ 3200]
loss: 0.862591  [    0/ 3200]
loss: 0.658458  [ 1600/ 3200]
loss: 0.505613  [    0/ 3200]
loss: 0.603629  [ 1600/ 3200]
loss: 0.534778  [    0/ 3200]
loss: 0.847506  [ 1600/ 3200]
loss: 0.492896  [    0/ 3200]
loss: 0.460841  [ 1600/ 3200]
loss: 0.822008  [    0/ 3200]
loss: 0.719577  [ 1600/ 3200]
loss: 0.954080  [    0/ 3200]
loss: 0.905187  [ 1600/ 3200]
loss: 0.661962  [    0/ 3200]
loss: 0.949927  [ 1600/ 3200]
loss: 0.538895  [    0/ 3200]
loss: 0.944903  [ 1600/ 3200]
loss: 0.560143  [    0/ 3200]
loss: 0.575046  [ 1600/ 3200]
loss: 0.682273  [    0/ 3200]
loss: 0.557075  [ 1600/ 3200]
loss: 0.559832  [    0/ 3200]
loss: 0.805009  [ 1600/ 3200]
loss: 0.744623  [    0/ 3200]
loss: 0.602459  [ 1600/ 3200]
loss: 0.645218  [    0/ 3200]
loss: 0.761538  [ 1600/ 3200]
loss: 0.787916  [    0/ 3200]
loss: 0.501134  [ 1600/ 3200]
loss: 0.681410  [    0/ 3200]
loss: 0.603060  [ 1600/ 3200]
loss: 0.787931  [    0/ 3200]
loss: 0.530984  [ 1600/ 3200]
loss: 0.685141  [    0/ 3200]
loss: 0.897259  [ 1600/ 3200]
loss: 0.726450  [    0/ 3200]
loss: 0.770157  [ 1600/ 3200]
loss: 0.534791  [    0/ 3200]
loss: 0.945330  [ 1600/ 3200]
loss: 0.649268  [    0/ 3200]
loss: 0.680836  [ 1600/ 3200]
loss: 0.734915  [    0/ 3200]
loss: 0.735178  [ 1600/ 3200]
loss: 0.804962  [    0/ 3200]
loss: 0.661529  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0521
F1 Score (macro-averaged): 0.6809
Accuracy: 67.2965
Confusion Matrix:
[[204  27  21  72]
 [ 34 246   0  17]
 [123  11 204  18]
 [ 86  25  16 272]]
Epoch 15
-------------------------------
loss: 0.507141  [    0/ 3200]
loss: 1.062066  [ 1600/ 3200]
loss: 0.964230  [    0/ 3200]
loss: 0.445389  [ 1600/ 3200]
loss: 0.810931  [    0/ 3200]
loss: 0.477256  [ 1600/ 3200]
loss: 0.371171  [    0/ 3200]
loss: 1.091300  [ 1600/ 3200]
loss: 0.558259  [    0/ 3200]
loss: 0.640508  [ 1600/ 3200]
loss: 0.723461  [    0/ 3200]
loss: 0.763687  [ 1600/ 3200]
loss: 0.568361  [    0/ 3200]
loss: 0.725635  [ 1600/ 3200]
loss: 0.930193  [    0/ 3200]
loss: 0.429722  [ 1600/ 3200]
loss: 0.789836  [    0/ 3200]
loss: 0.486164  [ 1600/ 3200]
loss: 1.084309  [    0/ 3200]
loss: 0.619294  [ 1600/ 3200]
loss: 0.744636  [    0/ 3200]
loss: 0.594389  [ 1600/ 3200]
loss: 0.703729  [    0/ 3200]
loss: 1.391278  [ 1600/ 3200]
loss: 1.174036  [    0/ 3200]
loss: 0.610636  [ 1600/ 3200]
loss: 0.595792  [    0/ 3200]
loss: 0.720262  [ 1600/ 3200]
loss: 0.359917  [    0/ 3200]
loss: 0.755470  [ 1600/ 3200]
loss: 0.423849  [    0/ 3200]
loss: 0.788572  [ 1600/ 3200]
loss: 0.571810  [    0/ 3200]
loss: 0.682316  [ 1600/ 3200]
loss: 0.607529  [    0/ 3200]
loss: 0.625161  [ 1600/ 3200]
loss: 0.460398  [    0/ 3200]
loss: 0.725554  [ 1600/ 3200]
loss: 0.655874  [    0/ 3200]
loss: 0.982896  [ 1600/ 3200]
loss: 0.403803  [    0/ 3200]
loss: 0.801917  [ 1600/ 3200]
loss: 0.551858  [    0/ 3200]
loss: 0.660399  [ 1600/ 3200]
loss: 0.910987  [    0/ 3200]
loss: 0.569377  [ 1600/ 3200]
loss: 1.191872  [    0/ 3200]
loss: 0.953950  [ 1600/ 3200]
loss: 0.450751  [    0/ 3200]
loss: 0.572257  [ 1600/ 3200]
loss: 0.570453  [    0/ 3200]
loss: 0.790107  [ 1600/ 3200]
loss: 0.886438  [    0/ 3200]
loss: 1.093083  [ 1600/ 3200]
loss: 1.069047  [    0/ 3200]
loss: 1.018631  [ 1600/ 3200]
loss: 0.544597  [    0/ 3200]
loss: 0.685280  [ 1600/ 3200]
loss: 0.499599  [    0/ 3200]
loss: 0.902746  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0505
F1 Score (macro-averaged): 0.6839
Accuracy: 68.0959
Confusion Matrix:
[[177  14  63  70]
 [ 58 210   6  23]
 [ 51   6 286  13]
 [ 73  12  50 264]]
Epoch 16
-------------------------------
loss: 0.854027  [    0/ 3200]
loss: 0.595134  [ 1600/ 3200]
loss: 0.825027  [    0/ 3200]
loss: 0.634523  [ 1600/ 3200]
loss: 1.058329  [    0/ 3200]
loss: 1.489820  [ 1600/ 3200]
loss: 0.861821  [    0/ 3200]
loss: 0.844092  [ 1600/ 3200]
loss: 0.802993  [    0/ 3200]
loss: 0.387171  [ 1600/ 3200]
loss: 0.550108  [    0/ 3200]
loss: 0.863181  [ 1600/ 3200]
loss: 0.572223  [    0/ 3200]
loss: 0.420493  [ 1600/ 3200]
loss: 1.046049  [    0/ 3200]
loss: 0.438172  [ 1600/ 3200]
loss: 0.871096  [    0/ 3200]
loss: 0.349387  [ 1600/ 3200]
loss: 0.635752  [    0/ 3200]
loss: 0.785044  [ 1600/ 3200]
loss: 0.942669  [    0/ 3200]
loss: 0.723068  [ 1600/ 3200]
loss: 0.825286  [    0/ 3200]
loss: 0.972999  [ 1600/ 3200]
loss: 0.839384  [    0/ 3200]
loss: 0.467447  [ 1600/ 3200]
loss: 0.637489  [    0/ 3200]
loss: 0.712854  [ 1600/ 3200]
loss: 0.647569  [    0/ 3200]
loss: 0.555719  [ 1600/ 3200]
loss: 0.786836  [    0/ 3200]
loss: 0.481411  [ 1600/ 3200]
loss: 0.599581  [    0/ 3200]
loss: 0.688473  [ 1600/ 3200]
loss: 0.727970  [    0/ 3200]
loss: 0.612752  [ 1600/ 3200]
loss: 0.681388  [    0/ 3200]
loss: 1.078784  [ 1600/ 3200]
loss: 0.678539  [    0/ 3200]
loss: 0.577620  [ 1600/ 3200]
loss: 0.502618  [    0/ 3200]
loss: 0.752931  [ 1600/ 3200]
loss: 0.513533  [    0/ 3200]
loss: 0.698053  [ 1600/ 3200]
loss: 0.388343  [    0/ 3200]
loss: 0.660307  [ 1600/ 3200]
loss: 0.784497  [    0/ 3200]
loss: 0.665174  [ 1600/ 3200]
loss: 0.896528  [    0/ 3200]
loss: 0.967373  [ 1600/ 3200]
loss: 0.721068  [    0/ 3200]
loss: 0.942688  [ 1600/ 3200]
loss: 0.633320  [    0/ 3200]
loss: 1.095631  [ 1600/ 3200]
loss: 0.434155  [    0/ 3200]
loss: 0.839606  [ 1600/ 3200]
loss: 0.899006  [    0/ 3200]
loss: 0.608503  [ 1600/ 3200]
loss: 0.651974  [    0/ 3200]
loss: 0.601853  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0510
F1 Score (macro-averaged): 0.6694
Accuracy: 67.9506
Confusion Matrix:
[[130  20 101  73]
 [ 35 229  11  22]
 [ 17  12 316  11]
 [ 50  26  63 260]]
Epoch 17
-------------------------------
loss: 0.697282  [    0/ 3200]
loss: 0.832444  [ 1600/ 3200]
loss: 0.706605  [    0/ 3200]
loss: 0.588169  [ 1600/ 3200]
loss: 0.990238  [    0/ 3200]
loss: 1.015437  [ 1600/ 3200]
loss: 0.811404  [    0/ 3200]
loss: 0.488254  [ 1600/ 3200]
loss: 0.771597  [    0/ 3200]
loss: 0.713046  [ 1600/ 3200]
loss: 0.970558  [    0/ 3200]
loss: 0.611118  [ 1600/ 3200]
loss: 0.996151  [    0/ 3200]
loss: 1.000329  [ 1600/ 3200]
loss: 0.532638  [    0/ 3200]
loss: 0.972035  [ 1600/ 3200]
loss: 0.750370  [    0/ 3200]
loss: 0.867848  [ 1600/ 3200]
loss: 0.773540  [    0/ 3200]
loss: 0.578589  [ 1600/ 3200]
loss: 0.363047  [    0/ 3200]
loss: 0.651931  [ 1600/ 3200]
loss: 0.490249  [    0/ 3200]
loss: 0.923396  [ 1600/ 3200]
loss: 0.575417  [    0/ 3200]
loss: 0.555430  [ 1600/ 3200]
loss: 1.059175  [    0/ 3200]
loss: 0.422604  [ 1600/ 3200]
loss: 0.490210  [    0/ 3200]
loss: 0.897803  [ 1600/ 3200]
loss: 0.441920  [    0/ 3200]
loss: 0.786885  [ 1600/ 3200]
loss: 0.782156  [    0/ 3200]
loss: 0.799918  [ 1600/ 3200]
loss: 0.648513  [    0/ 3200]
loss: 0.623364  [ 1600/ 3200]
loss: 0.593933  [    0/ 3200]
loss: 0.503992  [ 1600/ 3200]
loss: 1.215792  [    0/ 3200]
loss: 0.523393  [ 1600/ 3200]
loss: 1.044157  [    0/ 3200]
loss: 0.766456  [ 1600/ 3200]
loss: 0.671105  [    0/ 3200]
loss: 0.993709  [ 1600/ 3200]
loss: 0.729776  [    0/ 3200]
loss: 0.495755  [ 1600/ 3200]
loss: 0.562740  [    0/ 3200]
loss: 0.808391  [ 1600/ 3200]
loss: 0.743313  [    0/ 3200]
loss: 0.579723  [ 1600/ 3200]
loss: 0.984947  [    0/ 3200]
loss: 0.621072  [ 1600/ 3200]
loss: 0.955789  [    0/ 3200]
loss: 1.026088  [ 1600/ 3200]
loss: 0.769778  [    0/ 3200]
loss: 0.566570  [ 1600/ 3200]
loss: 0.822996  [    0/ 3200]
loss: 0.656445  [ 1600/ 3200]
loss: 0.807869  [    0/ 3200]
loss: 0.516890  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0512
F1 Score (macro-averaged): 0.6799
Accuracy: 67.7326
Confusion Matrix:
[[177  10  74  63]
 [ 72 200   7  18]
 [ 41   3 300  12]
 [ 77  11  56 255]]
Epoch 18
-------------------------------
loss: 0.479743  [    0/ 3200]
loss: 0.624988  [ 1600/ 3200]
loss: 0.577761  [    0/ 3200]
loss: 0.604401  [ 1600/ 3200]
loss: 0.906627  [    0/ 3200]
loss: 0.950461  [ 1600/ 3200]
loss: 0.780342  [    0/ 3200]
loss: 0.789437  [ 1600/ 3200]
loss: 0.988102  [    0/ 3200]
loss: 0.643309  [ 1600/ 3200]
loss: 0.419414  [    0/ 3200]
loss: 0.585554  [ 1600/ 3200]
loss: 0.670076  [    0/ 3200]
loss: 1.207610  [ 1600/ 3200]
loss: 0.499499  [    0/ 3200]
loss: 1.555068  [ 1600/ 3200]
loss: 1.066172  [    0/ 3200]
loss: 1.009539  [ 1600/ 3200]
loss: 0.721171  [    0/ 3200]
loss: 1.179328  [ 1600/ 3200]
loss: 0.508642  [    0/ 3200]
loss: 1.360321  [ 1600/ 3200]
loss: 0.540120  [    0/ 3200]
loss: 0.935695  [ 1600/ 3200]
loss: 0.852788  [    0/ 3200]
loss: 0.527434  [ 1600/ 3200]
loss: 0.840300  [    0/ 3200]
loss: 1.231466  [ 1600/ 3200]
loss: 0.596308  [    0/ 3200]
loss: 0.669060  [ 1600/ 3200]
loss: 0.886880  [    0/ 3200]
loss: 0.446725  [ 1600/ 3200]
loss: 0.796839  [    0/ 3200]
loss: 0.847849  [ 1600/ 3200]
loss: 0.484298  [    0/ 3200]
loss: 0.988918  [ 1600/ 3200]
loss: 1.076092  [    0/ 3200]
loss: 0.524830  [ 1600/ 3200]
loss: 0.657213  [    0/ 3200]
loss: 0.880886  [ 1600/ 3200]
loss: 0.530104  [    0/ 3200]
loss: 0.413874  [ 1600/ 3200]
loss: 0.570796  [    0/ 3200]
loss: 0.839251  [ 1600/ 3200]
loss: 0.406139  [    0/ 3200]
loss: 0.747217  [ 1600/ 3200]
loss: 0.635109  [    0/ 3200]
loss: 0.797550  [ 1600/ 3200]
loss: 0.891594  [    0/ 3200]
loss: 0.787904  [ 1600/ 3200]
loss: 1.087204  [    0/ 3200]
loss: 0.717865  [ 1600/ 3200]
loss: 0.499991  [    0/ 3200]
loss: 0.483996  [ 1600/ 3200]
loss: 0.774210  [    0/ 3200]
loss: 0.903146  [ 1600/ 3200]
loss: 0.572592  [    0/ 3200]
loss: 0.700875  [ 1600/ 3200]
loss: 0.690061  [    0/ 3200]
loss: 0.472841  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0488
F1 Score (macro-averaged): 0.6630
Accuracy: 68.1686
Confusion Matrix:
[[101  21  97 105]
 [ 31 226  12  28]
 [ 16   8 313  19]
 [ 24  14  63 298]]
Epoch 19
-------------------------------
loss: 0.764839  [    0/ 3200]
loss: 0.833985  [ 1600/ 3200]
loss: 0.672319  [    0/ 3200]
loss: 0.789583  [ 1600/ 3200]
loss: 0.621691  [    0/ 3200]
loss: 0.494217  [ 1600/ 3200]
loss: 0.459958  [    0/ 3200]
loss: 0.538746  [ 1600/ 3200]
loss: 0.567869  [    0/ 3200]
loss: 0.538263  [ 1600/ 3200]
loss: 0.620478  [    0/ 3200]
loss: 0.716834  [ 1600/ 3200]
loss: 0.575517  [    0/ 3200]
loss: 0.584935  [ 1600/ 3200]
loss: 0.487851  [    0/ 3200]
loss: 0.867228  [ 1600/ 3200]
loss: 0.684366  [    0/ 3200]
loss: 0.459065  [ 1600/ 3200]
loss: 0.524509  [    0/ 3200]
loss: 0.897744  [ 1600/ 3200]
loss: 0.790756  [    0/ 3200]
loss: 0.484341  [ 1600/ 3200]
loss: 0.579687  [    0/ 3200]
loss: 0.433957  [ 1600/ 3200]
loss: 0.708535  [    0/ 3200]
loss: 0.978788  [ 1600/ 3200]
loss: 0.833118  [    0/ 3200]
loss: 0.809151  [ 1600/ 3200]
loss: 0.595209  [    0/ 3200]
loss: 0.928248  [ 1600/ 3200]
loss: 0.770498  [    0/ 3200]
loss: 0.433662  [ 1600/ 3200]
loss: 0.834284  [    0/ 3200]
loss: 1.195972  [ 1600/ 3200]
loss: 1.102944  [    0/ 3200]
loss: 0.728498  [ 1600/ 3200]
loss: 0.798249  [    0/ 3200]
loss: 0.940665  [ 1600/ 3200]
loss: 0.567234  [    0/ 3200]
loss: 0.801979  [ 1600/ 3200]
loss: 0.568263  [    0/ 3200]
loss: 0.857180  [ 1600/ 3200]
loss: 0.675565  [    0/ 3200]
loss: 1.048785  [ 1600/ 3200]
loss: 1.270457  [    0/ 3200]
loss: 0.681069  [ 1600/ 3200]
loss: 1.303317  [    0/ 3200]
loss: 0.589583  [ 1600/ 3200]
loss: 0.546342  [    0/ 3200]
loss: 0.341892  [ 1600/ 3200]
loss: 0.972609  [    0/ 3200]
loss: 0.550519  [ 1600/ 3200]
loss: 0.537919  [    0/ 3200]
loss: 1.079470  [ 1600/ 3200]
loss: 0.511546  [    0/ 3200]
loss: 0.670146  [ 1600/ 3200]
loss: 0.825640  [    0/ 3200]
loss: 0.789104  [ 1600/ 3200]
loss: 1.095509  [    0/ 3200]
loss: 0.378679  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0499
F1 Score (macro-averaged): 0.6930
Accuracy: 69.5494
Confusion Matrix:
[[162  20  79  63]
 [ 33 244   7  13]
 [ 31  11 304  10]
 [ 70  25  57 247]]
Epoch 20
-------------------------------
loss: 0.583145  [    0/ 3200]
loss: 0.523221  [ 1600/ 3200]
loss: 0.590308  [    0/ 3200]
loss: 0.747161  [ 1600/ 3200]
loss: 1.027615  [    0/ 3200]
loss: 0.616037  [ 1600/ 3200]
loss: 0.486840  [    0/ 3200]
loss: 0.609732  [ 1600/ 3200]
loss: 0.392120  [    0/ 3200]
loss: 0.763225  [ 1600/ 3200]
loss: 0.782655  [    0/ 3200]
loss: 0.823354  [ 1600/ 3200]
loss: 0.341455  [    0/ 3200]
loss: 0.501718  [ 1600/ 3200]
loss: 0.798000  [    0/ 3200]
loss: 0.614337  [ 1600/ 3200]
loss: 0.959692  [    0/ 3200]
loss: 0.926763  [ 1600/ 3200]
loss: 1.027420  [    0/ 3200]
loss: 0.743361  [ 1600/ 3200]
loss: 0.642345  [    0/ 3200]
loss: 0.609719  [ 1600/ 3200]
loss: 0.542422  [    0/ 3200]
loss: 1.071165  [ 1600/ 3200]
loss: 1.061431  [    0/ 3200]
loss: 0.576132  [ 1600/ 3200]
loss: 0.888999  [    0/ 3200]
loss: 0.820446  [ 1600/ 3200]
loss: 0.634045  [    0/ 3200]
loss: 1.029378  [ 1600/ 3200]
loss: 0.566557  [    0/ 3200]
loss: 0.423125  [ 1600/ 3200]
loss: 0.499987  [    0/ 3200]
loss: 0.722677  [ 1600/ 3200]
loss: 0.837197  [    0/ 3200]
loss: 0.736929  [ 1600/ 3200]
loss: 0.513237  [    0/ 3200]
loss: 0.683595  [ 1600/ 3200]
loss: 0.884624  [    0/ 3200]
loss: 0.656303  [ 1600/ 3200]
loss: 0.910931  [    0/ 3200]
loss: 1.010170  [ 1600/ 3200]
loss: 0.821311  [    0/ 3200]
loss: 0.504866  [ 1600/ 3200]
loss: 0.836750  [    0/ 3200]
loss: 0.875545  [ 1600/ 3200]
loss: 0.688680  [    0/ 3200]
loss: 0.576647  [ 1600/ 3200]
loss: 1.072340  [    0/ 3200]
loss: 0.954824  [ 1600/ 3200]
loss: 0.906722  [    0/ 3200]
loss: 0.809659  [ 1600/ 3200]
loss: 0.765399  [    0/ 3200]
loss: 0.484258  [ 1600/ 3200]
loss: 0.762982  [    0/ 3200]
loss: 0.450529  [ 1600/ 3200]
loss: 0.809497  [    0/ 3200]
loss: 0.684734  [ 1600/ 3200]
loss: 0.654573  [    0/ 3200]
loss: 0.808592  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0486
F1 Score (macro-averaged): 0.6776
Accuracy: 68.8953
Confusion Matrix:
[[120  22  67 115]
 [ 30 232   5  30]
 [ 31  11 288  26]
 [ 32  15  44 308]]
Epoch 21
-------------------------------
loss: 0.669824  [    0/ 3200]
loss: 0.558713  [ 1600/ 3200]
loss: 0.964837  [    0/ 3200]
loss: 0.856632  [ 1600/ 3200]
loss: 0.988128  [    0/ 3200]
loss: 0.629070  [ 1600/ 3200]
loss: 0.710800  [    0/ 3200]
loss: 0.576550  [ 1600/ 3200]
loss: 0.703267  [    0/ 3200]
loss: 0.493143  [ 1600/ 3200]
loss: 0.655192  [    0/ 3200]
loss: 0.374569  [ 1600/ 3200]
loss: 0.776893  [    0/ 3200]
loss: 0.709407  [ 1600/ 3200]
loss: 0.608125  [    0/ 3200]
loss: 0.600323  [ 1600/ 3200]
loss: 0.624950  [    0/ 3200]
loss: 0.990246  [ 1600/ 3200]
loss: 0.768114  [    0/ 3200]
loss: 0.758198  [ 1600/ 3200]
loss: 0.997747  [    0/ 3200]
loss: 0.538188  [ 1600/ 3200]
loss: 0.558198  [    0/ 3200]
loss: 1.001478  [ 1600/ 3200]
loss: 0.423932  [    0/ 3200]
loss: 0.515033  [ 1600/ 3200]
loss: 0.544925  [    0/ 3200]
loss: 0.712608  [ 1600/ 3200]
loss: 1.298788  [    0/ 3200]
loss: 0.657169  [ 1600/ 3200]
loss: 0.932924  [    0/ 3200]
loss: 0.789186  [ 1600/ 3200]
loss: 0.640095  [    0/ 3200]
loss: 0.691406  [ 1600/ 3200]
loss: 1.265053  [    0/ 3200]
loss: 0.217155  [ 1600/ 3200]
loss: 1.151051  [    0/ 3200]
loss: 0.671279  [ 1600/ 3200]
loss: 0.495273  [    0/ 3200]
loss: 0.449976  [ 1600/ 3200]
loss: 0.960588  [    0/ 3200]
loss: 0.445706  [ 1600/ 3200]
loss: 0.621753  [    0/ 3200]
loss: 0.442248  [ 1600/ 3200]
loss: 0.758101  [    0/ 3200]
loss: 0.753884  [ 1600/ 3200]
loss: 0.770218  [    0/ 3200]
loss: 0.766824  [ 1600/ 3200]
loss: 0.818057  [    0/ 3200]
loss: 0.927038  [ 1600/ 3200]
loss: 0.699267  [    0/ 3200]
loss: 0.639886  [ 1600/ 3200]
loss: 1.568809  [    0/ 3200]
loss: 0.863508  [ 1600/ 3200]
loss: 0.768028  [    0/ 3200]
loss: 0.718476  [ 1600/ 3200]
loss: 0.442566  [    0/ 3200]
loss: 0.719116  [ 1600/ 3200]
loss: 0.628357  [    0/ 3200]
loss: 0.554939  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0508
F1 Score (macro-averaged): 0.6856
Accuracy: 68.5320
Confusion Matrix:
[[174  42  31  77]
 [ 18 268   1  10]
 [ 82  15 237  22]
 [ 67  42  26 264]]
Epoch 22
-------------------------------
loss: 0.841300  [    0/ 3200]
loss: 0.712168  [ 1600/ 3200]
loss: 0.544423  [    0/ 3200]
loss: 0.976297  [ 1600/ 3200]
loss: 0.733607  [    0/ 3200]
loss: 0.733788  [ 1600/ 3200]
loss: 0.792325  [    0/ 3200]
loss: 0.586342  [ 1600/ 3200]
loss: 0.847713  [    0/ 3200]
loss: 0.810180  [ 1600/ 3200]
loss: 0.875373  [    0/ 3200]
loss: 0.643075  [ 1600/ 3200]
loss: 0.580046  [    0/ 3200]
loss: 0.621886  [ 1600/ 3200]
loss: 0.610531  [    0/ 3200]
loss: 0.786342  [ 1600/ 3200]
loss: 1.071209  [    0/ 3200]
loss: 0.664877  [ 1600/ 3200]
loss: 0.814563  [    0/ 3200]
loss: 0.765261  [ 1600/ 3200]
loss: 0.590503  [    0/ 3200]
loss: 0.709194  [ 1600/ 3200]
loss: 0.855768  [    0/ 3200]
loss: 0.467384  [ 1600/ 3200]
loss: 0.589659  [    0/ 3200]
loss: 0.474738  [ 1600/ 3200]
loss: 0.794058  [    0/ 3200]
loss: 0.402124  [ 1600/ 3200]
loss: 0.787003  [    0/ 3200]
loss: 0.562553  [ 1600/ 3200]
loss: 0.754455  [    0/ 3200]
loss: 0.745804  [ 1600/ 3200]
loss: 0.494779  [    0/ 3200]
loss: 0.846503  [ 1600/ 3200]
loss: 0.364946  [    0/ 3200]
loss: 0.621667  [ 1600/ 3200]
loss: 0.658853  [    0/ 3200]
loss: 0.813347  [ 1600/ 3200]
loss: 0.938197  [    0/ 3200]
loss: 0.542254  [ 1600/ 3200]
loss: 1.005229  [    0/ 3200]
loss: 0.539139  [ 1600/ 3200]
loss: 0.754579  [    0/ 3200]
loss: 0.745523  [ 1600/ 3200]
loss: 0.855148  [    0/ 3200]
loss: 0.563978  [ 1600/ 3200]
loss: 0.814380  [    0/ 3200]
loss: 0.564273  [ 1600/ 3200]
loss: 0.644069  [    0/ 3200]
loss: 0.676516  [ 1600/ 3200]
loss: 1.156447  [    0/ 3200]
loss: 0.523039  [ 1600/ 3200]
loss: 0.657003  [    0/ 3200]
loss: 0.713181  [ 1600/ 3200]
loss: 0.473453  [    0/ 3200]
loss: 0.847065  [ 1600/ 3200]
loss: 0.647326  [    0/ 3200]
loss: 0.855132  [ 1600/ 3200]
loss: 0.769238  [    0/ 3200]
loss: 0.681129  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0484
F1 Score (macro-averaged): 0.7104
Accuracy: 70.6395
Confusion Matrix:
[[194  22  53  55]
 [ 33 253   3   8]
 [ 53   9 280  14]
 [ 91  20  43 245]]
Epoch 23
-------------------------------
loss: 0.861711  [    0/ 3200]
loss: 0.570480  [ 1600/ 3200]
loss: 0.631553  [    0/ 3200]
loss: 0.468779  [ 1600/ 3200]
loss: 1.094367  [    0/ 3200]
loss: 0.291264  [ 1600/ 3200]
loss: 0.749057  [    0/ 3200]
loss: 0.457626  [ 1600/ 3200]
loss: 0.603907  [    0/ 3200]
loss: 0.474742  [ 1600/ 3200]
loss: 0.522213  [    0/ 3200]
loss: 0.687248  [ 1600/ 3200]
loss: 0.555852  [    0/ 3200]
loss: 1.182709  [ 1600/ 3200]
loss: 0.611687  [    0/ 3200]
loss: 0.746362  [ 1600/ 3200]
loss: 0.388834  [    0/ 3200]
loss: 0.867602  [ 1600/ 3200]
loss: 0.591963  [    0/ 3200]
loss: 0.823269  [ 1600/ 3200]
loss: 0.804122  [    0/ 3200]
loss: 0.743222  [ 1600/ 3200]
loss: 0.420253  [    0/ 3200]
loss: 0.523997  [ 1600/ 3200]
loss: 0.561770  [    0/ 3200]
loss: 0.728512  [ 1600/ 3200]
loss: 0.661702  [    0/ 3200]
loss: 0.844373  [ 1600/ 3200]
loss: 0.948089  [    0/ 3200]
loss: 0.899895  [ 1600/ 3200]
loss: 0.655066  [    0/ 3200]
loss: 0.891241  [ 1600/ 3200]
loss: 0.808796  [    0/ 3200]
loss: 0.911631  [ 1600/ 3200]
loss: 0.717793  [    0/ 3200]
loss: 0.523303  [ 1600/ 3200]
loss: 0.803909  [    0/ 3200]
loss: 0.848269  [ 1600/ 3200]
loss: 0.466356  [    0/ 3200]
loss: 0.765942  [ 1600/ 3200]
loss: 0.875863  [    0/ 3200]
loss: 0.636225  [ 1600/ 3200]
loss: 0.901593  [    0/ 3200]
loss: 0.730374  [ 1600/ 3200]
loss: 0.945276  [    0/ 3200]
loss: 0.335862  [ 1600/ 3200]
loss: 0.743817  [    0/ 3200]
loss: 0.613224  [ 1600/ 3200]
loss: 0.830873  [    0/ 3200]
loss: 0.827831  [ 1600/ 3200]
loss: 0.764890  [    0/ 3200]
loss: 0.896935  [ 1600/ 3200]
loss: 0.690933  [    0/ 3200]
loss: 0.748665  [ 1600/ 3200]
loss: 0.762695  [    0/ 3200]
loss: 0.499614  [ 1600/ 3200]
loss: 0.680258  [    0/ 3200]
loss: 0.500070  [ 1600/ 3200]
loss: 0.696762  [    0/ 3200]
loss: 0.783601  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0550
F1 Score (macro-averaged): 0.6527
Accuracy: 64.9709
Confusion Matrix:
[[190  57  34  43]
 [ 17 274   1   5]
 [ 94  17 229  16]
 [120  54  24 201]]
Epoch 24
-------------------------------
loss: 0.922341  [    0/ 3200]
loss: 0.595747  [ 1600/ 3200]
loss: 1.189329  [    0/ 3200]
loss: 0.632271  [ 1600/ 3200]
loss: 0.675053  [    0/ 3200]
loss: 0.352174  [ 1600/ 3200]
loss: 0.984610  [    0/ 3200]
loss: 0.666910  [ 1600/ 3200]
loss: 0.620100  [    0/ 3200]
loss: 0.444827  [ 1600/ 3200]
loss: 0.487114  [    0/ 3200]
loss: 0.436716  [ 1600/ 3200]
loss: 0.727986  [    0/ 3200]
loss: 0.618565  [ 1600/ 3200]
loss: 0.970791  [    0/ 3200]
loss: 1.034962  [ 1600/ 3200]
loss: 0.507520  [    0/ 3200]
loss: 0.559412  [ 1600/ 3200]
loss: 0.612484  [    0/ 3200]
loss: 0.839007  [ 1600/ 3200]
loss: 0.553938  [    0/ 3200]
loss: 0.341535  [ 1600/ 3200]
loss: 1.099761  [    0/ 3200]
loss: 0.429799  [ 1600/ 3200]
loss: 0.414839  [    0/ 3200]
loss: 0.547483  [ 1600/ 3200]
loss: 0.682451  [    0/ 3200]
loss: 0.713904  [ 1600/ 3200]
loss: 0.573964  [    0/ 3200]
loss: 0.821421  [ 1600/ 3200]
loss: 0.554612  [    0/ 3200]
loss: 0.635225  [ 1600/ 3200]
loss: 0.606948  [    0/ 3200]
loss: 0.581439  [ 1600/ 3200]
loss: 0.861823  [    0/ 3200]
loss: 0.628985  [ 1600/ 3200]
loss: 0.472027  [    0/ 3200]
loss: 0.883986  [ 1600/ 3200]
loss: 1.195119  [    0/ 3200]
loss: 0.497731  [ 1600/ 3200]
loss: 0.532353  [    0/ 3200]
loss: 0.510502  [ 1600/ 3200]
loss: 0.669886  [    0/ 3200]
loss: 0.703553  [ 1600/ 3200]
loss: 0.726320  [    0/ 3200]
loss: 0.426009  [ 1600/ 3200]
loss: 0.591262  [    0/ 3200]
loss: 0.847072  [ 1600/ 3200]
loss: 0.691033  [    0/ 3200]
loss: 0.687500  [ 1600/ 3200]
loss: 0.633787  [    0/ 3200]
loss: 0.927231  [ 1600/ 3200]
loss: 0.687992  [    0/ 3200]
loss: 0.721604  [ 1600/ 3200]
loss: 0.671325  [    0/ 3200]
loss: 1.163334  [ 1600/ 3200]
loss: 0.723329  [    0/ 3200]
loss: 0.434904  [ 1600/ 3200]
loss: 0.686599  [    0/ 3200]
loss: 0.548702  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0506
F1 Score (macro-averaged): 0.6831
Accuracy: 68.6047
Confusion Matrix:
[[162  46  44  72]
 [ 13 272   1  11]
 [ 64  20 255  17]
 [ 68  45  31 255]]
Epoch 25
-------------------------------
loss: 0.587275  [    0/ 3200]
loss: 0.672590  [ 1600/ 3200]
loss: 1.023957  [    0/ 3200]
loss: 0.857507  [ 1600/ 3200]
loss: 0.754328  [    0/ 3200]
loss: 0.398204  [ 1600/ 3200]
loss: 0.538585  [    0/ 3200]
loss: 0.861497  [ 1600/ 3200]
loss: 0.680069  [    0/ 3200]
loss: 0.438666  [ 1600/ 3200]
loss: 0.444984  [    0/ 3200]
loss: 0.490674  [ 1600/ 3200]
loss: 0.459113  [    0/ 3200]
loss: 1.043982  [ 1600/ 3200]
loss: 0.564363  [    0/ 3200]
loss: 0.784578  [ 1600/ 3200]
loss: 0.404087  [    0/ 3200]
loss: 0.510847  [ 1600/ 3200]
loss: 0.560531  [    0/ 3200]
loss: 0.437193  [ 1600/ 3200]
loss: 0.794871  [    0/ 3200]
loss: 0.782643  [ 1600/ 3200]
loss: 0.938710  [    0/ 3200]
loss: 0.777238  [ 1600/ 3200]
loss: 0.532564  [    0/ 3200]
loss: 0.461989  [ 1600/ 3200]
loss: 0.496641  [    0/ 3200]
loss: 0.684319  [ 1600/ 3200]
loss: 0.783847  [    0/ 3200]
loss: 0.866884  [ 1600/ 3200]
loss: 0.519770  [    0/ 3200]
loss: 0.670683  [ 1600/ 3200]
loss: 0.724683  [    0/ 3200]
loss: 0.424611  [ 1600/ 3200]
loss: 0.818165  [    0/ 3200]
loss: 0.684339  [ 1600/ 3200]
loss: 0.452037  [    0/ 3200]
loss: 0.501885  [ 1600/ 3200]
loss: 0.790095  [    0/ 3200]
loss: 0.600958  [ 1600/ 3200]
loss: 0.309689  [    0/ 3200]
loss: 0.761018  [ 1600/ 3200]
loss: 0.659064  [    0/ 3200]
loss: 0.581963  [ 1600/ 3200]
loss: 1.158711  [    0/ 3200]
loss: 0.375131  [ 1600/ 3200]
loss: 0.491659  [    0/ 3200]
loss: 0.682951  [ 1600/ 3200]
loss: 0.725446  [    0/ 3200]
loss: 0.476112  [ 1600/ 3200]
loss: 0.649609  [    0/ 3200]
loss: 0.666176  [ 1600/ 3200]
loss: 0.770580  [    0/ 3200]
loss: 0.855744  [ 1600/ 3200]
loss: 0.646154  [    0/ 3200]
loss: 0.530805  [ 1600/ 3200]
loss: 0.815224  [    0/ 3200]
loss: 0.811026  [ 1600/ 3200]
loss: 0.500314  [    0/ 3200]
loss: 0.611416  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0496
F1 Score (macro-averaged): 0.6669
Accuracy: 68.6047
Confusion Matrix:
[[ 99  24  86 115]
 [ 17 242  10  28]
 [ 20  12 301  23]
 [ 25  17  55 302]]
Epoch 26
-------------------------------
loss: 0.681398  [    0/ 3200]
loss: 0.712973  [ 1600/ 3200]
loss: 0.577165  [    0/ 3200]
loss: 0.352795  [ 1600/ 3200]
loss: 0.693913  [    0/ 3200]
loss: 0.490641  [ 1600/ 3200]
loss: 0.777594  [    0/ 3200]
loss: 0.505678  [ 1600/ 3200]
loss: 0.677148  [    0/ 3200]
loss: 0.589229  [ 1600/ 3200]
loss: 1.052553  [    0/ 3200]
loss: 0.911130  [ 1600/ 3200]
loss: 1.057236  [    0/ 3200]
loss: 0.603283  [ 1600/ 3200]
loss: 0.676316  [    0/ 3200]
loss: 1.018046  [ 1600/ 3200]
loss: 0.396295  [    0/ 3200]
loss: 0.834656  [ 1600/ 3200]
loss: 0.697437  [    0/ 3200]
loss: 0.676943  [ 1600/ 3200]
loss: 0.614339  [    0/ 3200]
loss: 0.466402  [ 1600/ 3200]
loss: 0.504168  [    0/ 3200]
loss: 0.537088  [ 1600/ 3200]
loss: 1.058640  [    0/ 3200]
loss: 0.457086  [ 1600/ 3200]
loss: 0.597065  [    0/ 3200]
loss: 0.928951  [ 1600/ 3200]
loss: 0.763616  [    0/ 3200]
loss: 0.675873  [ 1600/ 3200]
loss: 1.077720  [    0/ 3200]
loss: 0.985320  [ 1600/ 3200]
loss: 0.971526  [    0/ 3200]
loss: 0.498474  [ 1600/ 3200]
loss: 0.684719  [    0/ 3200]
loss: 1.462320  [ 1600/ 3200]
loss: 0.519675  [    0/ 3200]
loss: 0.391589  [ 1600/ 3200]
loss: 0.728059  [    0/ 3200]
loss: 0.586604  [ 1600/ 3200]
loss: 0.644782  [    0/ 3200]
loss: 0.503233  [ 1600/ 3200]
loss: 0.687976  [    0/ 3200]
loss: 0.712111  [ 1600/ 3200]
loss: 0.663424  [    0/ 3200]
loss: 0.712059  [ 1600/ 3200]
loss: 0.868416  [    0/ 3200]
loss: 0.717309  [ 1600/ 3200]
loss: 0.383236  [    0/ 3200]
loss: 0.760537  [ 1600/ 3200]
loss: 0.696305  [    0/ 3200]
loss: 0.847571  [ 1600/ 3200]
loss: 0.562779  [    0/ 3200]
loss: 0.462393  [ 1600/ 3200]
loss: 1.004074  [    0/ 3200]
loss: 0.554678  [ 1600/ 3200]
loss: 0.309690  [    0/ 3200]
loss: 0.678683  [ 1600/ 3200]
loss: 0.717532  [    0/ 3200]
loss: 0.531868  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0485
F1 Score (macro-averaged): 0.6778
Accuracy: 69.2587
Confusion Matrix:
[[109  32  60 123]
 [ 21 251   3  22]
 [ 32  10 275  39]
 [ 28  15  38 318]]
Epoch 27
-------------------------------
loss: 0.812201  [    0/ 3200]
loss: 1.036929  [ 1600/ 3200]
loss: 0.564762  [    0/ 3200]
loss: 0.951259  [ 1600/ 3200]
loss: 0.642585  [    0/ 3200]
loss: 0.651905  [ 1600/ 3200]
loss: 0.804135  [    0/ 3200]
loss: 0.560458  [ 1600/ 3200]
loss: 0.800837  [    0/ 3200]
loss: 0.591671  [ 1600/ 3200]
loss: 0.292581  [    0/ 3200]
loss: 0.468953  [ 1600/ 3200]
loss: 0.863257  [    0/ 3200]
loss: 0.715134  [ 1600/ 3200]
loss: 0.731723  [    0/ 3200]
loss: 0.760078  [ 1600/ 3200]
loss: 0.720783  [    0/ 3200]
loss: 0.513966  [ 1600/ 3200]
loss: 0.957177  [    0/ 3200]
loss: 0.572766  [ 1600/ 3200]
loss: 0.875851  [    0/ 3200]
loss: 0.846950  [ 1600/ 3200]
loss: 0.860834  [    0/ 3200]
loss: 0.681304  [ 1600/ 3200]
loss: 0.618113  [    0/ 3200]
loss: 0.403859  [ 1600/ 3200]
loss: 0.609800  [    0/ 3200]
loss: 0.328403  [ 1600/ 3200]
loss: 0.763865  [    0/ 3200]
loss: 0.655399  [ 1600/ 3200]
loss: 0.394104  [    0/ 3200]
loss: 0.469628  [ 1600/ 3200]
loss: 0.884364  [    0/ 3200]
loss: 0.657755  [ 1600/ 3200]
loss: 0.576396  [    0/ 3200]
loss: 0.946851  [ 1600/ 3200]
loss: 0.830178  [    0/ 3200]
loss: 0.762869  [ 1600/ 3200]
loss: 0.614480  [    0/ 3200]
loss: 0.902805  [ 1600/ 3200]
loss: 0.682843  [    0/ 3200]
loss: 0.742387  [ 1600/ 3200]
loss: 0.798780  [    0/ 3200]
loss: 0.442813  [ 1600/ 3200]
loss: 0.669557  [    0/ 3200]
loss: 0.640464  [ 1600/ 3200]
loss: 0.644433  [    0/ 3200]
loss: 0.799384  [ 1600/ 3200]
loss: 0.859497  [    0/ 3200]
loss: 0.934912  [ 1600/ 3200]
loss: 0.645648  [    0/ 3200]
loss: 0.574507  [ 1600/ 3200]
loss: 0.727411  [    0/ 3200]
loss: 0.717708  [ 1600/ 3200]
loss: 0.608997  [    0/ 3200]
loss: 0.440564  [ 1600/ 3200]
loss: 0.754496  [    0/ 3200]
loss: 0.601008  [ 1600/ 3200]
loss: 0.856988  [    0/ 3200]
loss: 0.542348  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0479
F1 Score (macro-averaged): 0.6897
Accuracy: 69.6221
Confusion Matrix:
[[133  22  55 114]
 [ 26 242   2  27]
 [ 39  10 281  26]
 [ 40  15  42 302]]
Epoch 28
-------------------------------
loss: 0.562416  [    0/ 3200]
loss: 0.539046  [ 1600/ 3200]
loss: 0.762583  [    0/ 3200]
loss: 0.480436  [ 1600/ 3200]
loss: 0.755408  [    0/ 3200]
loss: 1.436178  [ 1600/ 3200]
loss: 0.597255  [    0/ 3200]
loss: 0.436142  [ 1600/ 3200]
loss: 0.837982  [    0/ 3200]
loss: 0.511766  [ 1600/ 3200]
loss: 0.502978  [    0/ 3200]
loss: 0.720856  [ 1600/ 3200]
loss: 0.636252  [    0/ 3200]
loss: 0.553212  [ 1600/ 3200]
loss: 0.472370  [    0/ 3200]
loss: 0.911291  [ 1600/ 3200]
loss: 0.708562  [    0/ 3200]
loss: 0.584842  [ 1600/ 3200]
loss: 0.787169  [    0/ 3200]
loss: 1.276227  [ 1600/ 3200]
loss: 0.649387  [    0/ 3200]
loss: 0.415807  [ 1600/ 3200]
loss: 0.631418  [    0/ 3200]
loss: 0.541566  [ 1600/ 3200]
loss: 0.449063  [    0/ 3200]
loss: 0.658716  [ 1600/ 3200]
loss: 0.790119  [    0/ 3200]
loss: 0.890743  [ 1600/ 3200]
loss: 0.512363  [    0/ 3200]
loss: 0.557580  [ 1600/ 3200]
loss: 0.544258  [    0/ 3200]
loss: 0.433976  [ 1600/ 3200]
loss: 0.548529  [    0/ 3200]
loss: 0.755868  [ 1600/ 3200]
loss: 0.660722  [    0/ 3200]
loss: 0.597218  [ 1600/ 3200]
loss: 0.595561  [    0/ 3200]
loss: 0.535186  [ 1600/ 3200]
loss: 0.719971  [    0/ 3200]
loss: 0.839083  [ 1600/ 3200]
loss: 0.480826  [    0/ 3200]
loss: 0.756755  [ 1600/ 3200]
loss: 0.350340  [    0/ 3200]
loss: 0.807252  [ 1600/ 3200]
loss: 0.627349  [    0/ 3200]
loss: 0.479199  [ 1600/ 3200]
loss: 0.647500  [    0/ 3200]
loss: 1.333974  [ 1600/ 3200]
loss: 0.810190  [    0/ 3200]
loss: 0.914570  [ 1600/ 3200]
loss: 1.373431  [    0/ 3200]
loss: 0.454723  [ 1600/ 3200]
loss: 0.582466  [    0/ 3200]
loss: 0.579639  [ 1600/ 3200]
loss: 0.431932  [    0/ 3200]
loss: 0.474715  [ 1600/ 3200]
loss: 1.270246  [    0/ 3200]
loss: 0.623193  [ 1600/ 3200]
loss: 0.733221  [    0/ 3200]
loss: 0.498761  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0485
F1 Score (macro-averaged): 0.6739
Accuracy: 68.3140
Confusion Matrix:
[[125  26  98  75]
 [ 35 239   9  14]
 [ 26   8 308  14]
 [ 52  15  64 268]]
Epoch 29
-------------------------------
loss: 0.643519  [    0/ 3200]
loss: 0.655354  [ 1600/ 3200]
loss: 1.110967  [    0/ 3200]
loss: 0.586691  [ 1600/ 3200]
loss: 0.511342  [    0/ 3200]
loss: 0.677399  [ 1600/ 3200]
loss: 0.490605  [    0/ 3200]
loss: 0.532156  [ 1600/ 3200]
loss: 0.293808  [    0/ 3200]
loss: 0.871666  [ 1600/ 3200]
loss: 1.020719  [    0/ 3200]
loss: 0.704602  [ 1600/ 3200]
loss: 0.343535  [    0/ 3200]
loss: 0.350273  [ 1600/ 3200]
loss: 0.611681  [    0/ 3200]
loss: 0.873935  [ 1600/ 3200]
loss: 1.251908  [    0/ 3200]
loss: 0.790579  [ 1600/ 3200]
loss: 0.483629  [    0/ 3200]
loss: 0.561417  [ 1600/ 3200]
loss: 0.609954  [    0/ 3200]
loss: 0.436855  [ 1600/ 3200]
loss: 0.750674  [    0/ 3200]
loss: 1.016002  [ 1600/ 3200]
loss: 0.460577  [    0/ 3200]
loss: 0.544859  [ 1600/ 3200]
loss: 0.865179  [    0/ 3200]
loss: 0.560508  [ 1600/ 3200]
loss: 0.767730  [    0/ 3200]
loss: 0.875175  [ 1600/ 3200]
loss: 0.699823  [    0/ 3200]
loss: 0.821819  [ 1600/ 3200]
loss: 0.464908  [    0/ 3200]
loss: 0.546533  [ 1600/ 3200]
loss: 0.480163  [    0/ 3200]
loss: 1.046234  [ 1600/ 3200]
loss: 0.802887  [    0/ 3200]
loss: 0.591752  [ 1600/ 3200]
loss: 0.389775  [    0/ 3200]
loss: 0.612309  [ 1600/ 3200]
loss: 0.520215  [    0/ 3200]
loss: 1.089953  [ 1600/ 3200]
loss: 0.804609  [    0/ 3200]
loss: 0.465109  [ 1600/ 3200]
loss: 0.636455  [    0/ 3200]
loss: 0.567746  [ 1600/ 3200]
loss: 0.684653  [    0/ 3200]
loss: 0.414186  [ 1600/ 3200]
loss: 0.190766  [    0/ 3200]
loss: 0.692367  [ 1600/ 3200]
loss: 1.053139  [    0/ 3200]
loss: 0.768711  [ 1600/ 3200]
loss: 0.649619  [    0/ 3200]
loss: 0.599746  [ 1600/ 3200]
loss: 0.730466  [    0/ 3200]
loss: 0.601294  [ 1600/ 3200]
loss: 0.662759  [    0/ 3200]
loss: 0.548230  [ 1600/ 3200]
loss: 0.497381  [    0/ 3200]
loss: 0.519933  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0462
F1 Score (macro-averaged): 0.7045
Accuracy: 71.1483
Confusion Matrix:
[[139  29  64  92]
 [ 23 260   2  12]
 [ 35   9 292  20]
 [ 50  15  46 288]]
Epoch 30
-------------------------------
loss: 0.581897  [    0/ 3200]
loss: 1.076555  [ 1600/ 3200]
loss: 0.818549  [    0/ 3200]
loss: 0.649274  [ 1600/ 3200]
loss: 1.115091  [    0/ 3200]
loss: 0.968333  [ 1600/ 3200]
loss: 0.574832  [    0/ 3200]
loss: 0.466290  [ 1600/ 3200]
loss: 0.795489  [    0/ 3200]
loss: 1.170809  [ 1600/ 3200]
loss: 0.305581  [    0/ 3200]
loss: 0.485034  [ 1600/ 3200]
loss: 0.648906  [    0/ 3200]
loss: 0.745450  [ 1600/ 3200]
loss: 1.169244  [    0/ 3200]
loss: 0.602624  [ 1600/ 3200]
loss: 0.573852  [    0/ 3200]
loss: 1.027466  [ 1600/ 3200]
loss: 0.508697  [    0/ 3200]
loss: 0.331121  [ 1600/ 3200]
loss: 0.460960  [    0/ 3200]
loss: 0.945963  [ 1600/ 3200]
loss: 1.219481  [    0/ 3200]
loss: 0.663986  [ 1600/ 3200]
loss: 0.564370  [    0/ 3200]
loss: 0.910118  [ 1600/ 3200]
loss: 0.963629  [    0/ 3200]
loss: 0.675652  [ 1600/ 3200]
loss: 0.758951  [    0/ 3200]
loss: 0.490530  [ 1600/ 3200]
loss: 0.779878  [    0/ 3200]
loss: 0.509818  [ 1600/ 3200]
loss: 0.636275  [    0/ 3200]
loss: 0.599661  [ 1600/ 3200]
loss: 0.689840  [    0/ 3200]
loss: 0.705575  [ 1600/ 3200]
loss: 0.419266  [    0/ 3200]
loss: 0.466832  [ 1600/ 3200]
loss: 0.533287  [    0/ 3200]
loss: 0.624341  [ 1600/ 3200]
loss: 0.645774  [    0/ 3200]
loss: 0.540163  [ 1600/ 3200]
loss: 0.630383  [    0/ 3200]
loss: 1.079049  [ 1600/ 3200]
loss: 0.764066  [    0/ 3200]
loss: 0.571483  [ 1600/ 3200]
loss: 0.870695  [    0/ 3200]
loss: 0.598832  [ 1600/ 3200]
loss: 0.498560  [    0/ 3200]
loss: 0.454279  [ 1600/ 3200]
loss: 0.516020  [    0/ 3200]
loss: 0.933423  [ 1600/ 3200]
loss: 0.612798  [    0/ 3200]
loss: 0.660441  [ 1600/ 3200]
loss: 0.652054  [    0/ 3200]
loss: 1.134640  [ 1600/ 3200]
loss: 1.068224  [    0/ 3200]
loss: 0.652826  [ 1600/ 3200]
loss: 0.956195  [    0/ 3200]
loss: 0.566217  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0469
F1 Score (macro-averaged): 0.7107
Accuracy: 70.7849
Confusion Matrix:
[[180  18  46  80]
 [ 36 240   2  19]
 [ 55   7 273  21]
 [ 63  16  39 281]]
Time taken for epoch: 120.75 seconds

print(device)
cpu

Γραφήματα επιδόσεων

import matplotlib.pyplot as plt

def plot_results(loss_values, accuracy_values, f1_values):
    epochs = range(1, len(loss_values) + 1)

    # Create subplots with 1 row and 3 columns
    fig, axs = plt.subplots(1, 3, figsize=(16, 4))

    # Plot loss
    axs[0].plot(epochs, loss_values, label='Loss')
    axs[0].set_title('Training Loss over Epochs')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].legend()

    # Plot accuracy
    axs[1].plot(epochs, accuracy_values, label='Accuracy')
    axs[1].set_title('Training Accuracy over Epochs')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel('Accuracy')
    axs[1].legend()

    # Plot F1 score
    axs[2].plot(epochs, f1_values, label='F1 Score')
    axs[2].set_title('Training F1 Score (Macro-averaged) over Epochs')
    axs[2].set_xlabel('Epoch')
    axs[2].set_ylabel('F1 Score')
    axs[2].legend()

def plot_confusion_matrix(confusion_mat):
    plt.figure(figsize=(5, 3))
    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # Adjust the spacing between subplots
    plt.tight_layout()

    # Display the plots
    plt.show()

# Plot the results
plot_results(loss_values, accuracy_values, f1_values)


Βέπουμε την αναμενόμενη διακύμανση σε όλα τα διαγράμματα με το loss να μειώνεται μέχρι τις τελευταίες εποχές π αρχίζει να σταθεροποιείται και αναμένουμε ίσως αν συνεχιζόταν η εκπαίδευση να άρχιζε να αυξάνεται λόγω over-fitting. Το accuracy αυξάνεται και φτάνει το peak στο 70% στις 26 εποχές.Όμοια το f1 score έχει αυξυτική τάση όπως είναι το επιθυμιτό.

Βήμα 6: Εκπαίδευση δικτύου με GPU

Επαναλάβετε το βήμα 5, αλλά αυτή την φορά να έχετε αρχικά μεταφέρει τα δεδομένα και το αρχικοποιημένο νευρωνικό σας δίκτυο στην GPU του colab. Βεβαιωθείτε ότι η εκπαίδευση τρέχει στην GPU και τυπώστε τις διαφορές στους χρόνους εκτέλεσης σε GPU και CPU. Βεβαιωθείτε ότι το colab session σας περιλαμβάνει χρήση GPU - η οποία είναι δωρεάν.

import time

# Optimizer, learning rate, loss function, and number of epochs
optimizer = torch.optim.SGD(model.parameters(), lr=0.002)
loss_fn = nn.CrossEntropyLoss()
num_epochs = 30

loss_values = []
accuracy_values = []
f1_values = []

# Start the timer
start_time = time.time()

# Train the model
for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}\n-------------------------------")



    # Move the model to GPU if available
    model = model.to(device)

    # Train
    train_loop(train_loader, model, loss_fn, optimizer, num_epochs)

    # Evaluate on the test set
    test_loss, test_f1, test_acc, test_cm = test_loop(test_loader, model, loss_fn)

    # Append the values to the respective lists
    loss_values.append(test_loss)
    accuracy_values.append(test_acc)
    f1_values.append(test_f1)



    # Print the evaluation scores and time taken
    print("Test Set Evaluation:")
    print(f"Loss: {test_loss:.4f}")
    print(f"F1 Score (macro-averaged): {test_f1:.4f}")
    print(f"Accuracy: {100*test_acc:.4f}")
    print("Confusion Matrix:")
    print(test_cm)

# Stop the timer
end_time = time.time()
epoch_time = end_time - start_time

print(f"Time taken for epoch: {epoch_time:.2f} seconds")
Epoch 1
-------------------------------
loss: 1.328725  [    0/ 3200]
loss: 1.391737  [ 1600/ 3200]
loss: 1.359673  [    0/ 3200]
loss: 1.386362  [ 1600/ 3200]
loss: 1.378098  [    0/ 3200]
loss: 1.387357  [ 1600/ 3200]
loss: 1.342674  [    0/ 3200]
loss: 1.338023  [ 1600/ 3200]
loss: 1.357052  [    0/ 3200]
loss: 1.329773  [ 1600/ 3200]
loss: 1.343483  [    0/ 3200]
loss: 1.354795  [ 1600/ 3200]
loss: 1.326511  [    0/ 3200]
loss: 1.348282  [ 1600/ 3200]
loss: 1.345121  [    0/ 3200]
loss: 1.302425  [ 1600/ 3200]
loss: 1.263883  [    0/ 3200]
loss: 1.332614  [ 1600/ 3200]
loss: 1.276047  [    0/ 3200]
loss: 1.304974  [ 1600/ 3200]
loss: 1.332779  [    0/ 3200]
loss: 1.242991  [ 1600/ 3200]
loss: 1.155364  [    0/ 3200]
loss: 1.210077  [ 1600/ 3200]
loss: 1.249216  [    0/ 3200]
loss: 1.189475  [ 1600/ 3200]
loss: 1.238851  [    0/ 3200]
loss: 1.217324  [ 1600/ 3200]
loss: 1.182285  [    0/ 3200]
loss: 1.139026  [ 1600/ 3200]
loss: 1.285333  [    0/ 3200]
loss: 1.222085  [ 1600/ 3200]
loss: 1.138510  [    0/ 3200]
loss: 1.125074  [ 1600/ 3200]
loss: 1.047378  [    0/ 3200]
loss: 1.140186  [ 1600/ 3200]
loss: 1.021631  [    0/ 3200]
loss: 1.077258  [ 1600/ 3200]
loss: 1.112249  [    0/ 3200]
loss: 1.121448  [ 1600/ 3200]
loss: 1.185406  [    0/ 3200]
loss: 1.097887  [ 1600/ 3200]
loss: 1.104422  [    0/ 3200]
loss: 0.986502  [ 1600/ 3200]
loss: 1.026701  [    0/ 3200]
loss: 1.358036  [ 1600/ 3200]
loss: 0.772682  [    0/ 3200]
loss: 0.987830  [ 1600/ 3200]
loss: 1.296486  [    0/ 3200]
loss: 0.990027  [ 1600/ 3200]
loss: 1.049971  [    0/ 3200]
loss: 1.101538  [ 1600/ 3200]
loss: 1.211646  [    0/ 3200]
loss: 0.863537  [ 1600/ 3200]
loss: 1.013237  [    0/ 3200]
loss: 0.924941  [ 1600/ 3200]
loss: 1.024362  [    0/ 3200]
loss: 1.055586  [ 1600/ 3200]
loss: 1.135469  [    0/ 3200]
loss: 0.743609  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0639
F1 Score (macro-averaged): 0.5775
Accuracy: 57.8488
Confusion Matrix:
[[136  56  47  85]
 [ 27 261   5   4]
 [119  30 186  21]
 [ 84  67  35 213]]
Epoch 2
-------------------------------
loss: 0.854588  [    0/ 3200]
loss: 1.110934  [ 1600/ 3200]
loss: 1.187240  [    0/ 3200]
loss: 0.905366  [ 1600/ 3200]
loss: 1.000537  [    0/ 3200]
loss: 1.065314  [ 1600/ 3200]
loss: 1.357996  [    0/ 3200]
loss: 0.757020  [ 1600/ 3200]
loss: 0.847214  [    0/ 3200]
loss: 0.871244  [ 1600/ 3200]
loss: 1.088275  [    0/ 3200]
loss: 0.948249  [ 1600/ 3200]
loss: 1.051978  [    0/ 3200]
loss: 1.233584  [ 1600/ 3200]
loss: 0.877192  [    0/ 3200]
loss: 0.910655  [ 1600/ 3200]
loss: 0.963882  [    0/ 3200]
loss: 0.877311  [ 1600/ 3200]
loss: 0.977303  [    0/ 3200]
loss: 0.678583  [ 1600/ 3200]
loss: 1.169634  [    0/ 3200]
loss: 0.797476  [ 1600/ 3200]
loss: 0.993405  [    0/ 3200]
loss: 1.063175  [ 1600/ 3200]
loss: 0.714605  [    0/ 3200]
loss: 0.847144  [ 1600/ 3200]
loss: 0.724365  [    0/ 3200]
loss: 0.932324  [ 1600/ 3200]
loss: 1.008364  [    0/ 3200]
loss: 1.197885  [ 1600/ 3200]
loss: 0.878508  [    0/ 3200]
loss: 0.998224  [ 1600/ 3200]
loss: 1.170796  [    0/ 3200]
loss: 0.773661  [ 1600/ 3200]
loss: 0.769651  [    0/ 3200]
loss: 0.705627  [ 1600/ 3200]
loss: 0.973890  [    0/ 3200]
loss: 1.190637  [ 1600/ 3200]
loss: 0.914604  [    0/ 3200]
loss: 1.037485  [ 1600/ 3200]
loss: 0.876509  [    0/ 3200]
loss: 0.902294  [ 1600/ 3200]
loss: 0.787176  [    0/ 3200]
loss: 0.835687  [ 1600/ 3200]
loss: 1.380353  [    0/ 3200]
loss: 0.967562  [ 1600/ 3200]
loss: 0.958659  [    0/ 3200]
loss: 0.790574  [ 1600/ 3200]
loss: 0.637546  [    0/ 3200]
loss: 1.155482  [ 1600/ 3200]
loss: 1.066320  [    0/ 3200]
loss: 0.630255  [ 1600/ 3200]
loss: 0.824000  [    0/ 3200]
loss: 1.107213  [ 1600/ 3200]
loss: 0.597265  [    0/ 3200]
loss: 0.972674  [ 1600/ 3200]
loss: 0.837387  [    0/ 3200]
loss: 0.817496  [ 1600/ 3200]
loss: 0.711398  [    0/ 3200]
loss: 0.864992  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0569
F1 Score (macro-averaged): 0.6217
Accuracy: 63.8808
Confusion Matrix:
[[ 89  35  64 136]
 [ 24 248   5  20]
 [ 62  16 244  34]
 [ 30  35  36 298]]
Epoch 3
-------------------------------
loss: 0.805804  [    0/ 3200]
loss: 0.796671  [ 1600/ 3200]
loss: 0.707293  [    0/ 3200]
loss: 1.114302  [ 1600/ 3200]
loss: 0.720984  [    0/ 3200]
loss: 0.944992  [ 1600/ 3200]
loss: 1.049969  [    0/ 3200]
loss: 1.051478  [ 1600/ 3200]
loss: 0.735273  [    0/ 3200]
loss: 0.932588  [ 1600/ 3200]
loss: 0.967542  [    0/ 3200]
loss: 0.959186  [ 1600/ 3200]
loss: 0.846833  [    0/ 3200]
loss: 0.880617  [ 1600/ 3200]
loss: 1.383269  [    0/ 3200]
loss: 0.870470  [ 1600/ 3200]
loss: 1.027943  [    0/ 3200]
loss: 1.157562  [ 1600/ 3200]
loss: 0.991800  [    0/ 3200]
loss: 1.021333  [ 1600/ 3200]
loss: 1.053960  [    0/ 3200]
loss: 0.940909  [ 1600/ 3200]
loss: 1.150174  [    0/ 3200]
loss: 0.814595  [ 1600/ 3200]
loss: 1.135230  [    0/ 3200]
loss: 0.754128  [ 1600/ 3200]
loss: 0.793210  [    0/ 3200]
loss: 0.883065  [ 1600/ 3200]
loss: 1.222437  [    0/ 3200]
loss: 0.788295  [ 1600/ 3200]
loss: 1.020703  [    0/ 3200]
loss: 0.982019  [ 1600/ 3200]
loss: 0.749279  [    0/ 3200]
loss: 0.780466  [ 1600/ 3200]
loss: 0.833820  [    0/ 3200]
loss: 1.189501  [ 1600/ 3200]
loss: 0.897233  [    0/ 3200]
loss: 0.511537  [ 1600/ 3200]
loss: 0.532827  [    0/ 3200]
loss: 1.103526  [ 1600/ 3200]
loss: 1.039079  [    0/ 3200]
loss: 0.904920  [ 1600/ 3200]
loss: 0.420155  [    0/ 3200]
loss: 0.784135  [ 1600/ 3200]
loss: 0.911247  [    0/ 3200]
loss: 0.647830  [ 1600/ 3200]
loss: 1.131086  [    0/ 3200]
loss: 0.705492  [ 1600/ 3200]
loss: 0.888233  [    0/ 3200]
loss: 0.926707  [ 1600/ 3200]
loss: 0.449866  [    0/ 3200]
loss: 0.870126  [ 1600/ 3200]
loss: 1.231776  [    0/ 3200]
loss: 0.861274  [ 1600/ 3200]
loss: 0.845178  [    0/ 3200]
loss: 1.016592  [ 1600/ 3200]
loss: 0.892636  [    0/ 3200]
loss: 0.607612  [ 1600/ 3200]
loss: 0.953854  [    0/ 3200]
loss: 0.702214  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0586
F1 Score (macro-averaged): 0.6024
Accuracy: 61.5552
Confusion Matrix:
[[101  71  49 103]
 [ 14 269   2  12]
 [ 91  22 219  24]
 [ 46  63  32 258]]
Epoch 4
-------------------------------
loss: 0.900502  [    0/ 3200]
loss: 0.698147  [ 1600/ 3200]
loss: 1.139269  [    0/ 3200]
loss: 0.612982  [ 1600/ 3200]
loss: 0.626826  [    0/ 3200]
loss: 0.882733  [ 1600/ 3200]
loss: 0.703044  [    0/ 3200]
loss: 1.107943  [ 1600/ 3200]
loss: 0.694027  [    0/ 3200]
loss: 0.860811  [ 1600/ 3200]
loss: 0.925339  [    0/ 3200]
loss: 0.517399  [ 1600/ 3200]
loss: 0.661357  [    0/ 3200]
loss: 0.693751  [ 1600/ 3200]
loss: 0.798506  [    0/ 3200]
loss: 0.928667  [ 1600/ 3200]
loss: 0.967686  [    0/ 3200]
loss: 0.593462  [ 1600/ 3200]
loss: 0.811562  [    0/ 3200]
loss: 1.375637  [ 1600/ 3200]
loss: 1.278398  [    0/ 3200]
loss: 0.979714  [ 1600/ 3200]
loss: 1.212899  [    0/ 3200]
loss: 0.882789  [ 1600/ 3200]
loss: 1.167210  [    0/ 3200]
loss: 0.696129  [ 1600/ 3200]
loss: 0.675949  [    0/ 3200]
loss: 0.646172  [ 1600/ 3200]
loss: 0.549028  [    0/ 3200]
loss: 0.967451  [ 1600/ 3200]
loss: 0.871702  [    0/ 3200]
loss: 1.389603  [ 1600/ 3200]
loss: 0.878157  [    0/ 3200]
loss: 0.776549  [ 1600/ 3200]
loss: 1.124504  [    0/ 3200]
loss: 1.529102  [ 1600/ 3200]
loss: 0.927769  [    0/ 3200]
loss: 0.799917  [ 1600/ 3200]
loss: 0.533841  [    0/ 3200]
loss: 1.145198  [ 1600/ 3200]
loss: 1.070624  [    0/ 3200]
loss: 1.074382  [ 1600/ 3200]
loss: 0.736751  [    0/ 3200]
loss: 0.867910  [ 1600/ 3200]
loss: 0.649972  [    0/ 3200]
loss: 0.784568  [ 1600/ 3200]
loss: 0.905374  [    0/ 3200]
loss: 0.727111  [ 1600/ 3200]
loss: 0.632573  [    0/ 3200]
loss: 0.915758  [ 1600/ 3200]
loss: 0.748224  [    0/ 3200]
loss: 0.761646  [ 1600/ 3200]
loss: 0.885407  [    0/ 3200]
loss: 0.956717  [ 1600/ 3200]
loss: 0.599161  [    0/ 3200]
loss: 0.899125  [ 1600/ 3200]
loss: 0.935799  [    0/ 3200]
loss: 0.628359  [ 1600/ 3200]
loss: 0.845787  [    0/ 3200]
loss: 0.973890  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0540
F1 Score (macro-averaged): 0.6253
Accuracy: 65.2616
Confusion Matrix:
[[ 71  17 101 135]
 [ 33 223   7  34]
 [ 24  11 295  26]
 [ 24  19  47 309]]
Epoch 5
-------------------------------
loss: 0.890946  [    0/ 3200]
loss: 0.867821  [ 1600/ 3200]
loss: 0.744215  [    0/ 3200]
loss: 0.536787  [ 1600/ 3200]
loss: 0.606539  [    0/ 3200]
loss: 0.951061  [ 1600/ 3200]
loss: 0.582130  [    0/ 3200]
loss: 0.693127  [ 1600/ 3200]
loss: 0.748084  [    0/ 3200]
loss: 0.912388  [ 1600/ 3200]
loss: 0.841814  [    0/ 3200]
loss: 0.807669  [ 1600/ 3200]
loss: 0.874258  [    0/ 3200]
loss: 1.270357  [ 1600/ 3200]
loss: 0.920691  [    0/ 3200]
loss: 0.649176  [ 1600/ 3200]
loss: 0.827495  [    0/ 3200]
loss: 0.880932  [ 1600/ 3200]
loss: 0.735489  [    0/ 3200]
loss: 0.840386  [ 1600/ 3200]
loss: 0.719832  [    0/ 3200]
loss: 0.483011  [ 1600/ 3200]
loss: 0.735941  [    0/ 3200]
loss: 0.614118  [ 1600/ 3200]
loss: 0.856251  [    0/ 3200]
loss: 0.997290  [ 1600/ 3200]
loss: 0.878315  [    0/ 3200]
loss: 0.925237  [ 1600/ 3200]
loss: 0.994764  [    0/ 3200]
loss: 0.989817  [ 1600/ 3200]
loss: 0.709131  [    0/ 3200]
loss: 0.902013  [ 1600/ 3200]
loss: 0.948630  [    0/ 3200]
loss: 0.837161  [ 1600/ 3200]
loss: 1.379650  [    0/ 3200]
loss: 0.559760  [ 1600/ 3200]
loss: 0.692792  [    0/ 3200]
loss: 0.669218  [ 1600/ 3200]
loss: 1.116564  [    0/ 3200]
loss: 0.826508  [ 1600/ 3200]
loss: 0.430990  [    0/ 3200]
loss: 0.762443  [ 1600/ 3200]
loss: 0.929273  [    0/ 3200]
loss: 0.904948  [ 1600/ 3200]
loss: 0.782446  [    0/ 3200]
loss: 0.532516  [ 1600/ 3200]
loss: 0.578668  [    0/ 3200]
loss: 0.600593  [ 1600/ 3200]
loss: 0.918635  [    0/ 3200]
loss: 0.896455  [ 1600/ 3200]
loss: 0.895691  [    0/ 3200]
loss: 0.568875  [ 1600/ 3200]
loss: 0.486277  [    0/ 3200]
loss: 0.654943  [ 1600/ 3200]
loss: 0.857309  [    0/ 3200]
loss: 0.928225  [ 1600/ 3200]
loss: 1.101631  [    0/ 3200]
loss: 0.843084  [ 1600/ 3200]
loss: 0.591705  [    0/ 3200]
loss: 0.881342  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0535
F1 Score (macro-averaged): 0.6405
Accuracy: 65.4797
Confusion Matrix:
[[ 98  22  64 140]
 [ 36 223   2  36]
 [ 43  12 265  36]
 [ 30  19  35 315]]
Epoch 6
-------------------------------
loss: 0.902239  [    0/ 3200]
loss: 0.742267  [ 1600/ 3200]
loss: 0.696778  [    0/ 3200]
loss: 0.879160  [ 1600/ 3200]
loss: 1.106640  [    0/ 3200]
loss: 0.766962  [ 1600/ 3200]
loss: 0.552378  [    0/ 3200]
loss: 0.754961  [ 1600/ 3200]
loss: 0.620643  [    0/ 3200]
loss: 0.654696  [ 1600/ 3200]
loss: 0.512882  [    0/ 3200]
loss: 0.822508  [ 1600/ 3200]
loss: 0.793954  [    0/ 3200]
loss: 0.750482  [ 1600/ 3200]
loss: 0.943484  [    0/ 3200]
loss: 0.829584  [ 1600/ 3200]
loss: 0.861278  [    0/ 3200]
loss: 0.636201  [ 1600/ 3200]
loss: 0.635620  [    0/ 3200]
loss: 0.776295  [ 1600/ 3200]
loss: 0.628115  [    0/ 3200]
loss: 0.767242  [ 1600/ 3200]
loss: 0.884357  [    0/ 3200]
loss: 1.452790  [ 1600/ 3200]
loss: 0.800496  [    0/ 3200]
loss: 0.694301  [ 1600/ 3200]
loss: 1.033397  [    0/ 3200]
loss: 0.797854  [ 1600/ 3200]
loss: 1.049087  [    0/ 3200]
loss: 0.873004  [ 1600/ 3200]
loss: 0.557725  [    0/ 3200]
loss: 0.783850  [ 1600/ 3200]
loss: 0.909877  [    0/ 3200]
loss: 1.078475  [ 1600/ 3200]
loss: 0.640197  [    0/ 3200]
loss: 0.742645  [ 1600/ 3200]
loss: 0.965598  [    0/ 3200]
loss: 0.736781  [ 1600/ 3200]
loss: 0.513658  [    0/ 3200]
loss: 0.802716  [ 1600/ 3200]
loss: 0.761853  [    0/ 3200]
loss: 1.008430  [ 1600/ 3200]
loss: 0.608495  [    0/ 3200]
loss: 0.810655  [ 1600/ 3200]
loss: 1.168076  [    0/ 3200]
loss: 0.882268  [ 1600/ 3200]
loss: 0.822946  [    0/ 3200]
loss: 0.684732  [ 1600/ 3200]
loss: 0.616782  [    0/ 3200]
loss: 0.798693  [ 1600/ 3200]
loss: 0.762016  [    0/ 3200]
loss: 0.709983  [ 1600/ 3200]
loss: 0.762746  [    0/ 3200]
loss: 1.326549  [ 1600/ 3200]
loss: 0.642691  [    0/ 3200]
loss: 0.826932  [ 1600/ 3200]
loss: 0.474028  [    0/ 3200]
loss: 0.886428  [ 1600/ 3200]
loss: 0.590806  [    0/ 3200]
loss: 0.576054  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0550
F1 Score (macro-averaged): 0.6445
Accuracy: 65.4070
Confusion Matrix:
[[123  57  67  77]
 [ 20 267   2   8]
 [ 61  14 270  11]
 [ 65  51  43 240]]
Epoch 7
-------------------------------
loss: 0.724182  [    0/ 3200]
loss: 0.955410  [ 1600/ 3200]
loss: 0.769418  [    0/ 3200]
loss: 0.907958  [ 1600/ 3200]
loss: 0.689610  [    0/ 3200]
loss: 0.718039  [ 1600/ 3200]
loss: 0.680614  [    0/ 3200]
loss: 1.001667  [ 1600/ 3200]
loss: 0.520077  [    0/ 3200]
loss: 0.447683  [ 1600/ 3200]
loss: 0.654579  [    0/ 3200]
loss: 0.856268  [ 1600/ 3200]
loss: 0.743053  [    0/ 3200]
loss: 0.754748  [ 1600/ 3200]
loss: 0.860325  [    0/ 3200]
loss: 0.752607  [ 1600/ 3200]
loss: 0.954171  [    0/ 3200]
loss: 1.250453  [ 1600/ 3200]
loss: 0.639856  [    0/ 3200]
loss: 0.510907  [ 1600/ 3200]
loss: 0.793377  [    0/ 3200]
loss: 0.671689  [ 1600/ 3200]
loss: 0.567402  [    0/ 3200]
loss: 0.793879  [ 1600/ 3200]
loss: 0.725518  [    0/ 3200]
loss: 0.905855  [ 1600/ 3200]
loss: 0.391630  [    0/ 3200]
loss: 0.872553  [ 1600/ 3200]
loss: 0.872543  [    0/ 3200]
loss: 1.086622  [ 1600/ 3200]
loss: 0.860753  [    0/ 3200]
loss: 0.746260  [ 1600/ 3200]
loss: 0.847578  [    0/ 3200]
loss: 0.846987  [ 1600/ 3200]
loss: 1.022707  [    0/ 3200]
loss: 1.072456  [ 1600/ 3200]
loss: 0.718226  [    0/ 3200]
loss: 0.935157  [ 1600/ 3200]
loss: 0.870957  [    0/ 3200]
loss: 0.697380  [ 1600/ 3200]
loss: 0.703559  [    0/ 3200]
loss: 0.869652  [ 1600/ 3200]
loss: 1.065541  [    0/ 3200]
loss: 0.606782  [ 1600/ 3200]
loss: 0.710812  [    0/ 3200]
loss: 0.684364  [ 1600/ 3200]
loss: 0.529687  [    0/ 3200]
loss: 0.506983  [ 1600/ 3200]
loss: 0.865965  [    0/ 3200]
loss: 0.736403  [ 1600/ 3200]
loss: 0.746131  [    0/ 3200]
loss: 0.718709  [ 1600/ 3200]
loss: 1.063045  [    0/ 3200]
loss: 0.935485  [ 1600/ 3200]
loss: 1.033228  [    0/ 3200]
loss: 0.870696  [ 1600/ 3200]
loss: 1.327093  [    0/ 3200]
loss: 0.786633  [ 1600/ 3200]
loss: 0.683167  [    0/ 3200]
loss: 0.832061  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0531
F1 Score (macro-averaged): 0.6594
Accuracy: 66.5698
Confusion Matrix:
[[131  37  60  96]
 [ 19 258   2  18]
 [ 64  14 259  19]
 [ 57  40  34 268]]
Epoch 8
-------------------------------
loss: 0.791165  [    0/ 3200]
loss: 1.355127  [ 1600/ 3200]
loss: 0.815440  [    0/ 3200]
loss: 0.450606  [ 1600/ 3200]
loss: 1.245595  [    0/ 3200]
loss: 0.954795  [ 1600/ 3200]
loss: 1.067438  [    0/ 3200]
loss: 1.269842  [ 1600/ 3200]
loss: 0.788775  [    0/ 3200]
loss: 0.653315  [ 1600/ 3200]
loss: 0.705138  [    0/ 3200]
loss: 0.752362  [ 1600/ 3200]
loss: 0.945910  [    0/ 3200]
loss: 0.945700  [ 1600/ 3200]
loss: 0.676604  [    0/ 3200]
loss: 0.964491  [ 1600/ 3200]
loss: 1.054898  [    0/ 3200]
loss: 0.898671  [ 1600/ 3200]
loss: 0.555955  [    0/ 3200]
loss: 0.575175  [ 1600/ 3200]
loss: 0.843756  [    0/ 3200]
loss: 0.803161  [ 1600/ 3200]
loss: 0.742556  [    0/ 3200]
loss: 0.783787  [ 1600/ 3200]
loss: 0.854061  [    0/ 3200]
loss: 0.733833  [ 1600/ 3200]
loss: 0.789422  [    0/ 3200]
loss: 0.527133  [ 1600/ 3200]
loss: 0.667499  [    0/ 3200]
loss: 0.846534  [ 1600/ 3200]
loss: 1.037067  [    0/ 3200]
loss: 0.715758  [ 1600/ 3200]
loss: 0.777999  [    0/ 3200]
loss: 0.715573  [ 1600/ 3200]
loss: 0.953323  [    0/ 3200]
loss: 0.721515  [ 1600/ 3200]
loss: 0.566374  [    0/ 3200]
loss: 0.773478  [ 1600/ 3200]
loss: 0.788179  [    0/ 3200]
loss: 1.232302  [ 1600/ 3200]
loss: 0.767740  [    0/ 3200]
loss: 0.444385  [ 1600/ 3200]
loss: 0.713070  [    0/ 3200]
loss: 0.413746  [ 1600/ 3200]
loss: 0.401288  [    0/ 3200]
loss: 0.933168  [ 1600/ 3200]
loss: 0.454037  [    0/ 3200]
loss: 0.589628  [ 1600/ 3200]
loss: 0.661655  [    0/ 3200]
loss: 0.944670  [ 1600/ 3200]
loss: 0.762336  [    0/ 3200]
loss: 0.817981  [ 1600/ 3200]
loss: 0.812203  [    0/ 3200]
loss: 0.927610  [ 1600/ 3200]
loss: 1.163412  [    0/ 3200]
loss: 1.070145  [ 1600/ 3200]
loss: 0.863925  [    0/ 3200]
loss: 0.835521  [ 1600/ 3200]
loss: 0.800025  [    0/ 3200]
loss: 1.008429  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0521
F1 Score (macro-averaged): 0.6442
Accuracy: 65.8430
Confusion Matrix:
[[106  15 113  90]
 [ 47 209  12  29]
 [ 15   8 317  16]
 [ 46  14  65 274]]
Epoch 9
-------------------------------
loss: 0.724897  [    0/ 3200]
loss: 0.626671  [ 1600/ 3200]
loss: 0.434546  [    0/ 3200]
loss: 0.254130  [ 1600/ 3200]
loss: 0.763396  [    0/ 3200]
loss: 1.131612  [ 1600/ 3200]
loss: 0.726131  [    0/ 3200]
loss: 0.782963  [ 1600/ 3200]
loss: 0.866748  [    0/ 3200]
loss: 0.417322  [ 1600/ 3200]
loss: 0.541102  [    0/ 3200]
loss: 0.761242  [ 1600/ 3200]
loss: 0.650172  [    0/ 3200]
loss: 0.530297  [ 1600/ 3200]
loss: 0.823902  [    0/ 3200]
loss: 0.860803  [ 1600/ 3200]
loss: 0.889304  [    0/ 3200]
loss: 1.008663  [ 1600/ 3200]
loss: 0.384361  [    0/ 3200]
loss: 1.273541  [ 1600/ 3200]
loss: 0.796029  [    0/ 3200]
loss: 0.660776  [ 1600/ 3200]
loss: 0.728170  [    0/ 3200]
loss: 0.815444  [ 1600/ 3200]
loss: 0.767314  [    0/ 3200]
loss: 0.627252  [ 1600/ 3200]
loss: 1.126123  [    0/ 3200]
loss: 0.593946  [ 1600/ 3200]
loss: 0.636691  [    0/ 3200]
loss: 0.656445  [ 1600/ 3200]
loss: 0.636429  [    0/ 3200]
loss: 0.599026  [ 1600/ 3200]
loss: 0.637304  [    0/ 3200]
loss: 0.716673  [ 1600/ 3200]
loss: 0.493552  [    0/ 3200]
loss: 0.795366  [ 1600/ 3200]
loss: 0.724334  [    0/ 3200]
loss: 1.103522  [ 1600/ 3200]
loss: 0.855129  [    0/ 3200]
loss: 0.719997  [ 1600/ 3200]
loss: 0.980443  [    0/ 3200]
loss: 0.598632  [ 1600/ 3200]
loss: 0.631710  [    0/ 3200]
loss: 0.994503  [ 1600/ 3200]
loss: 0.892394  [    0/ 3200]
loss: 0.629033  [ 1600/ 3200]
loss: 0.971174  [    0/ 3200]
loss: 0.489874  [ 1600/ 3200]
loss: 0.918639  [    0/ 3200]
loss: 1.192621  [ 1600/ 3200]
loss: 0.653478  [    0/ 3200]
loss: 0.869116  [ 1600/ 3200]
loss: 0.646901  [    0/ 3200]
loss: 1.346614  [ 1600/ 3200]
loss: 0.765505  [    0/ 3200]
loss: 0.794544  [ 1600/ 3200]
loss: 0.614585  [    0/ 3200]
loss: 1.050450  [ 1600/ 3200]
loss: 1.051889  [    0/ 3200]
loss: 0.456215  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0515
F1 Score (macro-averaged): 0.6472
Accuracy: 66.7878
Confusion Matrix:
[[ 90  20  77 137]
 [ 29 227   5  36]
 [ 21  12 286  37]
 [ 20  21  42 316]]
Epoch 10
-------------------------------
loss: 0.731570  [    0/ 3200]
loss: 0.472591  [ 1600/ 3200]
loss: 0.553333  [    0/ 3200]
loss: 0.314448  [ 1600/ 3200]
loss: 0.635002  [    0/ 3200]
loss: 0.711161  [ 1600/ 3200]
loss: 0.720001  [    0/ 3200]
loss: 0.618429  [ 1600/ 3200]
loss: 0.606686  [    0/ 3200]
loss: 0.644460  [ 1600/ 3200]
loss: 0.943855  [    0/ 3200]
loss: 0.573436  [ 1600/ 3200]
loss: 0.672834  [    0/ 3200]
loss: 0.616599  [ 1600/ 3200]
loss: 0.777625  [    0/ 3200]
loss: 0.563850  [ 1600/ 3200]
loss: 0.772529  [    0/ 3200]
loss: 1.313216  [ 1600/ 3200]
loss: 0.641488  [    0/ 3200]
loss: 0.760332  [ 1600/ 3200]
loss: 0.882666  [    0/ 3200]
loss: 0.979246  [ 1600/ 3200]
loss: 0.708059  [    0/ 3200]
loss: 1.103461  [ 1600/ 3200]
loss: 0.784440  [    0/ 3200]
loss: 1.072132  [ 1600/ 3200]
loss: 0.844205  [    0/ 3200]
loss: 1.019702  [ 1600/ 3200]
loss: 0.999373  [    0/ 3200]
loss: 0.994427  [ 1600/ 3200]
loss: 0.644342  [    0/ 3200]
loss: 0.613381  [ 1600/ 3200]
loss: 0.582523  [    0/ 3200]
loss: 0.738788  [ 1600/ 3200]
loss: 0.590803  [    0/ 3200]
loss: 0.989524  [ 1600/ 3200]
loss: 1.024275  [    0/ 3200]
loss: 0.833017  [ 1600/ 3200]
loss: 0.936130  [    0/ 3200]
loss: 0.838526  [ 1600/ 3200]
loss: 1.070075  [    0/ 3200]
loss: 1.126473  [ 1600/ 3200]
loss: 0.911036  [    0/ 3200]
loss: 0.706402  [ 1600/ 3200]
loss: 1.000251  [    0/ 3200]
loss: 1.118731  [ 1600/ 3200]
loss: 0.403418  [    0/ 3200]
loss: 0.884543  [ 1600/ 3200]
loss: 1.224330  [    0/ 3200]
loss: 1.038616  [ 1600/ 3200]
loss: 0.756757  [    0/ 3200]
loss: 0.769177  [ 1600/ 3200]
loss: 0.621737  [    0/ 3200]
loss: 0.608472  [ 1600/ 3200]
loss: 0.666967  [    0/ 3200]
loss: 0.651707  [ 1600/ 3200]
loss: 0.620947  [    0/ 3200]
loss: 0.874639  [ 1600/ 3200]
loss: 0.623317  [    0/ 3200]
loss: 0.810354  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0504
F1 Score (macro-averaged): 0.6613
Accuracy: 67.7326
Confusion Matrix:
[[106  30  70 118]
 [ 27 240   4  26]
 [ 30  13 285  28]
 [ 29  26  43 301]]
Epoch 11
-------------------------------
loss: 0.748694  [    0/ 3200]
loss: 0.625491  [ 1600/ 3200]
loss: 0.808039  [    0/ 3200]
loss: 0.931760  [ 1600/ 3200]
loss: 0.933179  [    0/ 3200]
loss: 0.580822  [ 1600/ 3200]
loss: 0.336752  [    0/ 3200]
loss: 0.612807  [ 1600/ 3200]
loss: 0.813701  [    0/ 3200]
loss: 0.509087  [ 1600/ 3200]
loss: 0.768616  [    0/ 3200]
loss: 0.826778  [ 1600/ 3200]
loss: 0.538982  [    0/ 3200]
loss: 0.745470  [ 1600/ 3200]
loss: 0.680556  [    0/ 3200]
loss: 0.819250  [ 1600/ 3200]
loss: 0.844300  [    0/ 3200]
loss: 0.587868  [ 1600/ 3200]
loss: 0.811467  [    0/ 3200]
loss: 0.649177  [ 1600/ 3200]
loss: 0.504529  [    0/ 3200]
loss: 0.677037  [ 1600/ 3200]
loss: 0.808464  [    0/ 3200]
loss: 0.838870  [ 1600/ 3200]
loss: 0.854095  [    0/ 3200]
loss: 0.954784  [ 1600/ 3200]
loss: 0.812356  [    0/ 3200]
loss: 0.541894  [ 1600/ 3200]
loss: 1.013533  [    0/ 3200]
loss: 0.630255  [ 1600/ 3200]
loss: 0.595270  [    0/ 3200]
loss: 0.689594  [ 1600/ 3200]
loss: 0.653343  [    0/ 3200]
loss: 0.849130  [ 1600/ 3200]
loss: 0.514188  [    0/ 3200]
loss: 1.032208  [ 1600/ 3200]
loss: 0.779930  [    0/ 3200]
loss: 0.683487  [ 1600/ 3200]
loss: 0.616096  [    0/ 3200]
loss: 0.631788  [ 1600/ 3200]
loss: 0.850471  [    0/ 3200]
loss: 1.086983  [ 1600/ 3200]
loss: 0.632701  [    0/ 3200]
loss: 1.093674  [ 1600/ 3200]
loss: 0.794020  [    0/ 3200]
loss: 0.693404  [ 1600/ 3200]
loss: 0.893976  [    0/ 3200]
loss: 0.825777  [ 1600/ 3200]
loss: 0.707313  [    0/ 3200]
loss: 0.836441  [ 1600/ 3200]
loss: 0.802366  [    0/ 3200]
loss: 0.614378  [ 1600/ 3200]
loss: 0.736797  [    0/ 3200]
loss: 0.840534  [ 1600/ 3200]
loss: 0.834023  [    0/ 3200]
loss: 0.592779  [ 1600/ 3200]
loss: 0.611819  [    0/ 3200]
loss: 0.827882  [ 1600/ 3200]
loss: 0.571700  [    0/ 3200]
loss: 0.447353  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0505
F1 Score (macro-averaged): 0.6681
Accuracy: 68.4593
Confusion Matrix:
[[113  44  78  89]
 [ 18 258   3  18]
 [ 23  17 297  19]
 [ 39  35  51 274]]
Epoch 12
-------------------------------
loss: 0.887858  [    0/ 3200]
loss: 0.885432  [ 1600/ 3200]
loss: 0.770950  [    0/ 3200]
loss: 0.650349  [ 1600/ 3200]
loss: 0.776917  [    0/ 3200]
loss: 0.829984  [ 1600/ 3200]
loss: 0.695864  [    0/ 3200]
loss: 0.890487  [ 1600/ 3200]
loss: 1.037286  [    0/ 3200]
loss: 0.653084  [ 1600/ 3200]
loss: 0.461236  [    0/ 3200]
loss: 1.216848  [ 1600/ 3200]
loss: 0.788073  [    0/ 3200]
loss: 0.643491  [ 1600/ 3200]
loss: 0.936708  [    0/ 3200]
loss: 0.664744  [ 1600/ 3200]
loss: 0.832844  [    0/ 3200]
loss: 0.635748  [ 1600/ 3200]
loss: 0.557722  [    0/ 3200]
loss: 0.579029  [ 1600/ 3200]
loss: 0.740699  [    0/ 3200]
loss: 0.739463  [ 1600/ 3200]
loss: 0.576631  [    0/ 3200]
loss: 0.801570  [ 1600/ 3200]
loss: 0.340040  [    0/ 3200]
loss: 0.732850  [ 1600/ 3200]
loss: 0.734160  [    0/ 3200]
loss: 0.612993  [ 1600/ 3200]
loss: 0.929889  [    0/ 3200]
loss: 0.830518  [ 1600/ 3200]
loss: 0.943185  [    0/ 3200]
loss: 0.636312  [ 1600/ 3200]
loss: 0.865791  [    0/ 3200]
loss: 0.491812  [ 1600/ 3200]
loss: 1.187732  [    0/ 3200]
loss: 0.696020  [ 1600/ 3200]
loss: 0.915522  [    0/ 3200]
loss: 0.932562  [ 1600/ 3200]
loss: 0.546145  [    0/ 3200]
loss: 0.967084  [ 1600/ 3200]
loss: 1.002965  [    0/ 3200]
loss: 0.335280  [ 1600/ 3200]
loss: 0.593338  [    0/ 3200]
loss: 0.640476  [ 1600/ 3200]
loss: 0.575520  [    0/ 3200]
loss: 0.565499  [ 1600/ 3200]
loss: 1.055093  [    0/ 3200]
loss: 0.674331  [ 1600/ 3200]
loss: 0.759475  [    0/ 3200]
loss: 0.600257  [ 1600/ 3200]
loss: 0.990615  [    0/ 3200]
loss: 0.797873  [ 1600/ 3200]
loss: 0.862417  [    0/ 3200]
loss: 0.806014  [ 1600/ 3200]
loss: 0.652991  [    0/ 3200]
loss: 0.546336  [ 1600/ 3200]
loss: 0.739114  [    0/ 3200]
loss: 0.676273  [ 1600/ 3200]
loss: 0.633963  [    0/ 3200]
loss: 0.802926  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0489
F1 Score (macro-averaged): 0.6832
Accuracy: 68.6773
Confusion Matrix:
[[149  27  53  95]
 [ 34 239   3  21]
 [ 51  12 269  24]
 [ 45  24  42 288]]
Epoch 13
-------------------------------
loss: 0.548784  [    0/ 3200]
loss: 0.654621  [ 1600/ 3200]
loss: 0.729067  [    0/ 3200]
loss: 0.866358  [ 1600/ 3200]
loss: 0.649536  [    0/ 3200]
loss: 0.936009  [ 1600/ 3200]
loss: 0.668748  [    0/ 3200]
loss: 1.106948  [ 1600/ 3200]
loss: 0.529879  [    0/ 3200]
loss: 0.720806  [ 1600/ 3200]
loss: 0.810203  [    0/ 3200]
loss: 0.681900  [ 1600/ 3200]
loss: 0.690122  [    0/ 3200]
loss: 0.601905  [ 1600/ 3200]
loss: 0.749215  [    0/ 3200]
loss: 0.999212  [ 1600/ 3200]
loss: 0.642304  [    0/ 3200]
loss: 0.381317  [ 1600/ 3200]
loss: 0.948107  [    0/ 3200]
loss: 0.688949  [ 1600/ 3200]
loss: 0.889153  [    0/ 3200]
loss: 0.957910  [ 1600/ 3200]
loss: 1.145001  [    0/ 3200]
loss: 0.463226  [ 1600/ 3200]
loss: 0.930016  [    0/ 3200]
loss: 0.653477  [ 1600/ 3200]
loss: 0.721482  [    0/ 3200]
loss: 0.671853  [ 1600/ 3200]
loss: 0.676963  [    0/ 3200]
loss: 0.453386  [ 1600/ 3200]
loss: 0.686523  [    0/ 3200]
loss: 0.755129  [ 1600/ 3200]
loss: 0.878861  [    0/ 3200]
loss: 0.757265  [ 1600/ 3200]
loss: 0.908215  [    0/ 3200]
loss: 0.619672  [ 1600/ 3200]
loss: 0.507661  [    0/ 3200]
loss: 0.652980  [ 1600/ 3200]
loss: 0.666985  [    0/ 3200]
loss: 0.742301  [ 1600/ 3200]
loss: 0.695411  [    0/ 3200]
loss: 0.910390  [ 1600/ 3200]
loss: 0.457789  [    0/ 3200]
loss: 1.070232  [ 1600/ 3200]
loss: 0.492266  [    0/ 3200]
loss: 0.693284  [ 1600/ 3200]
loss: 1.086070  [    0/ 3200]
loss: 0.873997  [ 1600/ 3200]
loss: 0.583012  [    0/ 3200]
loss: 0.583106  [ 1600/ 3200]
loss: 0.592490  [    0/ 3200]
loss: 0.670291  [ 1600/ 3200]
loss: 0.721118  [    0/ 3200]
loss: 0.584265  [ 1600/ 3200]
loss: 0.735831  [    0/ 3200]
loss: 0.584842  [ 1600/ 3200]
loss: 0.899825  [    0/ 3200]
loss: 0.731800  [ 1600/ 3200]
loss: 0.669489  [    0/ 3200]
loss: 0.882403  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0503
F1 Score (macro-averaged): 0.6578
Accuracy: 67.5145
Confusion Matrix:
[[101  32  56 135]
 [ 25 238   2  32]
 [ 33  14 271  38]
 [ 19  24  37 319]]
Epoch 14
-------------------------------
loss: 0.771747  [    0/ 3200]
loss: 0.951578  [ 1600/ 3200]
loss: 1.049535  [    0/ 3200]
loss: 0.707698  [ 1600/ 3200]
loss: 0.715435  [    0/ 3200]
loss: 0.998251  [ 1600/ 3200]
loss: 0.501798  [    0/ 3200]
loss: 0.742168  [ 1600/ 3200]
loss: 1.194077  [    0/ 3200]
loss: 0.734963  [ 1600/ 3200]
loss: 0.720309  [    0/ 3200]
loss: 0.497921  [ 1600/ 3200]
loss: 0.793362  [    0/ 3200]
loss: 0.974352  [ 1600/ 3200]
loss: 0.554023  [    0/ 3200]
loss: 0.460452  [ 1600/ 3200]
loss: 0.747280  [    0/ 3200]
loss: 0.809883  [ 1600/ 3200]
loss: 0.914404  [    0/ 3200]
loss: 0.604027  [ 1600/ 3200]
loss: 1.108329  [    0/ 3200]
loss: 0.466039  [ 1600/ 3200]
loss: 0.754550  [    0/ 3200]
loss: 0.895243  [ 1600/ 3200]
loss: 1.143062  [    0/ 3200]
loss: 0.748605  [ 1600/ 3200]
loss: 0.717239  [    0/ 3200]
loss: 0.613479  [ 1600/ 3200]
loss: 0.661882  [    0/ 3200]
loss: 0.353459  [ 1600/ 3200]
loss: 0.949052  [    0/ 3200]
loss: 1.125531  [ 1600/ 3200]
loss: 0.775349  [    0/ 3200]
loss: 0.794905  [ 1600/ 3200]
loss: 0.571467  [    0/ 3200]
loss: 0.681543  [ 1600/ 3200]
loss: 0.653446  [    0/ 3200]
loss: 1.011443  [ 1600/ 3200]
loss: 0.835602  [    0/ 3200]
loss: 0.727185  [ 1600/ 3200]
loss: 0.683682  [    0/ 3200]
loss: 0.617795  [ 1600/ 3200]
loss: 0.583976  [    0/ 3200]
loss: 0.787887  [ 1600/ 3200]
loss: 0.578541  [    0/ 3200]
loss: 0.692067  [ 1600/ 3200]
loss: 0.993748  [    0/ 3200]
loss: 0.750405  [ 1600/ 3200]
loss: 0.620641  [    0/ 3200]
loss: 0.960591  [ 1600/ 3200]
loss: 1.058336  [    0/ 3200]
loss: 0.695406  [ 1600/ 3200]
loss: 1.143983  [    0/ 3200]
loss: 0.466271  [ 1600/ 3200]
loss: 0.653099  [    0/ 3200]
loss: 0.749328  [ 1600/ 3200]
loss: 1.152432  [    0/ 3200]
loss: 0.712407  [ 1600/ 3200]
loss: 0.570044  [    0/ 3200]
loss: 0.704407  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0496
F1 Score (macro-averaged): 0.6843
Accuracy: 68.8227
Confusion Matrix:
[[153  22  71  78]
 [ 37 234   5  21]
 [ 34  14 290  18]
 [ 55  25  49 270]]
Epoch 15
-------------------------------
loss: 0.835836  [    0/ 3200]
loss: 0.482430  [ 1600/ 3200]
loss: 0.767925  [    0/ 3200]
loss: 0.744301  [ 1600/ 3200]
loss: 0.866129  [    0/ 3200]
loss: 0.632082  [ 1600/ 3200]
loss: 0.846585  [    0/ 3200]
loss: 0.900199  [ 1600/ 3200]
loss: 1.123800  [    0/ 3200]
loss: 0.837142  [ 1600/ 3200]
loss: 1.009151  [    0/ 3200]
loss: 0.793547  [ 1600/ 3200]
loss: 0.983944  [    0/ 3200]
loss: 0.474172  [ 1600/ 3200]
loss: 0.685160  [    0/ 3200]
loss: 0.803543  [ 1600/ 3200]
loss: 0.653439  [    0/ 3200]
loss: 0.554568  [ 1600/ 3200]
loss: 0.843428  [    0/ 3200]
loss: 0.574068  [ 1600/ 3200]
loss: 0.956665  [    0/ 3200]
loss: 0.453897  [ 1600/ 3200]
loss: 0.401375  [    0/ 3200]
loss: 0.856732  [ 1600/ 3200]
loss: 0.866013  [    0/ 3200]
loss: 0.398630  [ 1600/ 3200]
loss: 0.395044  [    0/ 3200]
loss: 0.806469  [ 1600/ 3200]
loss: 0.584049  [    0/ 3200]
loss: 0.507242  [ 1600/ 3200]
loss: 0.625435  [    0/ 3200]
loss: 0.741771  [ 1600/ 3200]
loss: 0.595288  [    0/ 3200]
loss: 0.776945  [ 1600/ 3200]
loss: 0.803978  [    0/ 3200]
loss: 0.622401  [ 1600/ 3200]
loss: 0.846195  [    0/ 3200]
loss: 1.004396  [ 1600/ 3200]
loss: 0.405269  [    0/ 3200]
loss: 0.675197  [ 1600/ 3200]
loss: 0.770275  [    0/ 3200]
loss: 0.974111  [ 1600/ 3200]
loss: 0.992052  [    0/ 3200]
loss: 0.748821  [ 1600/ 3200]
loss: 0.682361  [    0/ 3200]
loss: 0.902179  [ 1600/ 3200]
loss: 1.163946  [    0/ 3200]
loss: 1.123147  [ 1600/ 3200]
loss: 0.805251  [    0/ 3200]
loss: 0.574360  [ 1600/ 3200]
loss: 0.543573  [    0/ 3200]
loss: 0.444268  [ 1600/ 3200]
loss: 0.605173  [    0/ 3200]
loss: 0.524897  [ 1600/ 3200]
loss: 0.869802  [    0/ 3200]
loss: 0.678310  [ 1600/ 3200]
loss: 0.934792  [    0/ 3200]
loss: 0.808350  [ 1600/ 3200]
loss: 0.785020  [    0/ 3200]
loss: 0.983736  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0502
F1 Score (macro-averaged): 0.6766
Accuracy: 67.5145
Confusion Matrix:
[[166  13  47  98]
 [ 62 201   5  29]
 [ 54   5 268  29]
 [ 54  13  38 294]]
Epoch 16
-------------------------------
loss: 0.500929  [    0/ 3200]
loss: 0.868138  [ 1600/ 3200]
loss: 0.594221  [    0/ 3200]
loss: 0.793352  [ 1600/ 3200]
loss: 0.367686  [    0/ 3200]
loss: 0.532277  [ 1600/ 3200]
loss: 0.747499  [    0/ 3200]
loss: 0.815159  [ 1600/ 3200]
loss: 0.929027  [    0/ 3200]
loss: 0.998016  [ 1600/ 3200]
loss: 0.602367  [    0/ 3200]
loss: 0.672129  [ 1600/ 3200]
loss: 0.632446  [    0/ 3200]
loss: 0.729072  [ 1600/ 3200]
loss: 0.771742  [    0/ 3200]
loss: 0.770562  [ 1600/ 3200]
loss: 0.953644  [    0/ 3200]
loss: 0.552490  [ 1600/ 3200]
loss: 0.666845  [    0/ 3200]
loss: 0.812691  [ 1600/ 3200]
loss: 0.844510  [    0/ 3200]
loss: 0.937684  [ 1600/ 3200]
loss: 0.718695  [    0/ 3200]
loss: 0.555804  [ 1600/ 3200]
loss: 0.554177  [    0/ 3200]
loss: 0.630412  [ 1600/ 3200]
loss: 0.564892  [    0/ 3200]
loss: 0.450895  [ 1600/ 3200]
loss: 0.707727  [    0/ 3200]
loss: 0.995746  [ 1600/ 3200]
loss: 0.627307  [    0/ 3200]
loss: 0.855620  [ 1600/ 3200]
loss: 0.623423  [    0/ 3200]
loss: 0.663839  [ 1600/ 3200]
loss: 0.926902  [    0/ 3200]
loss: 1.108436  [ 1600/ 3200]
loss: 0.905873  [    0/ 3200]
loss: 0.699755  [ 1600/ 3200]
loss: 0.795942  [    0/ 3200]
loss: 0.900590  [ 1600/ 3200]
loss: 1.006394  [    0/ 3200]
loss: 0.637311  [ 1600/ 3200]
loss: 0.441801  [    0/ 3200]
loss: 0.601206  [ 1600/ 3200]
loss: 0.966967  [    0/ 3200]
loss: 0.512839  [ 1600/ 3200]
loss: 0.940834  [    0/ 3200]
loss: 0.580932  [ 1600/ 3200]
loss: 0.665218  [    0/ 3200]
loss: 0.367437  [ 1600/ 3200]
loss: 0.659844  [    0/ 3200]
loss: 0.619658  [ 1600/ 3200]
loss: 0.795772  [    0/ 3200]
loss: 0.935532  [ 1600/ 3200]
loss: 0.591079  [    0/ 3200]
loss: 0.497154  [ 1600/ 3200]
loss: 0.577236  [    0/ 3200]
loss: 0.935359  [ 1600/ 3200]
loss: 0.881042  [    0/ 3200]
loss: 0.798644  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0490
F1 Score (macro-averaged): 0.6841
Accuracy: 68.9680
Confusion Matrix:
[[145  18  85  76]
 [ 42 226   8  21]
 [ 25   8 307  16]
 [ 51  19  58 271]]
Epoch 17
-------------------------------
loss: 0.794800  [    0/ 3200]
loss: 0.638782  [ 1600/ 3200]
loss: 0.756795  [    0/ 3200]
loss: 0.845709  [ 1600/ 3200]
loss: 0.902693  [    0/ 3200]
loss: 0.858695  [ 1600/ 3200]
loss: 0.495141  [    0/ 3200]
loss: 0.670364  [ 1600/ 3200]
loss: 0.439837  [    0/ 3200]
loss: 1.084814  [ 1600/ 3200]
loss: 0.448108  [    0/ 3200]
loss: 0.798569  [ 1600/ 3200]
loss: 0.703606  [    0/ 3200]
loss: 0.788001  [ 1600/ 3200]
loss: 0.514439  [    0/ 3200]
loss: 0.599300  [ 1600/ 3200]
loss: 0.844419  [    0/ 3200]
loss: 0.390067  [ 1600/ 3200]
loss: 0.625996  [    0/ 3200]
loss: 0.688113  [ 1600/ 3200]
loss: 0.647584  [    0/ 3200]
loss: 0.575666  [ 1600/ 3200]
loss: 1.181304  [    0/ 3200]
loss: 0.948525  [ 1600/ 3200]
loss: 1.011935  [    0/ 3200]
loss: 0.729431  [ 1600/ 3200]
loss: 0.439250  [    0/ 3200]
loss: 0.738873  [ 1600/ 3200]
loss: 0.595490  [    0/ 3200]
loss: 0.742684  [ 1600/ 3200]
loss: 0.795683  [    0/ 3200]
loss: 0.753381  [ 1600/ 3200]
loss: 0.841221  [    0/ 3200]
loss: 0.759432  [ 1600/ 3200]
loss: 0.719727  [    0/ 3200]
loss: 0.547689  [ 1600/ 3200]
loss: 0.770167  [    0/ 3200]
loss: 0.901851  [ 1600/ 3200]
loss: 0.761695  [    0/ 3200]
loss: 0.813850  [ 1600/ 3200]
loss: 0.886082  [    0/ 3200]
loss: 0.512156  [ 1600/ 3200]
loss: 0.886544  [    0/ 3200]
loss: 0.293263  [ 1600/ 3200]
loss: 0.673871  [    0/ 3200]
loss: 1.137701  [ 1600/ 3200]
loss: 0.644812  [    0/ 3200]
loss: 0.597304  [ 1600/ 3200]
loss: 0.947946  [    0/ 3200]
loss: 0.758134  [ 1600/ 3200]
loss: 0.784188  [    0/ 3200]
loss: 0.657269  [ 1600/ 3200]
loss: 0.609276  [    0/ 3200]
loss: 0.821621  [ 1600/ 3200]
loss: 0.933590  [    0/ 3200]
loss: 0.687909  [ 1600/ 3200]
loss: 0.553295  [    0/ 3200]
loss: 0.859951  [ 1600/ 3200]
loss: 0.862269  [    0/ 3200]
loss: 0.708355  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0494
F1 Score (macro-averaged): 0.6932
Accuracy: 69.0407
Confusion Matrix:
[[177  22  36  89]
 [ 36 239   1  21]
 [ 69  13 249  25]
 [ 62  23  29 285]]
Epoch 18
-------------------------------
loss: 0.978753  [    0/ 3200]
loss: 0.771744  [ 1600/ 3200]
loss: 0.793498  [    0/ 3200]
loss: 0.463043  [ 1600/ 3200]
loss: 0.827927  [    0/ 3200]
loss: 0.479620  [ 1600/ 3200]
loss: 0.481304  [    0/ 3200]
loss: 0.676985  [ 1600/ 3200]
loss: 0.389567  [    0/ 3200]
loss: 0.757641  [ 1600/ 3200]
loss: 0.415085  [    0/ 3200]
loss: 0.674032  [ 1600/ 3200]
loss: 1.098343  [    0/ 3200]
loss: 0.481882  [ 1600/ 3200]
loss: 0.623366  [    0/ 3200]
loss: 0.672798  [ 1600/ 3200]
loss: 0.735400  [    0/ 3200]
loss: 0.663815  [ 1600/ 3200]
loss: 0.524885  [    0/ 3200]
loss: 0.735923  [ 1600/ 3200]
loss: 0.520695  [    0/ 3200]
loss: 0.719511  [ 1600/ 3200]
loss: 0.469259  [    0/ 3200]
loss: 0.692440  [ 1600/ 3200]
loss: 0.890642  [    0/ 3200]
loss: 0.557081  [ 1600/ 3200]
loss: 1.022735  [    0/ 3200]
loss: 0.628136  [ 1600/ 3200]
loss: 0.867977  [    0/ 3200]
loss: 0.653915  [ 1600/ 3200]
loss: 0.556132  [    0/ 3200]
loss: 0.638483  [ 1600/ 3200]
loss: 0.832441  [    0/ 3200]
loss: 0.797977  [ 1600/ 3200]
loss: 0.823045  [    0/ 3200]
loss: 0.751626  [ 1600/ 3200]
loss: 0.801817  [    0/ 3200]
loss: 0.814107  [ 1600/ 3200]
loss: 1.081680  [    0/ 3200]
loss: 0.533516  [ 1600/ 3200]
loss: 0.512671  [    0/ 3200]
loss: 0.813115  [ 1600/ 3200]
loss: 0.568432  [    0/ 3200]
loss: 0.802704  [ 1600/ 3200]
loss: 0.686721  [    0/ 3200]
loss: 0.816966  [ 1600/ 3200]
loss: 0.945034  [    0/ 3200]
loss: 0.935962  [ 1600/ 3200]
loss: 0.622352  [    0/ 3200]
loss: 0.538751  [ 1600/ 3200]
loss: 0.578185  [    0/ 3200]
loss: 0.506366  [ 1600/ 3200]
loss: 0.970428  [    0/ 3200]
loss: 0.740223  [ 1600/ 3200]
loss: 1.013058  [    0/ 3200]
loss: 0.571306  [ 1600/ 3200]
loss: 0.881328  [    0/ 3200]
loss: 0.886957  [ 1600/ 3200]
loss: 0.745933  [    0/ 3200]
loss: 0.841807  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0492
F1 Score (macro-averaged): 0.6980
Accuracy: 69.6221
Confusion Matrix:
[[182  34  50  58]
 [ 29 261   1   6]
 [ 57  12 271  16]
 [ 86  26  43 244]]
Epoch 19
-------------------------------
loss: 0.809272  [    0/ 3200]
loss: 0.650959  [ 1600/ 3200]
loss: 0.492166  [    0/ 3200]
loss: 0.868216  [ 1600/ 3200]
loss: 1.069593  [    0/ 3200]
loss: 0.547288  [ 1600/ 3200]
loss: 0.690208  [    0/ 3200]
loss: 0.654244  [ 1600/ 3200]
loss: 0.548677  [    0/ 3200]
loss: 0.865400  [ 1600/ 3200]
loss: 0.895138  [    0/ 3200]
loss: 0.780455  [ 1600/ 3200]
loss: 0.598064  [    0/ 3200]
loss: 0.597504  [ 1600/ 3200]
loss: 0.617885  [    0/ 3200]
loss: 0.614752  [ 1600/ 3200]
loss: 0.635490  [    0/ 3200]
loss: 0.972134  [ 1600/ 3200]
loss: 0.440552  [    0/ 3200]
loss: 0.975446  [ 1600/ 3200]
loss: 0.477745  [    0/ 3200]
loss: 0.554171  [ 1600/ 3200]
loss: 0.811234  [    0/ 3200]
loss: 0.943130  [ 1600/ 3200]
loss: 0.904777  [    0/ 3200]
loss: 0.623031  [ 1600/ 3200]
loss: 0.593170  [    0/ 3200]
loss: 0.446371  [ 1600/ 3200]
loss: 0.884310  [    0/ 3200]
loss: 0.726838  [ 1600/ 3200]
loss: 0.719480  [    0/ 3200]
loss: 0.703308  [ 1600/ 3200]
loss: 0.755463  [    0/ 3200]
loss: 0.613301  [ 1600/ 3200]
loss: 0.626659  [    0/ 3200]
loss: 0.933282  [ 1600/ 3200]
loss: 0.845239  [    0/ 3200]
loss: 0.753796  [ 1600/ 3200]
loss: 0.638353  [    0/ 3200]
loss: 0.463371  [ 1600/ 3200]
loss: 0.737283  [    0/ 3200]
loss: 0.516846  [ 1600/ 3200]
loss: 0.577519  [    0/ 3200]
loss: 0.589603  [ 1600/ 3200]
loss: 0.589769  [    0/ 3200]
loss: 0.620065  [ 1600/ 3200]
loss: 0.617313  [    0/ 3200]
loss: 0.520648  [ 1600/ 3200]
loss: 0.716761  [    0/ 3200]
loss: 0.731366  [ 1600/ 3200]
loss: 0.621952  [    0/ 3200]
loss: 0.968542  [ 1600/ 3200]
loss: 0.881122  [    0/ 3200]
loss: 0.625038  [ 1600/ 3200]
loss: 0.923495  [    0/ 3200]
loss: 0.662819  [ 1600/ 3200]
loss: 0.421943  [    0/ 3200]
loss: 0.771076  [ 1600/ 3200]
loss: 0.489684  [    0/ 3200]
loss: 0.622678  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0483
F1 Score (macro-averaged): 0.6858
Accuracy: 68.5320
Confusion Matrix:
[[162  14  67  81]
 [ 55 218   4  20]
 [ 43   6 283  24]
 [ 60  15  44 280]]
Epoch 20
-------------------------------
loss: 0.651097  [    0/ 3200]
loss: 1.352859  [ 1600/ 3200]
loss: 0.657084  [    0/ 3200]
loss: 0.606560  [ 1600/ 3200]
loss: 0.936124  [    0/ 3200]
loss: 0.686188  [ 1600/ 3200]
loss: 0.557034  [    0/ 3200]
loss: 0.651118  [ 1600/ 3200]
loss: 0.616957  [    0/ 3200]
loss: 0.764323  [ 1600/ 3200]
loss: 0.638362  [    0/ 3200]
loss: 0.405915  [ 1600/ 3200]
loss: 0.913509  [    0/ 3200]
loss: 0.640720  [ 1600/ 3200]
loss: 0.751790  [    0/ 3200]
loss: 0.433982  [ 1600/ 3200]
loss: 0.639044  [    0/ 3200]
loss: 0.868912  [ 1600/ 3200]
loss: 0.496757  [    0/ 3200]
loss: 0.572809  [ 1600/ 3200]
loss: 0.652985  [    0/ 3200]
loss: 0.846406  [ 1600/ 3200]
loss: 0.835795  [    0/ 3200]
loss: 0.821100  [ 1600/ 3200]
loss: 0.454747  [    0/ 3200]
loss: 1.361827  [ 1600/ 3200]
loss: 0.552782  [    0/ 3200]
loss: 0.947785  [ 1600/ 3200]
loss: 1.088754  [    0/ 3200]
loss: 0.574834  [ 1600/ 3200]
loss: 0.637653  [    0/ 3200]
loss: 0.626964  [ 1600/ 3200]
loss: 0.742220  [    0/ 3200]
loss: 0.788754  [ 1600/ 3200]
loss: 0.566336  [    0/ 3200]
loss: 0.454559  [ 1600/ 3200]
loss: 0.579122  [    0/ 3200]
loss: 0.835277  [ 1600/ 3200]
loss: 0.489368  [    0/ 3200]
loss: 0.713360  [ 1600/ 3200]
loss: 0.392257  [    0/ 3200]
loss: 0.502266  [ 1600/ 3200]
loss: 0.897101  [    0/ 3200]
loss: 0.693725  [ 1600/ 3200]
loss: 0.554879  [    0/ 3200]
loss: 0.671548  [ 1600/ 3200]
loss: 0.902182  [    0/ 3200]
loss: 0.780975  [ 1600/ 3200]
loss: 0.656139  [    0/ 3200]
loss: 0.290574  [ 1600/ 3200]
loss: 0.761794  [    0/ 3200]
loss: 0.656627  [ 1600/ 3200]
loss: 0.564317  [    0/ 3200]
loss: 1.342428  [ 1600/ 3200]
loss: 0.727345  [    0/ 3200]
loss: 0.530273  [ 1600/ 3200]
loss: 0.648149  [    0/ 3200]
loss: 0.582700  [ 1600/ 3200]
loss: 0.658804  [    0/ 3200]
loss: 0.688184  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0499
F1 Score (macro-averaged): 0.6683
Accuracy: 67.1512
Confusion Matrix:
[[150  10  84  80]
 [ 67 193   8  29]
 [ 34   3 294  25]
 [ 51  10  51 287]]
Epoch 21
-------------------------------
loss: 0.864583  [    0/ 3200]
loss: 1.070184  [ 1600/ 3200]
loss: 0.642121  [    0/ 3200]
loss: 0.840398  [ 1600/ 3200]
loss: 0.758079  [    0/ 3200]
loss: 0.460570  [ 1600/ 3200]
loss: 0.709993  [    0/ 3200]
loss: 0.790006  [ 1600/ 3200]
loss: 0.487165  [    0/ 3200]
loss: 0.815480  [ 1600/ 3200]
loss: 0.609749  [    0/ 3200]
loss: 0.625654  [ 1600/ 3200]
loss: 0.368455  [    0/ 3200]
loss: 0.628171  [ 1600/ 3200]
loss: 0.503784  [    0/ 3200]
loss: 0.516009  [ 1600/ 3200]
loss: 0.831041  [    0/ 3200]
loss: 0.647794  [ 1600/ 3200]
loss: 0.638269  [    0/ 3200]
loss: 0.932027  [ 1600/ 3200]
loss: 0.671812  [    0/ 3200]
loss: 0.665133  [ 1600/ 3200]
loss: 0.718940  [    0/ 3200]
loss: 0.721129  [ 1600/ 3200]
loss: 1.036410  [    0/ 3200]
loss: 0.757612  [ 1600/ 3200]
loss: 0.917427  [    0/ 3200]
loss: 0.981979  [ 1600/ 3200]
loss: 0.747513  [    0/ 3200]
loss: 0.740258  [ 1600/ 3200]
loss: 0.778310  [    0/ 3200]
loss: 0.921523  [ 1600/ 3200]
loss: 0.546056  [    0/ 3200]
loss: 0.680058  [ 1600/ 3200]
loss: 0.787309  [    0/ 3200]
loss: 0.837703  [ 1600/ 3200]
loss: 0.585100  [    0/ 3200]
loss: 0.497270  [ 1600/ 3200]
loss: 0.752865  [    0/ 3200]
loss: 0.837876  [ 1600/ 3200]
loss: 1.108130  [    0/ 3200]
loss: 0.664037  [ 1600/ 3200]
loss: 0.603727  [    0/ 3200]
loss: 0.884568  [ 1600/ 3200]
loss: 0.678428  [    0/ 3200]
loss: 0.456905  [ 1600/ 3200]
loss: 0.772197  [    0/ 3200]
loss: 0.674837  [ 1600/ 3200]
loss: 0.854511  [    0/ 3200]
loss: 0.739638  [ 1600/ 3200]
loss: 0.633493  [    0/ 3200]
loss: 0.516956  [ 1600/ 3200]
loss: 0.659302  [    0/ 3200]
loss: 0.351893  [ 1600/ 3200]
loss: 0.734756  [    0/ 3200]
loss: 0.542041  [ 1600/ 3200]
loss: 0.766040  [    0/ 3200]
loss: 0.609654  [ 1600/ 3200]
loss: 0.424896  [    0/ 3200]
loss: 0.544888  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0478
F1 Score (macro-averaged): 0.6642
Accuracy: 68.7500
Confusion Matrix:
[[ 90  33  75 126]
 [ 14 252   3  28]
 [ 24  12 284  36]
 [ 18  19  42 320]]
Epoch 22
-------------------------------
loss: 1.087858  [    0/ 3200]
loss: 0.898752  [ 1600/ 3200]
loss: 0.629894  [    0/ 3200]
loss: 0.622217  [ 1600/ 3200]
loss: 0.924804  [    0/ 3200]
loss: 0.968985  [ 1600/ 3200]
loss: 0.787940  [    0/ 3200]
loss: 0.695987  [ 1600/ 3200]
loss: 0.562031  [    0/ 3200]
loss: 0.902112  [ 1600/ 3200]
loss: 0.628959  [    0/ 3200]
loss: 0.630032  [ 1600/ 3200]
loss: 1.035681  [    0/ 3200]
loss: 0.601816  [ 1600/ 3200]
loss: 0.426107  [    0/ 3200]
loss: 0.792413  [ 1600/ 3200]
loss: 0.606799  [    0/ 3200]
loss: 0.517691  [ 1600/ 3200]
loss: 1.057394  [    0/ 3200]
loss: 0.482777  [ 1600/ 3200]
loss: 0.628811  [    0/ 3200]
loss: 0.653721  [ 1600/ 3200]
loss: 0.917553  [    0/ 3200]
loss: 0.586134  [ 1600/ 3200]
loss: 0.669176  [    0/ 3200]
loss: 0.983927  [ 1600/ 3200]
loss: 0.519416  [    0/ 3200]
loss: 0.424183  [ 1600/ 3200]
loss: 0.618719  [    0/ 3200]
loss: 0.527524  [ 1600/ 3200]
loss: 0.739015  [    0/ 3200]
loss: 0.701754  [ 1600/ 3200]
loss: 0.896147  [    0/ 3200]
loss: 0.658399  [ 1600/ 3200]
loss: 0.625073  [    0/ 3200]
loss: 0.613757  [ 1600/ 3200]
loss: 0.610467  [    0/ 3200]
loss: 0.513212  [ 1600/ 3200]
loss: 0.430939  [    0/ 3200]
loss: 0.642707  [ 1600/ 3200]
loss: 0.495121  [    0/ 3200]
loss: 0.666134  [ 1600/ 3200]
loss: 0.951102  [    0/ 3200]
loss: 0.720803  [ 1600/ 3200]
loss: 0.708036  [    0/ 3200]
loss: 0.699278  [ 1600/ 3200]
loss: 0.893280  [    0/ 3200]
loss: 1.051864  [ 1600/ 3200]
loss: 0.455046  [    0/ 3200]
loss: 1.033769  [ 1600/ 3200]
loss: 0.804719  [    0/ 3200]
loss: 0.680788  [ 1600/ 3200]
loss: 0.625491  [    0/ 3200]
loss: 0.521220  [ 1600/ 3200]
loss: 0.811988  [    0/ 3200]
loss: 0.687824  [ 1600/ 3200]
loss: 0.778884  [    0/ 3200]
loss: 0.653086  [ 1600/ 3200]
loss: 0.598105  [    0/ 3200]
loss: 1.018927  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0481
F1 Score (macro-averaged): 0.6689
Accuracy: 68.2413
Confusion Matrix:
[[114  18  93  99]
 [ 32 221  12  32]
 [ 20   8 304  24]
 [ 31  12  56 300]]
Epoch 23
-------------------------------
loss: 0.946305  [    0/ 3200]
loss: 0.724076  [ 1600/ 3200]
loss: 0.481227  [    0/ 3200]
loss: 0.784122  [ 1600/ 3200]
loss: 0.923989  [    0/ 3200]
loss: 0.674859  [ 1600/ 3200]
loss: 0.762995  [    0/ 3200]
loss: 0.860422  [ 1600/ 3200]
loss: 0.709036  [    0/ 3200]
loss: 0.302516  [ 1600/ 3200]
loss: 0.621351  [    0/ 3200]
loss: 0.782318  [ 1600/ 3200]
loss: 0.497214  [    0/ 3200]
loss: 0.432554  [ 1600/ 3200]
loss: 0.677089  [    0/ 3200]
loss: 0.636550  [ 1600/ 3200]
loss: 0.532513  [    0/ 3200]
loss: 0.656504  [ 1600/ 3200]
loss: 0.625140  [    0/ 3200]
loss: 0.526948  [ 1600/ 3200]
loss: 0.496135  [    0/ 3200]
loss: 0.765485  [ 1600/ 3200]
loss: 0.511698  [    0/ 3200]
loss: 0.598063  [ 1600/ 3200]
loss: 0.708743  [    0/ 3200]
loss: 0.980795  [ 1600/ 3200]
loss: 0.764754  [    0/ 3200]
loss: 0.900510  [ 1600/ 3200]
loss: 0.613669  [    0/ 3200]
loss: 0.477708  [ 1600/ 3200]
loss: 0.704542  [    0/ 3200]
loss: 0.474664  [ 1600/ 3200]
loss: 0.405961  [    0/ 3200]
loss: 0.874066  [ 1600/ 3200]
loss: 0.566981  [    0/ 3200]
loss: 0.682864  [ 1600/ 3200]
loss: 0.468470  [    0/ 3200]
loss: 0.525980  [ 1600/ 3200]
loss: 0.662313  [    0/ 3200]
loss: 0.986120  [ 1600/ 3200]
loss: 0.724389  [    0/ 3200]
loss: 0.567515  [ 1600/ 3200]
loss: 0.698631  [    0/ 3200]
loss: 0.547612  [ 1600/ 3200]
loss: 0.738078  [    0/ 3200]
loss: 0.758439  [ 1600/ 3200]
loss: 0.348093  [    0/ 3200]
loss: 0.528096  [ 1600/ 3200]
loss: 0.834332  [    0/ 3200]
loss: 0.567531  [ 1600/ 3200]
loss: 0.421184  [    0/ 3200]
loss: 0.853541  [ 1600/ 3200]
loss: 0.369490  [    0/ 3200]
loss: 0.685115  [ 1600/ 3200]
loss: 0.773911  [    0/ 3200]
loss: 0.842823  [ 1600/ 3200]
loss: 0.615133  [    0/ 3200]
loss: 0.719407  [ 1600/ 3200]
loss: 0.300702  [    0/ 3200]
loss: 0.595565  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0531
F1 Score (macro-averaged): 0.6769
Accuracy: 67.8779
Confusion Matrix:
[[180  60  36  48]
 [ 14 276   0   7]
 [ 74  17 252  13]
 [ 72  66  35 226]]
Epoch 24
-------------------------------
loss: 0.829965  [    0/ 3200]
loss: 0.801613  [ 1600/ 3200]
loss: 0.915349  [    0/ 3200]
loss: 0.766647  [ 1600/ 3200]
loss: 0.433873  [    0/ 3200]
loss: 0.689963  [ 1600/ 3200]
loss: 0.404263  [    0/ 3200]
loss: 0.877491  [ 1600/ 3200]
loss: 0.439072  [    0/ 3200]
loss: 0.756332  [ 1600/ 3200]
loss: 0.745204  [    0/ 3200]
loss: 0.832213  [ 1600/ 3200]
loss: 0.719374  [    0/ 3200]
loss: 1.180936  [ 1600/ 3200]
loss: 0.894302  [    0/ 3200]
loss: 0.494266  [ 1600/ 3200]
loss: 0.781970  [    0/ 3200]
loss: 0.578857  [ 1600/ 3200]
loss: 0.748755  [    0/ 3200]
loss: 1.000309  [ 1600/ 3200]
loss: 0.873821  [    0/ 3200]
loss: 0.931561  [ 1600/ 3200]
loss: 0.683425  [    0/ 3200]
loss: 0.620783  [ 1600/ 3200]
loss: 1.039532  [    0/ 3200]
loss: 0.888103  [ 1600/ 3200]
loss: 0.729413  [    0/ 3200]
loss: 0.578424  [ 1600/ 3200]
loss: 0.806858  [    0/ 3200]
loss: 0.604141  [ 1600/ 3200]
loss: 0.660278  [    0/ 3200]
loss: 0.824386  [ 1600/ 3200]
loss: 0.830758  [    0/ 3200]
loss: 0.374770  [ 1600/ 3200]
loss: 0.617032  [    0/ 3200]
loss: 0.442005  [ 1600/ 3200]
loss: 0.469903  [    0/ 3200]
loss: 0.652611  [ 1600/ 3200]
loss: 0.738878  [    0/ 3200]
loss: 1.148138  [ 1600/ 3200]
loss: 0.992877  [    0/ 3200]
loss: 0.734175  [ 1600/ 3200]
loss: 0.326171  [    0/ 3200]
loss: 0.365578  [ 1600/ 3200]
loss: 0.453950  [    0/ 3200]
loss: 0.694682  [ 1600/ 3200]
loss: 0.676003  [    0/ 3200]
loss: 0.457089  [ 1600/ 3200]
loss: 0.731890  [    0/ 3200]
loss: 0.439927  [ 1600/ 3200]
loss: 0.527416  [    0/ 3200]
loss: 0.969190  [ 1600/ 3200]
loss: 0.700571  [    0/ 3200]
loss: 0.647538  [ 1600/ 3200]
loss: 0.728484  [    0/ 3200]
loss: 0.719417  [ 1600/ 3200]
loss: 0.790726  [    0/ 3200]
loss: 1.120767  [ 1600/ 3200]
loss: 0.743954  [    0/ 3200]
loss: 1.003385  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0488
F1 Score (macro-averaged): 0.6923
Accuracy: 70.0581
Confusion Matrix:
[[144  36  92  52]
 [ 19 263   6   9]
 [ 23  13 310  10]
 [ 63  27  62 247]]
Epoch 25
-------------------------------
loss: 0.459668  [    0/ 3200]
loss: 0.515153  [ 1600/ 3200]
loss: 0.375253  [    0/ 3200]
loss: 0.807187  [ 1600/ 3200]
loss: 0.530615  [    0/ 3200]
loss: 0.408655  [ 1600/ 3200]
loss: 0.819646  [    0/ 3200]
loss: 0.423959  [ 1600/ 3200]
loss: 0.617572  [    0/ 3200]
loss: 0.591937  [ 1600/ 3200]
loss: 0.425926  [    0/ 3200]
loss: 0.607764  [ 1600/ 3200]
loss: 0.444072  [    0/ 3200]
loss: 0.909475  [ 1600/ 3200]
loss: 0.668872  [    0/ 3200]
loss: 0.547904  [ 1600/ 3200]
loss: 0.462474  [    0/ 3200]
loss: 0.463494  [ 1600/ 3200]
loss: 0.921423  [    0/ 3200]
loss: 0.737066  [ 1600/ 3200]
loss: 0.493395  [    0/ 3200]
loss: 0.778167  [ 1600/ 3200]
loss: 0.785844  [    0/ 3200]
loss: 0.456089  [ 1600/ 3200]
loss: 0.937452  [    0/ 3200]
loss: 1.059090  [ 1600/ 3200]
loss: 0.456955  [    0/ 3200]
loss: 0.509692  [ 1600/ 3200]
loss: 0.369405  [    0/ 3200]
loss: 0.732466  [ 1600/ 3200]
loss: 0.764697  [    0/ 3200]
loss: 0.674286  [ 1600/ 3200]
loss: 0.373346  [    0/ 3200]
loss: 0.464699  [ 1600/ 3200]
loss: 0.491322  [    0/ 3200]
loss: 0.583336  [ 1600/ 3200]
loss: 0.607961  [    0/ 3200]
loss: 0.795863  [ 1600/ 3200]
loss: 0.470606  [    0/ 3200]
loss: 0.825057  [ 1600/ 3200]
loss: 0.900448  [    0/ 3200]
loss: 0.843142  [ 1600/ 3200]
loss: 0.795339  [    0/ 3200]
loss: 0.376828  [ 1600/ 3200]
loss: 0.791728  [    0/ 3200]
loss: 0.736076  [ 1600/ 3200]
loss: 0.683101  [    0/ 3200]
loss: 0.777521  [ 1600/ 3200]
loss: 0.376905  [    0/ 3200]
loss: 0.840880  [ 1600/ 3200]
loss: 0.714105  [    0/ 3200]
loss: 0.738092  [ 1600/ 3200]
loss: 0.533318  [    0/ 3200]
loss: 0.536951  [ 1600/ 3200]
loss: 0.567237  [    0/ 3200]
loss: 0.639089  [ 1600/ 3200]
loss: 0.554324  [    0/ 3200]
loss: 0.804080  [ 1600/ 3200]
loss: 0.578103  [    0/ 3200]
loss: 0.483769  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0468
F1 Score (macro-averaged): 0.6879
Accuracy: 69.8401
Confusion Matrix:
[[126  39  50 109]
 [ 19 260   2  16]
 [ 40  11 275  30]
 [ 38  23  38 300]]
Epoch 26
-------------------------------
loss: 0.761619  [    0/ 3200]
loss: 0.753673  [ 1600/ 3200]
loss: 1.161043  [    0/ 3200]
loss: 0.655229  [ 1600/ 3200]
loss: 0.634832  [    0/ 3200]
loss: 0.441520  [ 1600/ 3200]
loss: 0.704224  [    0/ 3200]
loss: 0.606915  [ 1600/ 3200]
loss: 0.397697  [    0/ 3200]
loss: 0.818141  [ 1600/ 3200]
loss: 0.928484  [    0/ 3200]
loss: 0.425909  [ 1600/ 3200]
loss: 0.463312  [    0/ 3200]
loss: 0.627445  [ 1600/ 3200]
loss: 0.427113  [    0/ 3200]
loss: 0.705195  [ 1600/ 3200]
loss: 0.624889  [    0/ 3200]
loss: 0.869078  [ 1600/ 3200]
loss: 1.047518  [    0/ 3200]
loss: 0.603152  [ 1600/ 3200]
loss: 1.077344  [    0/ 3200]
loss: 0.437234  [ 1600/ 3200]
loss: 0.505546  [    0/ 3200]
loss: 0.410815  [ 1600/ 3200]
loss: 0.604917  [    0/ 3200]
loss: 0.890121  [ 1600/ 3200]
loss: 0.740642  [    0/ 3200]
loss: 0.416727  [ 1600/ 3200]
loss: 0.544140  [    0/ 3200]
loss: 0.639093  [ 1600/ 3200]
loss: 0.365431  [    0/ 3200]
loss: 0.659423  [ 1600/ 3200]
loss: 0.788832  [    0/ 3200]
loss: 0.543970  [ 1600/ 3200]
loss: 0.549671  [    0/ 3200]
loss: 0.497350  [ 1600/ 3200]
loss: 0.598032  [    0/ 3200]
loss: 0.641277  [ 1600/ 3200]
loss: 0.820076  [    0/ 3200]
loss: 0.555206  [ 1600/ 3200]
loss: 0.482731  [    0/ 3200]
loss: 0.878655  [ 1600/ 3200]
loss: 0.422586  [    0/ 3200]
loss: 0.705369  [ 1600/ 3200]
loss: 0.647820  [    0/ 3200]
loss: 0.720363  [ 1600/ 3200]
loss: 0.745852  [    0/ 3200]
loss: 0.923240  [ 1600/ 3200]
loss: 0.460009  [    0/ 3200]
loss: 0.955175  [ 1600/ 3200]
loss: 0.783866  [    0/ 3200]
loss: 0.879462  [ 1600/ 3200]
loss: 0.901731  [    0/ 3200]
loss: 0.799304  [ 1600/ 3200]
loss: 0.449753  [    0/ 3200]
loss: 0.449472  [ 1600/ 3200]
loss: 0.712282  [    0/ 3200]
loss: 0.555968  [ 1600/ 3200]
loss: 0.620826  [    0/ 3200]
loss: 0.482707  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0467
F1 Score (macro-averaged): 0.6942
Accuracy: 69.9855
Confusion Matrix:
[[144  30  75  75]
 [ 28 254   4  11]
 [ 33  11 291  21]
 [ 56  18  51 274]]
Epoch 27
-------------------------------
loss: 0.582704  [    0/ 3200]
loss: 0.340575  [ 1600/ 3200]
loss: 0.640387  [    0/ 3200]
loss: 0.731540  [ 1600/ 3200]
loss: 0.604377  [    0/ 3200]
loss: 0.710406  [ 1600/ 3200]
loss: 0.875564  [    0/ 3200]
loss: 1.101151  [ 1600/ 3200]
loss: 0.641689  [    0/ 3200]
loss: 0.986961  [ 1600/ 3200]
loss: 0.291051  [    0/ 3200]
loss: 0.538896  [ 1600/ 3200]
loss: 1.253130  [    0/ 3200]
loss: 0.457976  [ 1600/ 3200]
loss: 0.444867  [    0/ 3200]
loss: 0.516371  [ 1600/ 3200]
loss: 0.371655  [    0/ 3200]
loss: 0.881196  [ 1600/ 3200]
loss: 0.866614  [    0/ 3200]
loss: 0.394703  [ 1600/ 3200]
loss: 0.675605  [    0/ 3200]
loss: 0.845220  [ 1600/ 3200]
loss: 0.461741  [    0/ 3200]
loss: 0.514728  [ 1600/ 3200]
loss: 0.685668  [    0/ 3200]
loss: 0.379249  [ 1600/ 3200]
loss: 0.905520  [    0/ 3200]
loss: 0.991106  [ 1600/ 3200]
loss: 0.777259  [    0/ 3200]
loss: 0.785382  [ 1600/ 3200]
loss: 0.603309  [    0/ 3200]
loss: 0.553355  [ 1600/ 3200]
loss: 0.472796  [    0/ 3200]
loss: 0.889979  [ 1600/ 3200]
loss: 0.668283  [    0/ 3200]
loss: 0.562971  [ 1600/ 3200]
loss: 0.532098  [    0/ 3200]
loss: 1.045148  [ 1600/ 3200]
loss: 0.599122  [    0/ 3200]
loss: 0.637300  [ 1600/ 3200]
loss: 0.654502  [    0/ 3200]
loss: 0.755219  [ 1600/ 3200]
loss: 0.864262  [    0/ 3200]
loss: 0.388526  [ 1600/ 3200]
loss: 0.315961  [    0/ 3200]
loss: 0.689139  [ 1600/ 3200]
loss: 0.369532  [    0/ 3200]
loss: 0.457877  [ 1600/ 3200]
loss: 0.592740  [    0/ 3200]
loss: 0.867170  [ 1600/ 3200]
loss: 0.798418  [    0/ 3200]
loss: 1.165159  [ 1600/ 3200]
loss: 0.801553  [    0/ 3200]
loss: 0.658638  [ 1600/ 3200]
loss: 0.903394  [    0/ 3200]
loss: 0.765158  [ 1600/ 3200]
loss: 0.652979  [    0/ 3200]
loss: 0.839168  [ 1600/ 3200]
loss: 0.678704  [    0/ 3200]
loss: 0.798670  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0473
F1 Score (macro-averaged): 0.6817
Accuracy: 69.9128
Confusion Matrix:
[[109  37  82  96]
 [ 16 261   3  17]
 [ 24  12 298  22]
 [ 35  21  49 294]]
Epoch 28
-------------------------------
loss: 1.333370  [    0/ 3200]
loss: 0.564152  [ 1600/ 3200]
loss: 0.662725  [    0/ 3200]
loss: 0.613642  [ 1600/ 3200]
loss: 0.889720  [    0/ 3200]
loss: 0.627979  [ 1600/ 3200]
loss: 0.775947  [    0/ 3200]
loss: 0.791119  [ 1600/ 3200]
loss: 0.620939  [    0/ 3200]
loss: 0.504522  [ 1600/ 3200]
loss: 0.674062  [    0/ 3200]
loss: 0.832560  [ 1600/ 3200]
loss: 0.717470  [    0/ 3200]
loss: 0.371057  [ 1600/ 3200]
loss: 0.607739  [    0/ 3200]
loss: 0.570753  [ 1600/ 3200]
loss: 0.913503  [    0/ 3200]
loss: 0.548462  [ 1600/ 3200]
loss: 0.741686  [    0/ 3200]
loss: 0.527916  [ 1600/ 3200]
loss: 1.002102  [    0/ 3200]
loss: 0.604631  [ 1600/ 3200]
loss: 0.744214  [    0/ 3200]
loss: 0.883771  [ 1600/ 3200]
loss: 1.235474  [    0/ 3200]
loss: 1.124268  [ 1600/ 3200]
loss: 0.881565  [    0/ 3200]
loss: 0.809734  [ 1600/ 3200]
loss: 0.592372  [    0/ 3200]
loss: 0.573745  [ 1600/ 3200]
loss: 1.063120  [    0/ 3200]
loss: 0.704256  [ 1600/ 3200]
loss: 0.378716  [    0/ 3200]
loss: 0.751352  [ 1600/ 3200]
loss: 0.735543  [    0/ 3200]
loss: 0.622150  [ 1600/ 3200]
loss: 0.644883  [    0/ 3200]
loss: 0.577248  [ 1600/ 3200]
loss: 0.769821  [    0/ 3200]
loss: 0.414415  [ 1600/ 3200]
loss: 0.486564  [    0/ 3200]
loss: 0.382209  [ 1600/ 3200]
loss: 0.547635  [    0/ 3200]
loss: 0.734923  [ 1600/ 3200]
loss: 0.485559  [    0/ 3200]
loss: 0.728326  [ 1600/ 3200]
loss: 0.771653  [    0/ 3200]
loss: 0.743636  [ 1600/ 3200]
loss: 0.623468  [    0/ 3200]
loss: 0.531302  [ 1600/ 3200]
loss: 0.708296  [    0/ 3200]
loss: 0.264355  [ 1600/ 3200]
loss: 0.510348  [    0/ 3200]
loss: 0.441684  [ 1600/ 3200]
loss: 0.530525  [    0/ 3200]
loss: 0.754101  [ 1600/ 3200]
loss: 0.803936  [    0/ 3200]
loss: 1.015893  [ 1600/ 3200]
loss: 0.644939  [    0/ 3200]
loss: 0.469781  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0495
F1 Score (macro-averaged): 0.6886
Accuracy: 69.5494
Confusion Matrix:
[[154  60  54  56]
 [ 10 280   0   7]
 [ 45  21 269  21]
 [ 64  42  39 254]]
Epoch 29
-------------------------------
loss: 0.578576  [    0/ 3200]
loss: 0.991441  [ 1600/ 3200]
loss: 0.641920  [    0/ 3200]
loss: 0.642788  [ 1600/ 3200]
loss: 0.384081  [    0/ 3200]
loss: 0.467550  [ 1600/ 3200]
loss: 0.706364  [    0/ 3200]
loss: 0.958204  [ 1600/ 3200]
loss: 0.431054  [    0/ 3200]
loss: 0.693058  [ 1600/ 3200]
loss: 1.109639  [    0/ 3200]
loss: 0.686964  [ 1600/ 3200]
loss: 0.355222  [    0/ 3200]
loss: 0.554771  [ 1600/ 3200]
loss: 0.656222  [    0/ 3200]
loss: 0.681722  [ 1600/ 3200]
loss: 0.505319  [    0/ 3200]
loss: 0.632712  [ 1600/ 3200]
loss: 0.574269  [    0/ 3200]
loss: 0.722980  [ 1600/ 3200]
loss: 0.809137  [    0/ 3200]
loss: 0.711088  [ 1600/ 3200]
loss: 0.595664  [    0/ 3200]
loss: 0.760259  [ 1600/ 3200]
loss: 0.611225  [    0/ 3200]
loss: 0.296915  [ 1600/ 3200]
loss: 0.466742  [    0/ 3200]
loss: 0.388382  [ 1600/ 3200]
loss: 0.792988  [    0/ 3200]
loss: 0.519680  [ 1600/ 3200]
loss: 0.751843  [    0/ 3200]
loss: 1.154374  [ 1600/ 3200]
loss: 0.386700  [    0/ 3200]
loss: 0.935757  [ 1600/ 3200]
loss: 0.769408  [    0/ 3200]
loss: 0.884009  [ 1600/ 3200]
loss: 1.030580  [    0/ 3200]
loss: 0.552697  [ 1600/ 3200]
loss: 0.781053  [    0/ 3200]
loss: 0.503527  [ 1600/ 3200]
loss: 0.306351  [    0/ 3200]
loss: 0.802266  [ 1600/ 3200]
loss: 0.450173  [    0/ 3200]
loss: 0.468168  [ 1600/ 3200]
loss: 1.023524  [    0/ 3200]
loss: 0.523445  [ 1600/ 3200]
loss: 0.697480  [    0/ 3200]
loss: 0.538953  [ 1600/ 3200]
loss: 0.274798  [    0/ 3200]
loss: 0.650716  [ 1600/ 3200]
loss: 0.782734  [    0/ 3200]
loss: 0.385257  [ 1600/ 3200]
loss: 0.557074  [    0/ 3200]
loss: 0.689569  [ 1600/ 3200]
loss: 0.660385  [    0/ 3200]
loss: 0.961417  [ 1600/ 3200]
loss: 0.554171  [    0/ 3200]
loss: 0.544928  [ 1600/ 3200]
loss: 0.632543  [    0/ 3200]
loss: 0.291989  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0529
F1 Score (macro-averaged): 0.6475
Accuracy: 66.5698
Confusion Matrix:
[[105  72  33 114]
 [  9 277   0  11]
 [ 54  21 234  47]
 [ 21  53  25 300]]
Epoch 30
-------------------------------
loss: 1.006387  [    0/ 3200]
loss: 0.426284  [ 1600/ 3200]
loss: 0.446065  [    0/ 3200]
loss: 0.759357  [ 1600/ 3200]
loss: 0.703116  [    0/ 3200]
loss: 0.609590  [ 1600/ 3200]
loss: 0.924062  [    0/ 3200]
loss: 0.261257  [ 1600/ 3200]
loss: 0.629911  [    0/ 3200]
loss: 0.385518  [ 1600/ 3200]
loss: 0.634011  [    0/ 3200]
loss: 0.788369  [ 1600/ 3200]
loss: 0.559161  [    0/ 3200]
loss: 0.637405  [ 1600/ 3200]
loss: 0.603102  [    0/ 3200]
loss: 0.375674  [ 1600/ 3200]
loss: 0.682323  [    0/ 3200]
loss: 0.597122  [ 1600/ 3200]
loss: 0.701265  [    0/ 3200]
loss: 0.496475  [ 1600/ 3200]
loss: 0.668385  [    0/ 3200]
loss: 0.779786  [ 1600/ 3200]
loss: 0.899335  [    0/ 3200]
loss: 0.919697  [ 1600/ 3200]
loss: 0.339480  [    0/ 3200]
loss: 0.593356  [ 1600/ 3200]
loss: 0.581873  [    0/ 3200]
loss: 0.602136  [ 1600/ 3200]
loss: 0.597095  [    0/ 3200]
loss: 0.652582  [ 1600/ 3200]
loss: 0.630866  [    0/ 3200]
loss: 0.555175  [ 1600/ 3200]
loss: 0.773150  [    0/ 3200]
loss: 0.442228  [ 1600/ 3200]
loss: 0.684891  [    0/ 3200]
loss: 0.509940  [ 1600/ 3200]
loss: 0.445382  [    0/ 3200]
loss: 0.556403  [ 1600/ 3200]
loss: 0.530592  [    0/ 3200]
loss: 0.938569  [ 1600/ 3200]
loss: 0.459872  [    0/ 3200]
loss: 1.162323  [ 1600/ 3200]
loss: 0.350323  [    0/ 3200]
loss: 0.866733  [ 1600/ 3200]
loss: 0.930000  [    0/ 3200]
loss: 0.648201  [ 1600/ 3200]
loss: 0.514514  [    0/ 3200]
loss: 0.737238  [ 1600/ 3200]
loss: 0.610300  [    0/ 3200]
loss: 0.344035  [ 1600/ 3200]
loss: 0.954847  [    0/ 3200]
loss: 0.558401  [ 1600/ 3200]
loss: 0.565223  [    0/ 3200]
loss: 0.804763  [ 1600/ 3200]
loss: 1.103623  [    0/ 3200]
loss: 0.492062  [ 1600/ 3200]
loss: 0.547222  [    0/ 3200]
loss: 0.711552  [ 1600/ 3200]
loss: 0.822921  [    0/ 3200]
loss: 0.715526  [ 1600/ 3200]
Test Set Evaluation:
Loss: 0.0473
F1 Score (macro-averaged): 0.6776
Accuracy: 69.8401
Confusion Matrix:
[[101  42  84  97]
 [ 11 268   7  11]
 [ 26   9 299  22]
 [ 28  23  55 293]]
Time taken for epoch: 210.77 seconds

print(device)
cuda

Γραφήματα επιδόσεων

import matplotlib.pyplot as plt

def plot_results(loss_values, accuracy_values, f1_values):
    epochs = range(1, len(loss_values) + 1)

    # Create subplots with 1 row and 3 columns
    fig, axs = plt.subplots(1, 3, figsize=(16, 4))

    # Plot loss
    axs[0].plot(epochs, loss_values, label='Loss')
    axs[0].set_title('Training Loss over Epochs')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].legend()

    # Plot accuracy
    axs[1].plot(epochs, accuracy_values, label='Accuracy')
    axs[1].set_title('Training Accuracy over Epochs')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel('Accuracy')
    axs[1].legend()

    # Plot F1 score
    axs[2].plot(epochs, f1_values, label='F1 Score')
    axs[2].set_title('Training F1 Score (Macro-averaged) over Epochs')
    axs[2].set_xlabel('Epoch')
    axs[2].set_ylabel('F1 Score')
    axs[2].legend()


    # Adjust the spacing between subplots
    plt.tight_layout()

    # Display the plots
    plt.show()

#  the results
plot_results(loss_values, accuracy_values, f1_values)

Βήμα 7: Επιλογή μοντέλου

Κατά την διάρκεια εκπαίδευσης (30 εποχές) προκύπτουν διαφορετικά στιγμιότυπα του νευρωνικού μας, δηλαδή μοντέλα που έχουν διαφορετικά βάρη. Κατά την διαδικασία βελτιστοποίησης, δεν γνωρίζουμε ποιο στιγμιότυπο του μοντέλου μας έχει την καλύτερη δυνατότητα γενίκευσης. Για τον λόγο αυτό θα χρησιμοποιήσουμε το validation set στο τέλος κάθε εποχής ώστε να αξιολογούμε τα στιγμιότυπα του μοντέλου. Αποθηκεύστε το μοντέλο που έχει την καλύτερη επίδοση στην μετρική f1 για το validation set και χρησιμοποιήστε το για να μετρήσετε την απόδοση στο test set. Σχολιάστε τα αποτελέσματα.

# Variables to track the best F1 score and corresponding model
best_f1 = 0.0
best_model = None

# Optimizer, learning rate, loss function, and number of epochs
optimizer = torch.optim.SGD(model.parameters(), lr=0.002)
loss_fn = nn.CrossEntropyLoss()
num_epochs = 30

loss_values = []
accuracy_values = []
f1_values = []
cm = []

# Train the model
for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}\n-------------------------------")

    # Move the model to GPU if available
    model = model.to(device)

    # Train
    train_loop(train_loader, model, loss_fn, optimizer, num_epochs)

    # Evaluate -validation set
    val_loss, val_f1, val_acc, val_cm = test_loop(val_loader, model, loss_fn)

    # Check if current epoch's F1 score is better than the best F1 score
    if val_f1 > best_f1:
        best_f1 = val_f1
        best_model = model.state_dict().copy()

    # Append the evaluation scores to the respective lists
    loss_values.append(val_loss)
    accuracy_values.append(val_acc)
    f1_values.append(val_f1)
    cm.append(val_cm)

    # Print the evaluation scores
    print("Validation Set Evaluation:")
    print(f"Loss: {val_loss:.4f}")
    print(f"F1 Score (macro-averaged): {val_f1:.4f}")
    print(f"Accuracy: {100*val_acc:.4f}")
    print("Confusion Matrix:")
    print(val_cm)

# best model for test set evaluation
model.load_state_dict(best_model)

# Evaluate the best model on the test set
test_loss, test_f1, test_acc, test_cm = test_loop(test_loader, model, loss_fn)

#evaluation scores for the best model on the test set
print("Test Set Evaluation for the Best Model:")
print(f"Loss: {test_loss:.4f}")
print(f"F1 Score (macro-averaged): {test_f1:.4f}")
print(f"Accuracy: {100*test_acc:.4f}")
print("Confusion Matrix:")
print(test_cm)

Epoch 1
-------------------------------
loss: 0.553537  [    0/ 3200]
loss: 0.274559  [ 1600/ 3200]
loss: 0.795354  [    0/ 3200]
loss: 0.335748  [ 1600/ 3200]
loss: 0.542920  [    0/ 3200]
loss: 0.662264  [ 1600/ 3200]
loss: 0.477138  [    0/ 3200]
loss: 0.616041  [ 1600/ 3200]
loss: 0.559786  [    0/ 3200]
loss: 1.022161  [ 1600/ 3200]
loss: 0.676557  [    0/ 3200]
loss: 0.475602  [ 1600/ 3200]
loss: 0.493051  [    0/ 3200]
loss: 0.861359  [ 1600/ 3200]
loss: 0.556271  [    0/ 3200]
loss: 0.830960  [ 1600/ 3200]
loss: 0.732731  [    0/ 3200]
loss: 0.730200  [ 1600/ 3200]
loss: 0.768068  [    0/ 3200]
loss: 0.490535  [ 1600/ 3200]
loss: 0.516112  [    0/ 3200]
loss: 0.641330  [ 1600/ 3200]
loss: 0.436942  [    0/ 3200]
loss: 0.295150  [ 1600/ 3200]
loss: 0.579535  [    0/ 3200]
loss: 0.516965  [ 1600/ 3200]
loss: 0.600046  [    0/ 3200]
loss: 0.422275  [ 1600/ 3200]
loss: 0.657247  [    0/ 3200]
loss: 0.466052  [ 1600/ 3200]
loss: 0.939907  [    0/ 3200]
loss: 0.477797  [ 1600/ 3200]
loss: 0.436858  [    0/ 3200]
loss: 0.581056  [ 1600/ 3200]
loss: 0.545186  [    0/ 3200]
loss: 0.475038  [ 1600/ 3200]
loss: 0.643820  [    0/ 3200]
loss: 0.448633  [ 1600/ 3200]
loss: 0.579785  [    0/ 3200]
loss: 0.632222  [ 1600/ 3200]
loss: 0.848579  [    0/ 3200]
loss: 0.776880  [ 1600/ 3200]
loss: 0.668570  [    0/ 3200]
loss: 0.719995  [ 1600/ 3200]
loss: 1.320391  [    0/ 3200]
loss: 0.885066  [ 1600/ 3200]
loss: 0.584836  [    0/ 3200]
loss: 0.453134  [ 1600/ 3200]
loss: 0.910896  [    0/ 3200]
loss: 0.493904  [ 1600/ 3200]
loss: 0.529654  [    0/ 3200]
loss: 0.712026  [ 1600/ 3200]
loss: 0.989513  [    0/ 3200]
loss: 1.205047  [ 1600/ 3200]
loss: 0.816926  [    0/ 3200]
loss: 0.640733  [ 1600/ 3200]
loss: 0.558872  [    0/ 3200]
loss: 0.739387  [ 1600/ 3200]
loss: 0.450693  [    0/ 3200]
loss: 0.668491  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0454
F1 Score (macro-averaged): 0.7035
Accuracy: 71.0000
Confusion Matrix:
[[ 96  52  36  16]
 [  8 190   1   1]
 [ 42   6 142  10]
 [ 27  19  14 140]]
Epoch 2
-------------------------------
loss: 0.524416  [    0/ 3200]
loss: 0.581328  [ 1600/ 3200]
loss: 0.391876  [    0/ 3200]
loss: 0.389179  [ 1600/ 3200]
loss: 0.558952  [    0/ 3200]
loss: 0.712255  [ 1600/ 3200]
loss: 0.663150  [    0/ 3200]
loss: 0.366938  [ 1600/ 3200]
loss: 0.696468  [    0/ 3200]
loss: 0.592160  [ 1600/ 3200]
loss: 0.609728  [    0/ 3200]
loss: 0.442368  [ 1600/ 3200]
loss: 0.502342  [    0/ 3200]
loss: 0.750406  [ 1600/ 3200]
loss: 0.658072  [    0/ 3200]
loss: 0.974838  [ 1600/ 3200]
loss: 0.895578  [    0/ 3200]
loss: 0.591017  [ 1600/ 3200]
loss: 0.726202  [    0/ 3200]
loss: 0.751674  [ 1600/ 3200]
loss: 0.720985  [    0/ 3200]
loss: 0.797792  [ 1600/ 3200]
loss: 0.792806  [    0/ 3200]
loss: 0.636115  [ 1600/ 3200]
loss: 0.840196  [    0/ 3200]
loss: 0.340481  [ 1600/ 3200]
loss: 0.739486  [    0/ 3200]
loss: 0.774549  [ 1600/ 3200]
loss: 0.400664  [    0/ 3200]
loss: 0.811393  [ 1600/ 3200]
loss: 0.665462  [    0/ 3200]
loss: 0.702502  [ 1600/ 3200]
loss: 0.585157  [    0/ 3200]
loss: 0.367062  [ 1600/ 3200]
loss: 0.893591  [    0/ 3200]
loss: 0.835611  [ 1600/ 3200]
loss: 0.577545  [    0/ 3200]
loss: 0.760549  [ 1600/ 3200]
loss: 0.617219  [    0/ 3200]
loss: 0.657680  [ 1600/ 3200]
loss: 0.803404  [    0/ 3200]
loss: 0.489975  [ 1600/ 3200]
loss: 0.549870  [    0/ 3200]
loss: 0.547792  [ 1600/ 3200]
loss: 0.845007  [    0/ 3200]
loss: 0.697230  [ 1600/ 3200]
loss: 0.716340  [    0/ 3200]
loss: 0.676744  [ 1600/ 3200]
loss: 0.491807  [    0/ 3200]
loss: 0.538043  [ 1600/ 3200]
loss: 0.710821  [    0/ 3200]
loss: 0.815545  [ 1600/ 3200]
loss: 0.928794  [    0/ 3200]
loss: 0.294147  [ 1600/ 3200]
loss: 0.741897  [    0/ 3200]
loss: 0.626044  [ 1600/ 3200]
loss: 0.420024  [    0/ 3200]
loss: 0.609660  [ 1600/ 3200]
loss: 0.277781  [    0/ 3200]
loss: 0.591500  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0425
F1 Score (macro-averaged): 0.7108
Accuracy: 71.5000
Confusion Matrix:
[[ 92  30  50  28]
 [ 26 168   0   6]
 [ 32   1 155  12]
 [ 25   5  13 157]]
Epoch 3
-------------------------------
loss: 0.700178  [    0/ 3200]
loss: 0.669128  [ 1600/ 3200]
loss: 0.653918  [    0/ 3200]
loss: 0.500992  [ 1600/ 3200]
loss: 0.709481  [    0/ 3200]
loss: 0.991495  [ 1600/ 3200]
loss: 0.539619  [    0/ 3200]
loss: 0.722020  [ 1600/ 3200]
loss: 0.476527  [    0/ 3200]
loss: 1.059530  [ 1600/ 3200]
loss: 0.503387  [    0/ 3200]
loss: 0.502635  [ 1600/ 3200]
loss: 0.445966  [    0/ 3200]
loss: 0.961650  [ 1600/ 3200]
loss: 0.662792  [    0/ 3200]
loss: 0.579469  [ 1600/ 3200]
loss: 0.671301  [    0/ 3200]
loss: 0.479134  [ 1600/ 3200]
loss: 0.616186  [    0/ 3200]
loss: 0.396125  [ 1600/ 3200]
loss: 0.943249  [    0/ 3200]
loss: 0.849678  [ 1600/ 3200]
loss: 0.962871  [    0/ 3200]
loss: 0.463116  [ 1600/ 3200]
loss: 0.471329  [    0/ 3200]
loss: 0.433314  [ 1600/ 3200]
loss: 0.669816  [    0/ 3200]
loss: 0.676645  [ 1600/ 3200]
loss: 0.630604  [    0/ 3200]
loss: 0.937504  [ 1600/ 3200]
loss: 0.524327  [    0/ 3200]
loss: 0.442714  [ 1600/ 3200]
loss: 0.741524  [    0/ 3200]
loss: 0.963518  [ 1600/ 3200]
loss: 0.596301  [    0/ 3200]
loss: 0.632942  [ 1600/ 3200]
loss: 0.662835  [    0/ 3200]
loss: 0.571776  [ 1600/ 3200]
loss: 0.708098  [    0/ 3200]
loss: 0.322509  [ 1600/ 3200]
loss: 0.892525  [    0/ 3200]
loss: 0.377398  [ 1600/ 3200]
loss: 1.882182  [    0/ 3200]
loss: 0.511506  [ 1600/ 3200]
loss: 0.432639  [    0/ 3200]
loss: 0.735804  [ 1600/ 3200]
loss: 0.991780  [    0/ 3200]
loss: 0.843930  [ 1600/ 3200]
loss: 0.992899  [    0/ 3200]
loss: 0.600565  [ 1600/ 3200]
loss: 1.013472  [    0/ 3200]
loss: 0.411154  [ 1600/ 3200]
loss: 0.656530  [    0/ 3200]
loss: 0.436983  [ 1600/ 3200]
loss: 0.928509  [    0/ 3200]
loss: 0.278714  [ 1600/ 3200]
loss: 0.553355  [    0/ 3200]
loss: 0.427878  [ 1600/ 3200]
loss: 0.505599  [    0/ 3200]
loss: 0.801364  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0479
F1 Score (macro-averaged): 0.6229
Accuracy: 65.5000
Confusion Matrix:
[[ 40  35  92  33]
 [ 33 137   3  27]
 [  8   0 184   8]
 [  6   3  28 163]]
Epoch 4
-------------------------------
loss: 0.771811  [    0/ 3200]
loss: 1.042764  [ 1600/ 3200]
loss: 0.335884  [    0/ 3200]
loss: 0.573224  [ 1600/ 3200]
loss: 0.598086  [    0/ 3200]
loss: 0.666510  [ 1600/ 3200]
loss: 0.916293  [    0/ 3200]
loss: 0.515572  [ 1600/ 3200]
loss: 0.807651  [    0/ 3200]
loss: 0.507657  [ 1600/ 3200]
loss: 1.221432  [    0/ 3200]
loss: 0.620630  [ 1600/ 3200]
loss: 0.708893  [    0/ 3200]
loss: 0.710396  [ 1600/ 3200]
loss: 0.329718  [    0/ 3200]
loss: 0.835910  [ 1600/ 3200]
loss: 0.719573  [    0/ 3200]
loss: 0.949316  [ 1600/ 3200]
loss: 0.728840  [    0/ 3200]
loss: 0.617950  [ 1600/ 3200]
loss: 0.404077  [    0/ 3200]
loss: 0.522552  [ 1600/ 3200]
loss: 0.553168  [    0/ 3200]
loss: 1.011601  [ 1600/ 3200]
loss: 0.474144  [    0/ 3200]
loss: 0.638774  [ 1600/ 3200]
loss: 0.826944  [    0/ 3200]
loss: 0.484521  [ 1600/ 3200]
loss: 0.538127  [    0/ 3200]
loss: 0.612071  [ 1600/ 3200]
loss: 0.650630  [    0/ 3200]
loss: 1.094543  [ 1600/ 3200]
loss: 0.492083  [    0/ 3200]
loss: 0.527071  [ 1600/ 3200]
loss: 0.468056  [    0/ 3200]
loss: 0.687604  [ 1600/ 3200]
loss: 0.941732  [    0/ 3200]
loss: 1.044123  [ 1600/ 3200]
loss: 0.715883  [    0/ 3200]
loss: 0.559268  [ 1600/ 3200]
loss: 0.469972  [    0/ 3200]
loss: 0.555995  [ 1600/ 3200]
loss: 0.422785  [    0/ 3200]
loss: 0.635752  [ 1600/ 3200]
loss: 0.518682  [    0/ 3200]
loss: 0.613877  [ 1600/ 3200]
loss: 0.575125  [    0/ 3200]
loss: 0.581816  [ 1600/ 3200]
loss: 0.709816  [    0/ 3200]
loss: 0.818106  [ 1600/ 3200]
loss: 0.709462  [    0/ 3200]
loss: 0.371979  [ 1600/ 3200]
loss: 0.717809  [    0/ 3200]
loss: 0.805322  [ 1600/ 3200]
loss: 0.610267  [    0/ 3200]
loss: 0.738340  [ 1600/ 3200]
loss: 0.681500  [    0/ 3200]
loss: 0.574996  [ 1600/ 3200]
loss: 0.362214  [    0/ 3200]
loss: 0.833142  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0449
F1 Score (macro-averaged): 0.6617
Accuracy: 68.7500
Confusion Matrix:
[[ 50  44  73  33]
 [ 21 170   0   9]
 [ 21   1 170   8]
 [ 10   7  23 160]]
Epoch 5
-------------------------------
loss: 1.149993  [    0/ 3200]
loss: 0.407066  [ 1600/ 3200]
loss: 0.542488  [    0/ 3200]
loss: 0.687942  [ 1600/ 3200]
loss: 0.360611  [    0/ 3200]
loss: 0.756879  [ 1600/ 3200]
loss: 0.364914  [    0/ 3200]
loss: 0.609834  [ 1600/ 3200]
loss: 0.533614  [    0/ 3200]
loss: 0.423357  [ 1600/ 3200]
loss: 1.125720  [    0/ 3200]
loss: 0.366772  [ 1600/ 3200]
loss: 0.925287  [    0/ 3200]
loss: 0.878367  [ 1600/ 3200]
loss: 0.563319  [    0/ 3200]
loss: 0.623264  [ 1600/ 3200]
loss: 0.635405  [    0/ 3200]
loss: 0.764524  [ 1600/ 3200]
loss: 0.498781  [    0/ 3200]
loss: 0.660116  [ 1600/ 3200]
loss: 0.853520  [    0/ 3200]
loss: 0.584611  [ 1600/ 3200]
loss: 0.463057  [    0/ 3200]
loss: 0.450268  [ 1600/ 3200]
loss: 0.764855  [    0/ 3200]
loss: 0.762246  [ 1600/ 3200]
loss: 0.722220  [    0/ 3200]
loss: 0.506949  [ 1600/ 3200]
loss: 0.664838  [    0/ 3200]
loss: 0.621772  [ 1600/ 3200]
loss: 0.674800  [    0/ 3200]
loss: 0.836930  [ 1600/ 3200]
loss: 0.673380  [    0/ 3200]
loss: 0.398359  [ 1600/ 3200]
loss: 0.464132  [    0/ 3200]
loss: 0.322460  [ 1600/ 3200]
loss: 0.493680  [    0/ 3200]
loss: 0.580531  [ 1600/ 3200]
loss: 0.871980  [    0/ 3200]
loss: 0.990985  [ 1600/ 3200]
loss: 0.439648  [    0/ 3200]
loss: 0.704742  [ 1600/ 3200]
loss: 0.744434  [    0/ 3200]
loss: 0.789139  [ 1600/ 3200]
loss: 0.602467  [    0/ 3200]
loss: 0.549575  [ 1600/ 3200]
loss: 0.674192  [    0/ 3200]
loss: 0.767245  [ 1600/ 3200]
loss: 0.742082  [    0/ 3200]
loss: 0.522239  [ 1600/ 3200]
loss: 0.931038  [    0/ 3200]
loss: 0.748804  [ 1600/ 3200]
loss: 0.419110  [    0/ 3200]
loss: 0.563393  [ 1600/ 3200]
loss: 0.735575  [    0/ 3200]
loss: 0.675785  [ 1600/ 3200]
loss: 0.696055  [    0/ 3200]
loss: 0.785176  [ 1600/ 3200]
loss: 0.530877  [    0/ 3200]
loss: 0.407073  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0450
F1 Score (macro-averaged): 0.6741
Accuracy: 70.0000
Confusion Matrix:
[[ 54  48  71  27]
 [ 11 181   1   7]
 [ 20   1 170   9]
 [ 12  10  23 155]]
Epoch 6
-------------------------------
loss: 0.852865  [    0/ 3200]
loss: 0.825435  [ 1600/ 3200]
loss: 0.779340  [    0/ 3200]
loss: 0.323702  [ 1600/ 3200]
loss: 0.737531  [    0/ 3200]
loss: 0.581589  [ 1600/ 3200]
loss: 0.439889  [    0/ 3200]
loss: 0.678224  [ 1600/ 3200]
loss: 0.733107  [    0/ 3200]
loss: 0.610014  [ 1600/ 3200]
loss: 0.473404  [    0/ 3200]
loss: 0.697249  [ 1600/ 3200]
loss: 1.339847  [    0/ 3200]
loss: 0.677025  [ 1600/ 3200]
loss: 0.755678  [    0/ 3200]
loss: 0.598338  [ 1600/ 3200]
loss: 0.713283  [    0/ 3200]
loss: 0.433681  [ 1600/ 3200]
loss: 0.723256  [    0/ 3200]
loss: 0.569151  [ 1600/ 3200]
loss: 0.995510  [    0/ 3200]
loss: 0.549172  [ 1600/ 3200]
loss: 0.932290  [    0/ 3200]
loss: 0.390713  [ 1600/ 3200]
loss: 0.605808  [    0/ 3200]
loss: 0.745120  [ 1600/ 3200]
loss: 0.249808  [    0/ 3200]
loss: 0.405069  [ 1600/ 3200]
loss: 0.349371  [    0/ 3200]
loss: 0.886004  [ 1600/ 3200]
loss: 0.349853  [    0/ 3200]
loss: 0.508165  [ 1600/ 3200]
loss: 0.489306  [    0/ 3200]
loss: 0.657252  [ 1600/ 3200]
loss: 0.718457  [    0/ 3200]
loss: 0.321044  [ 1600/ 3200]
loss: 0.765391  [    0/ 3200]
loss: 0.547367  [ 1600/ 3200]
loss: 0.448411  [    0/ 3200]
loss: 0.979521  [ 1600/ 3200]
loss: 0.354104  [    0/ 3200]
loss: 0.581719  [ 1600/ 3200]
loss: 0.571146  [    0/ 3200]
loss: 0.738295  [ 1600/ 3200]
loss: 0.423915  [    0/ 3200]
loss: 0.668268  [ 1600/ 3200]
loss: 0.747133  [    0/ 3200]
loss: 0.620119  [ 1600/ 3200]
loss: 0.793730  [    0/ 3200]
loss: 0.448325  [ 1600/ 3200]
loss: 0.528222  [    0/ 3200]
loss: 0.629530  [ 1600/ 3200]
loss: 0.371830  [    0/ 3200]
loss: 0.992778  [ 1600/ 3200]
loss: 0.784455  [    0/ 3200]
loss: 0.554931  [ 1600/ 3200]
loss: 0.682727  [    0/ 3200]
loss: 0.723384  [ 1600/ 3200]
loss: 0.604367  [    0/ 3200]
loss: 1.091343  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0449
F1 Score (macro-averaged): 0.6657
Accuracy: 68.7500
Confusion Matrix:
[[ 58  45  52  45]
 [ 18 165   0  17]
 [ 23   1 158  18]
 [ 10   7  14 169]]
Epoch 7
-------------------------------
loss: 1.036304  [    0/ 3200]
loss: 0.418851  [ 1600/ 3200]
loss: 0.619048  [    0/ 3200]
loss: 0.478959  [ 1600/ 3200]
loss: 0.413217  [    0/ 3200]
loss: 0.467532  [ 1600/ 3200]
loss: 0.394045  [    0/ 3200]
loss: 0.248652  [ 1600/ 3200]
loss: 0.491096  [    0/ 3200]
loss: 0.746240  [ 1600/ 3200]
loss: 0.621029  [    0/ 3200]
loss: 0.683545  [ 1600/ 3200]
loss: 0.488832  [    0/ 3200]
loss: 0.694124  [ 1600/ 3200]
loss: 0.347782  [    0/ 3200]
loss: 0.659842  [ 1600/ 3200]
loss: 0.474311  [    0/ 3200]
loss: 0.756228  [ 1600/ 3200]
loss: 0.584659  [    0/ 3200]
loss: 0.707210  [ 1600/ 3200]
loss: 0.595074  [    0/ 3200]
loss: 0.853453  [ 1600/ 3200]
loss: 0.424186  [    0/ 3200]
loss: 0.646165  [ 1600/ 3200]
loss: 0.334596  [    0/ 3200]
loss: 0.442316  [ 1600/ 3200]
loss: 0.630109  [    0/ 3200]
loss: 0.748744  [ 1600/ 3200]
loss: 0.618404  [    0/ 3200]
loss: 0.850968  [ 1600/ 3200]
loss: 0.463391  [    0/ 3200]
loss: 0.257864  [ 1600/ 3200]
loss: 0.427485  [    0/ 3200]
loss: 0.620503  [ 1600/ 3200]
loss: 0.787565  [    0/ 3200]
loss: 0.523609  [ 1600/ 3200]
loss: 0.384186  [    0/ 3200]
loss: 0.426835  [ 1600/ 3200]
loss: 0.476236  [    0/ 3200]
loss: 0.559797  [ 1600/ 3200]
loss: 0.456947  [    0/ 3200]
loss: 1.067692  [ 1600/ 3200]
loss: 0.384074  [    0/ 3200]
loss: 0.324198  [ 1600/ 3200]
loss: 0.425182  [    0/ 3200]
loss: 0.469308  [ 1600/ 3200]
loss: 0.644680  [    0/ 3200]
loss: 0.907070  [ 1600/ 3200]
loss: 0.505000  [    0/ 3200]
loss: 0.275737  [ 1600/ 3200]
loss: 0.633944  [    0/ 3200]
loss: 0.513546  [ 1600/ 3200]
loss: 0.521327  [    0/ 3200]
loss: 0.961933  [ 1600/ 3200]
loss: 0.459935  [    0/ 3200]
loss: 0.676014  [ 1600/ 3200]
loss: 0.464652  [    0/ 3200]
loss: 0.688397  [ 1600/ 3200]
loss: 0.909533  [    0/ 3200]
loss: 0.920438  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0456
F1 Score (macro-averaged): 0.7168
Accuracy: 71.3750
Confusion Matrix:
[[117  33  26  24]
 [ 32 163   0   5]
 [ 48   1 140  11]
 [ 29   8  12 151]]
Epoch 8
-------------------------------
loss: 0.569219  [    0/ 3200]
loss: 0.680412  [ 1600/ 3200]
loss: 0.828391  [    0/ 3200]
loss: 1.009050  [ 1600/ 3200]
loss: 0.449510  [    0/ 3200]
loss: 0.424054  [ 1600/ 3200]
loss: 0.456399  [    0/ 3200]
loss: 0.603862  [ 1600/ 3200]
loss: 0.827686  [    0/ 3200]
loss: 0.587781  [ 1600/ 3200]
loss: 0.570750  [    0/ 3200]
loss: 0.521395  [ 1600/ 3200]
loss: 0.791216  [    0/ 3200]
loss: 0.758449  [ 1600/ 3200]
loss: 1.103728  [    0/ 3200]
loss: 0.575169  [ 1600/ 3200]
loss: 0.644167  [    0/ 3200]
loss: 0.454084  [ 1600/ 3200]
loss: 0.595621  [    0/ 3200]
loss: 0.790164  [ 1600/ 3200]
loss: 0.754966  [    0/ 3200]
loss: 0.360961  [ 1600/ 3200]
loss: 0.673329  [    0/ 3200]
loss: 0.706454  [ 1600/ 3200]
loss: 0.527969  [    0/ 3200]
loss: 0.737880  [ 1600/ 3200]
loss: 1.083271  [    0/ 3200]
loss: 0.409844  [ 1600/ 3200]
loss: 0.795242  [    0/ 3200]
loss: 0.383576  [ 1600/ 3200]
loss: 0.635086  [    0/ 3200]
loss: 0.607982  [ 1600/ 3200]
loss: 0.372365  [    0/ 3200]
loss: 0.651059  [ 1600/ 3200]
loss: 0.798109  [    0/ 3200]
loss: 0.765561  [ 1600/ 3200]
loss: 0.693444  [    0/ 3200]
loss: 0.271337  [ 1600/ 3200]
loss: 0.388518  [    0/ 3200]
loss: 0.637851  [ 1600/ 3200]
loss: 0.436786  [    0/ 3200]
loss: 0.412035  [ 1600/ 3200]
loss: 0.567443  [    0/ 3200]
loss: 0.589843  [ 1600/ 3200]
loss: 0.630592  [    0/ 3200]
loss: 0.444167  [ 1600/ 3200]
loss: 0.429266  [    0/ 3200]
loss: 0.342161  [ 1600/ 3200]
loss: 0.514666  [    0/ 3200]
loss: 0.379272  [ 1600/ 3200]
loss: 0.670148  [    0/ 3200]
loss: 0.373631  [ 1600/ 3200]
loss: 0.689248  [    0/ 3200]
loss: 0.683477  [ 1600/ 3200]
loss: 0.628811  [    0/ 3200]
loss: 0.610299  [ 1600/ 3200]
loss: 0.305292  [    0/ 3200]
loss: 0.398167  [ 1600/ 3200]
loss: 0.396970  [    0/ 3200]
loss: 0.519309  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0471
F1 Score (macro-averaged): 0.6936
Accuracy: 70.2500
Confusion Matrix:
[[ 86  53  44  17]
 [  9 189   0   2]
 [ 37   5 149   9]
 [ 30  21  11 138]]
Epoch 9
-------------------------------
loss: 0.860132  [    0/ 3200]
loss: 0.554996  [ 1600/ 3200]
loss: 0.601593  [    0/ 3200]
loss: 0.962964  [ 1600/ 3200]
loss: 0.325402  [    0/ 3200]
loss: 1.025515  [ 1600/ 3200]
loss: 0.608624  [    0/ 3200]
loss: 0.580277  [ 1600/ 3200]
loss: 0.589179  [    0/ 3200]
loss: 0.388740  [ 1600/ 3200]
loss: 0.514271  [    0/ 3200]
loss: 0.330278  [ 1600/ 3200]
loss: 0.885057  [    0/ 3200]
loss: 0.817631  [ 1600/ 3200]
loss: 0.206744  [    0/ 3200]
loss: 1.023567  [ 1600/ 3200]
loss: 0.583285  [    0/ 3200]
loss: 0.851661  [ 1600/ 3200]
loss: 0.812048  [    0/ 3200]
loss: 0.577221  [ 1600/ 3200]
loss: 0.764314  [    0/ 3200]
loss: 0.862406  [ 1600/ 3200]
loss: 0.799556  [    0/ 3200]
loss: 0.776652  [ 1600/ 3200]
loss: 0.651320  [    0/ 3200]
loss: 1.069481  [ 1600/ 3200]
loss: 0.672955  [    0/ 3200]
loss: 0.532589  [ 1600/ 3200]
loss: 0.702266  [    0/ 3200]
loss: 0.808716  [ 1600/ 3200]
loss: 0.799716  [    0/ 3200]
loss: 0.244036  [ 1600/ 3200]
loss: 0.591148  [    0/ 3200]
loss: 0.890263  [ 1600/ 3200]
loss: 0.857154  [    0/ 3200]
loss: 0.596618  [ 1600/ 3200]
loss: 0.449206  [    0/ 3200]
loss: 1.058094  [ 1600/ 3200]
loss: 0.483317  [    0/ 3200]
loss: 0.635658  [ 1600/ 3200]
loss: 0.542599  [    0/ 3200]
loss: 0.790001  [ 1600/ 3200]
loss: 0.575355  [    0/ 3200]
loss: 0.859993  [ 1600/ 3200]
loss: 0.679624  [    0/ 3200]
loss: 0.629885  [ 1600/ 3200]
loss: 0.423981  [    0/ 3200]
loss: 1.140821  [ 1600/ 3200]
loss: 0.350210  [    0/ 3200]
loss: 0.561540  [ 1600/ 3200]
loss: 0.745542  [    0/ 3200]
loss: 0.603819  [ 1600/ 3200]
loss: 0.770401  [    0/ 3200]
loss: 0.982745  [ 1600/ 3200]
loss: 0.810276  [    0/ 3200]
loss: 0.261721  [ 1600/ 3200]
loss: 0.520281  [    0/ 3200]
loss: 0.606268  [ 1600/ 3200]
loss: 0.751601  [    0/ 3200]
loss: 0.575869  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0437
F1 Score (macro-averaged): 0.7025
Accuracy: 71.2500
Confusion Matrix:
[[ 82  41  43  34]
 [ 17 177   0   6]
 [ 35   1 151  13]
 [ 18   9  13 160]]
Epoch 10
-------------------------------
loss: 0.532607  [    0/ 3200]
loss: 0.519731  [ 1600/ 3200]
loss: 0.462029  [    0/ 3200]
loss: 0.465691  [ 1600/ 3200]
loss: 0.965278  [    0/ 3200]
loss: 0.480176  [ 1600/ 3200]
loss: 1.092300  [    0/ 3200]
loss: 0.730863  [ 1600/ 3200]
loss: 0.785917  [    0/ 3200]
loss: 0.539200  [ 1600/ 3200]
loss: 0.598441  [    0/ 3200]
loss: 0.811143  [ 1600/ 3200]
loss: 0.395042  [    0/ 3200]
loss: 0.832224  [ 1600/ 3200]
loss: 0.537885  [    0/ 3200]
loss: 0.720337  [ 1600/ 3200]
loss: 0.847892  [    0/ 3200]
loss: 0.541685  [ 1600/ 3200]
loss: 0.887779  [    0/ 3200]
loss: 0.659492  [ 1600/ 3200]
loss: 0.621508  [    0/ 3200]
loss: 0.538386  [ 1600/ 3200]
loss: 0.614812  [    0/ 3200]
loss: 0.573962  [ 1600/ 3200]
loss: 0.449321  [    0/ 3200]
loss: 0.376583  [ 1600/ 3200]
loss: 0.539335  [    0/ 3200]
loss: 0.362697  [ 1600/ 3200]
loss: 0.796372  [    0/ 3200]
loss: 0.395316  [ 1600/ 3200]
loss: 0.557535  [    0/ 3200]
loss: 0.967031  [ 1600/ 3200]
loss: 0.915119  [    0/ 3200]
loss: 0.600439  [ 1600/ 3200]
loss: 0.403441  [    0/ 3200]
loss: 0.698537  [ 1600/ 3200]
loss: 0.657472  [    0/ 3200]
loss: 0.297109  [ 1600/ 3200]
loss: 0.617680  [    0/ 3200]
loss: 0.545500  [ 1600/ 3200]
loss: 0.858236  [    0/ 3200]
loss: 0.390512  [ 1600/ 3200]
loss: 0.548789  [    0/ 3200]
loss: 0.873470  [ 1600/ 3200]
loss: 0.374615  [    0/ 3200]
loss: 0.814325  [ 1600/ 3200]
loss: 0.452962  [    0/ 3200]
loss: 0.689974  [ 1600/ 3200]
loss: 0.301605  [    0/ 3200]
loss: 0.390664  [ 1600/ 3200]
loss: 0.349559  [    0/ 3200]
loss: 0.573526  [ 1600/ 3200]
loss: 0.707391  [    0/ 3200]
loss: 0.624345  [ 1600/ 3200]
loss: 0.592517  [    0/ 3200]
loss: 0.482261  [ 1600/ 3200]
loss: 0.385539  [    0/ 3200]
loss: 0.432855  [ 1600/ 3200]
loss: 0.728744  [    0/ 3200]
loss: 0.477157  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0479
F1 Score (macro-averaged): 0.6839
Accuracy: 69.1250
Confusion Matrix:
[[ 96  42  17  45]
 [ 12 179   0   9]
 [ 56   2 112  30]
 [ 18   9   7 166]]
Epoch 11
-------------------------------
loss: 0.607105  [    0/ 3200]
loss: 0.591966  [ 1600/ 3200]
loss: 0.398420  [    0/ 3200]
loss: 0.752432  [ 1600/ 3200]
loss: 0.701381  [    0/ 3200]
loss: 0.354071  [ 1600/ 3200]
loss: 0.688303  [    0/ 3200]
loss: 0.891701  [ 1600/ 3200]
loss: 0.218497  [    0/ 3200]
loss: 1.085347  [ 1600/ 3200]
loss: 0.813654  [    0/ 3200]
loss: 0.634100  [ 1600/ 3200]
loss: 0.538900  [    0/ 3200]
loss: 0.637510  [ 1600/ 3200]
loss: 0.732891  [    0/ 3200]
loss: 0.596396  [ 1600/ 3200]
loss: 0.829876  [    0/ 3200]
loss: 0.461608  [ 1600/ 3200]
loss: 0.487664  [    0/ 3200]
loss: 0.498882  [ 1600/ 3200]
loss: 0.803422  [    0/ 3200]
loss: 0.882519  [ 1600/ 3200]
loss: 0.732431  [    0/ 3200]
loss: 0.672586  [ 1600/ 3200]
loss: 0.404480  [    0/ 3200]
loss: 0.620811  [ 1600/ 3200]
loss: 0.701373  [    0/ 3200]
loss: 0.389946  [ 1600/ 3200]
loss: 0.897452  [    0/ 3200]
loss: 0.674248  [ 1600/ 3200]
loss: 0.522450  [    0/ 3200]
loss: 0.869672  [ 1600/ 3200]
loss: 0.924423  [    0/ 3200]
loss: 0.479247  [ 1600/ 3200]
loss: 0.249830  [    0/ 3200]
loss: 0.390261  [ 1600/ 3200]
loss: 0.719089  [    0/ 3200]
loss: 0.637610  [ 1600/ 3200]
loss: 0.539351  [    0/ 3200]
loss: 0.941314  [ 1600/ 3200]
loss: 0.741121  [    0/ 3200]
loss: 0.514095  [ 1600/ 3200]
loss: 1.130436  [    0/ 3200]
loss: 0.971237  [ 1600/ 3200]
loss: 0.609946  [    0/ 3200]
loss: 0.340974  [ 1600/ 3200]
loss: 0.724142  [    0/ 3200]
loss: 0.583120  [ 1600/ 3200]
loss: 0.724839  [    0/ 3200]
loss: 0.526019  [ 1600/ 3200]
loss: 0.374304  [    0/ 3200]
loss: 0.654602  [ 1600/ 3200]
loss: 0.679134  [    0/ 3200]
loss: 0.468950  [ 1600/ 3200]
loss: 0.307442  [    0/ 3200]
loss: 0.847697  [ 1600/ 3200]
loss: 0.442403  [    0/ 3200]
loss: 0.500770  [ 1600/ 3200]
loss: 0.700774  [    0/ 3200]
loss: 0.899811  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0448
F1 Score (macro-averaged): 0.6953
Accuracy: 70.3750
Confusion Matrix:
[[ 84  43  35  38]
 [ 19 167   0  14]
 [ 38   1 146  15]
 [ 16   8  10 166]]
Epoch 12
-------------------------------
loss: 0.657667  [    0/ 3200]
loss: 0.606040  [ 1600/ 3200]
loss: 0.813707  [    0/ 3200]
loss: 0.500096  [ 1600/ 3200]
loss: 0.792541  [    0/ 3200]
loss: 0.970913  [ 1600/ 3200]
loss: 0.994634  [    0/ 3200]
loss: 0.384247  [ 1600/ 3200]
loss: 0.570372  [    0/ 3200]
loss: 0.698235  [ 1600/ 3200]
loss: 0.589954  [    0/ 3200]
loss: 0.884819  [ 1600/ 3200]
loss: 0.852014  [    0/ 3200]
loss: 0.484100  [ 1600/ 3200]
loss: 0.811153  [    0/ 3200]
loss: 0.290731  [ 1600/ 3200]
loss: 0.726575  [    0/ 3200]
loss: 0.464658  [ 1600/ 3200]
loss: 0.548235  [    0/ 3200]
loss: 1.167997  [ 1600/ 3200]
loss: 0.532554  [    0/ 3200]
loss: 0.590478  [ 1600/ 3200]
loss: 0.564317  [    0/ 3200]
loss: 0.319014  [ 1600/ 3200]
loss: 0.368183  [    0/ 3200]
loss: 0.598805  [ 1600/ 3200]
loss: 0.482397  [    0/ 3200]
loss: 0.951925  [ 1600/ 3200]
loss: 0.298451  [    0/ 3200]
loss: 0.345900  [ 1600/ 3200]
loss: 0.483869  [    0/ 3200]
loss: 0.440700  [ 1600/ 3200]
loss: 0.368260  [    0/ 3200]
loss: 0.757178  [ 1600/ 3200]
loss: 0.945684  [    0/ 3200]
loss: 0.344899  [ 1600/ 3200]
loss: 0.934042  [    0/ 3200]
loss: 0.381025  [ 1600/ 3200]
loss: 0.305009  [    0/ 3200]
loss: 0.471741  [ 1600/ 3200]
loss: 0.499041  [    0/ 3200]
loss: 0.432990  [ 1600/ 3200]
loss: 0.727811  [    0/ 3200]
loss: 0.562456  [ 1600/ 3200]
loss: 0.631292  [    0/ 3200]
loss: 0.908655  [ 1600/ 3200]
loss: 0.660663  [    0/ 3200]
loss: 0.202014  [ 1600/ 3200]
loss: 0.642646  [    0/ 3200]
loss: 0.578377  [ 1600/ 3200]
loss: 0.697345  [    0/ 3200]
loss: 0.953235  [ 1600/ 3200]
loss: 0.840689  [    0/ 3200]
loss: 0.689020  [ 1600/ 3200]
loss: 0.699224  [    0/ 3200]
loss: 0.347962  [ 1600/ 3200]
loss: 0.772997  [    0/ 3200]
loss: 0.646331  [ 1600/ 3200]
loss: 0.475522  [    0/ 3200]
loss: 0.434669  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0453
F1 Score (macro-averaged): 0.6855
Accuracy: 68.2500
Confusion Matrix:
[[106  28  33  33]
 [ 44 137   0  19]
 [ 47   0 141  12]
 [ 25   3  10 162]]
Epoch 13
-------------------------------
loss: 0.711890  [    0/ 3200]
loss: 0.896582  [ 1600/ 3200]
loss: 0.649842  [    0/ 3200]
loss: 0.516227  [ 1600/ 3200]
loss: 0.715021  [    0/ 3200]
loss: 0.817706  [ 1600/ 3200]
loss: 0.798736  [    0/ 3200]
loss: 0.521954  [ 1600/ 3200]
loss: 0.562241  [    0/ 3200]
loss: 0.839598  [ 1600/ 3200]
loss: 0.771773  [    0/ 3200]
loss: 0.825799  [ 1600/ 3200]
loss: 0.984395  [    0/ 3200]
loss: 0.436092  [ 1600/ 3200]
loss: 0.272984  [    0/ 3200]
loss: 0.376598  [ 1600/ 3200]
loss: 0.418730  [    0/ 3200]
loss: 0.746351  [ 1600/ 3200]
loss: 0.674100  [    0/ 3200]
loss: 0.500653  [ 1600/ 3200]
loss: 0.455143  [    0/ 3200]
loss: 0.516022  [ 1600/ 3200]
loss: 0.591665  [    0/ 3200]
loss: 0.378763  [ 1600/ 3200]
loss: 0.326492  [    0/ 3200]
loss: 0.611641  [ 1600/ 3200]
loss: 0.709643  [    0/ 3200]
loss: 0.398953  [ 1600/ 3200]
loss: 0.453177  [    0/ 3200]
loss: 0.711518  [ 1600/ 3200]
loss: 0.842064  [    0/ 3200]
loss: 0.421046  [ 1600/ 3200]
loss: 0.274221  [    0/ 3200]
loss: 0.782395  [ 1600/ 3200]
loss: 0.601854  [    0/ 3200]
loss: 1.186489  [ 1600/ 3200]
loss: 0.484408  [    0/ 3200]
loss: 0.906838  [ 1600/ 3200]
loss: 0.769391  [    0/ 3200]
loss: 0.397791  [ 1600/ 3200]
loss: 0.707845  [    0/ 3200]
loss: 0.567532  [ 1600/ 3200]
loss: 0.793853  [    0/ 3200]
loss: 0.581258  [ 1600/ 3200]
loss: 0.497511  [    0/ 3200]
loss: 0.938189  [ 1600/ 3200]
loss: 0.563454  [    0/ 3200]
loss: 0.496454  [ 1600/ 3200]
loss: 0.402334  [    0/ 3200]
loss: 0.641850  [ 1600/ 3200]
loss: 0.651070  [    0/ 3200]
loss: 0.495893  [ 1600/ 3200]
loss: 0.585701  [    0/ 3200]
loss: 0.371765  [ 1600/ 3200]
loss: 0.498915  [    0/ 3200]
loss: 0.691835  [ 1600/ 3200]
loss: 0.610540  [    0/ 3200]
loss: 1.226418  [ 1600/ 3200]
loss: 0.814548  [    0/ 3200]
loss: 0.912532  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0470
F1 Score (macro-averaged): 0.6864
Accuracy: 68.5000
Confusion Matrix:
[[102  36  47  15]
 [ 31 167   0   2]
 [ 41   1 150   8]
 [ 40  13  18 129]]
Epoch 14
-------------------------------
loss: 0.475117  [    0/ 3200]
loss: 0.505738  [ 1600/ 3200]
loss: 0.495072  [    0/ 3200]
loss: 0.597963  [ 1600/ 3200]
loss: 0.394412  [    0/ 3200]
loss: 0.480307  [ 1600/ 3200]
loss: 0.650019  [    0/ 3200]
loss: 0.484005  [ 1600/ 3200]
loss: 0.633285  [    0/ 3200]
loss: 0.452190  [ 1600/ 3200]
loss: 0.487843  [    0/ 3200]
loss: 0.779238  [ 1600/ 3200]
loss: 0.839179  [    0/ 3200]
loss: 0.732322  [ 1600/ 3200]
loss: 0.672891  [    0/ 3200]
loss: 0.696954  [ 1600/ 3200]
loss: 0.682214  [    0/ 3200]
loss: 0.374808  [ 1600/ 3200]
loss: 0.356762  [    0/ 3200]
loss: 0.650084  [ 1600/ 3200]
loss: 0.697685  [    0/ 3200]
loss: 1.043449  [ 1600/ 3200]
loss: 0.499333  [    0/ 3200]
loss: 0.383238  [ 1600/ 3200]
loss: 0.449397  [    0/ 3200]
loss: 0.651178  [ 1600/ 3200]
loss: 0.312326  [    0/ 3200]
loss: 0.607713  [ 1600/ 3200]
loss: 0.384790  [    0/ 3200]
loss: 0.451795  [ 1600/ 3200]
loss: 0.575331  [    0/ 3200]
loss: 0.727815  [ 1600/ 3200]
loss: 0.610516  [    0/ 3200]
loss: 0.690383  [ 1600/ 3200]
loss: 0.277129  [    0/ 3200]
loss: 0.629984  [ 1600/ 3200]
loss: 0.718715  [    0/ 3200]
loss: 1.063519  [ 1600/ 3200]
loss: 0.446045  [    0/ 3200]
loss: 0.475191  [ 1600/ 3200]
loss: 0.543652  [    0/ 3200]
loss: 0.858556  [ 1600/ 3200]
loss: 0.357439  [    0/ 3200]
loss: 0.743461  [ 1600/ 3200]
loss: 0.470815  [    0/ 3200]
loss: 0.537300  [ 1600/ 3200]
loss: 0.643852  [    0/ 3200]
loss: 0.566266  [ 1600/ 3200]
loss: 0.708281  [    0/ 3200]
loss: 0.336921  [ 1600/ 3200]
loss: 0.652151  [    0/ 3200]
loss: 0.515114  [ 1600/ 3200]
loss: 0.785277  [    0/ 3200]
loss: 0.894531  [ 1600/ 3200]
loss: 0.699148  [    0/ 3200]
loss: 0.470315  [ 1600/ 3200]
loss: 0.735836  [    0/ 3200]
loss: 0.512652  [ 1600/ 3200]
loss: 0.723269  [    0/ 3200]
loss: 0.729128  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0489
F1 Score (macro-averaged): 0.6532
Accuracy: 65.5000
Confusion Matrix:
[[ 93  30  21  56]
 [ 41 128   0  31]
 [ 44   0 125  31]
 [ 13   2   7 178]]
Epoch 15
-------------------------------
loss: 0.392945  [    0/ 3200]
loss: 0.698527  [ 1600/ 3200]
loss: 0.400274  [    0/ 3200]
loss: 1.055157  [ 1600/ 3200]
loss: 0.453971  [    0/ 3200]
loss: 0.549839  [ 1600/ 3200]
loss: 0.868332  [    0/ 3200]
loss: 0.463846  [ 1600/ 3200]
loss: 1.025642  [    0/ 3200]
loss: 0.682904  [ 1600/ 3200]
loss: 0.765127  [    0/ 3200]
loss: 0.386326  [ 1600/ 3200]
loss: 0.837598  [    0/ 3200]
loss: 0.540515  [ 1600/ 3200]
loss: 0.434874  [    0/ 3200]
loss: 0.582154  [ 1600/ 3200]
loss: 0.495035  [    0/ 3200]
loss: 0.446790  [ 1600/ 3200]
loss: 0.530101  [    0/ 3200]
loss: 0.404251  [ 1600/ 3200]
loss: 0.567841  [    0/ 3200]
loss: 0.419961  [ 1600/ 3200]
loss: 1.075200  [    0/ 3200]
loss: 0.785739  [ 1600/ 3200]
loss: 0.700590  [    0/ 3200]
loss: 0.650958  [ 1600/ 3200]
loss: 0.552297  [    0/ 3200]
loss: 0.451471  [ 1600/ 3200]
loss: 0.408526  [    0/ 3200]
loss: 0.618427  [ 1600/ 3200]
loss: 0.506710  [    0/ 3200]
loss: 0.661000  [ 1600/ 3200]
loss: 0.598437  [    0/ 3200]
loss: 0.785564  [ 1600/ 3200]
loss: 0.453811  [    0/ 3200]
loss: 0.429740  [ 1600/ 3200]
loss: 0.507296  [    0/ 3200]
loss: 0.771971  [ 1600/ 3200]
loss: 0.821314  [    0/ 3200]
loss: 0.688312  [ 1600/ 3200]
loss: 0.655740  [    0/ 3200]
loss: 0.595677  [ 1600/ 3200]
loss: 0.574197  [    0/ 3200]
loss: 0.709668  [ 1600/ 3200]
loss: 0.781768  [    0/ 3200]
loss: 0.551747  [ 1600/ 3200]
loss: 0.853174  [    0/ 3200]
loss: 0.607279  [ 1600/ 3200]
loss: 0.626663  [    0/ 3200]
loss: 0.942986  [ 1600/ 3200]
loss: 0.595943  [    0/ 3200]
loss: 0.770686  [ 1600/ 3200]
loss: 0.361811  [    0/ 3200]
loss: 0.694288  [ 1600/ 3200]
loss: 0.508688  [    0/ 3200]
loss: 0.507532  [ 1600/ 3200]
loss: 0.762688  [    0/ 3200]
loss: 0.442713  [ 1600/ 3200]
loss: 0.669208  [    0/ 3200]
loss: 0.389622  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0474
F1 Score (macro-averaged): 0.6466
Accuracy: 68.5000
Confusion Matrix:
[[ 38  42  72  48]
 [ 15 164   0  21]
 [ 14   0 169  17]
 [  4   4  15 177]]
Epoch 16
-------------------------------
loss: 0.390073  [    0/ 3200]
loss: 0.682354  [ 1600/ 3200]
loss: 0.844156  [    0/ 3200]
loss: 0.605540  [ 1600/ 3200]
loss: 0.672801  [    0/ 3200]
loss: 0.722143  [ 1600/ 3200]
loss: 0.481507  [    0/ 3200]
loss: 0.459381  [ 1600/ 3200]
loss: 0.614845  [    0/ 3200]
loss: 0.449034  [ 1600/ 3200]
loss: 0.942504  [    0/ 3200]
loss: 0.336317  [ 1600/ 3200]
loss: 0.595581  [    0/ 3200]
loss: 0.600671  [ 1600/ 3200]
loss: 0.672187  [    0/ 3200]
loss: 0.477570  [ 1600/ 3200]
loss: 0.706912  [    0/ 3200]
loss: 0.561415  [ 1600/ 3200]
loss: 0.771114  [    0/ 3200]
loss: 0.371028  [ 1600/ 3200]
loss: 0.718174  [    0/ 3200]
loss: 0.846892  [ 1600/ 3200]
loss: 0.552156  [    0/ 3200]
loss: 0.914805  [ 1600/ 3200]
loss: 0.693079  [    0/ 3200]
loss: 0.463020  [ 1600/ 3200]
loss: 0.306486  [    0/ 3200]
loss: 1.019184  [ 1600/ 3200]
loss: 0.565667  [    0/ 3200]
loss: 0.533741  [ 1600/ 3200]
loss: 0.537802  [    0/ 3200]
loss: 0.772643  [ 1600/ 3200]
loss: 0.352352  [    0/ 3200]
loss: 0.868788  [ 1600/ 3200]
loss: 0.611123  [    0/ 3200]
loss: 0.493213  [ 1600/ 3200]
loss: 0.573801  [    0/ 3200]
loss: 0.635633  [ 1600/ 3200]
loss: 0.679460  [    0/ 3200]
loss: 0.342797  [ 1600/ 3200]
loss: 0.447777  [    0/ 3200]
loss: 0.818523  [ 1600/ 3200]
loss: 0.440864  [    0/ 3200]
loss: 0.373651  [ 1600/ 3200]
loss: 0.634221  [    0/ 3200]
loss: 0.496190  [ 1600/ 3200]
loss: 0.684467  [    0/ 3200]
loss: 0.770626  [ 1600/ 3200]
loss: 1.010952  [    0/ 3200]
loss: 0.977085  [ 1600/ 3200]
loss: 0.478290  [    0/ 3200]
loss: 0.774125  [ 1600/ 3200]
loss: 0.941301  [    0/ 3200]
loss: 0.890134  [ 1600/ 3200]
loss: 0.561646  [    0/ 3200]
loss: 0.681941  [ 1600/ 3200]
loss: 0.504644  [    0/ 3200]
loss: 0.305304  [ 1600/ 3200]
loss: 0.798598  [    0/ 3200]
loss: 0.719262  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0469
F1 Score (macro-averaged): 0.6857
Accuracy: 69.2500
Confusion Matrix:
[[ 94  39  19  48]
 [ 14 180   0   6]
 [ 56   4 117  23]
 [ 18  11   8 163]]
Epoch 17
-------------------------------
loss: 0.477609  [    0/ 3200]
loss: 0.481011  [ 1600/ 3200]
loss: 0.733923  [    0/ 3200]
loss: 0.483069  [ 1600/ 3200]
loss: 0.834744  [    0/ 3200]
loss: 0.744064  [ 1600/ 3200]
loss: 0.677031  [    0/ 3200]
loss: 0.511090  [ 1600/ 3200]
loss: 0.731927  [    0/ 3200]
loss: 0.937427  [ 1600/ 3200]
loss: 0.304414  [    0/ 3200]
loss: 0.558306  [ 1600/ 3200]
loss: 0.505175  [    0/ 3200]
loss: 0.456594  [ 1600/ 3200]
loss: 0.369554  [    0/ 3200]
loss: 0.603940  [ 1600/ 3200]
loss: 0.541097  [    0/ 3200]
loss: 0.609481  [ 1600/ 3200]
loss: 0.702363  [    0/ 3200]
loss: 0.414487  [ 1600/ 3200]
loss: 0.890856  [    0/ 3200]
loss: 0.591348  [ 1600/ 3200]
loss: 0.440912  [    0/ 3200]
loss: 0.630581  [ 1600/ 3200]
loss: 0.777570  [    0/ 3200]
loss: 0.501191  [ 1600/ 3200]
loss: 0.336483  [    0/ 3200]
loss: 0.932361  [ 1600/ 3200]
loss: 0.793456  [    0/ 3200]
loss: 0.485261  [ 1600/ 3200]
loss: 0.459354  [    0/ 3200]
loss: 0.491722  [ 1600/ 3200]
loss: 0.342230  [    0/ 3200]
loss: 0.885220  [ 1600/ 3200]
loss: 0.575716  [    0/ 3200]
loss: 0.527551  [ 1600/ 3200]
loss: 0.275652  [    0/ 3200]
loss: 0.617990  [ 1600/ 3200]
loss: 0.544397  [    0/ 3200]
loss: 0.588742  [ 1600/ 3200]
loss: 0.336801  [    0/ 3200]
loss: 0.448491  [ 1600/ 3200]
loss: 0.578899  [    0/ 3200]
loss: 0.789103  [ 1600/ 3200]
loss: 0.724153  [    0/ 3200]
loss: 0.417250  [ 1600/ 3200]
loss: 0.445252  [    0/ 3200]
loss: 0.601433  [ 1600/ 3200]
loss: 0.510058  [    0/ 3200]
loss: 0.531925  [ 1600/ 3200]
loss: 0.580701  [    0/ 3200]
loss: 0.555765  [ 1600/ 3200]
loss: 0.985481  [    0/ 3200]
loss: 0.542318  [ 1600/ 3200]
loss: 0.994201  [    0/ 3200]
loss: 0.270595  [ 1600/ 3200]
loss: 0.555558  [    0/ 3200]
loss: 0.823573  [ 1600/ 3200]
loss: 0.563426  [    0/ 3200]
loss: 0.402600  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0476
F1 Score (macro-averaged): 0.7057
Accuracy: 71.2500
Confusion Matrix:
[[ 96  44  28  32]
 [  7 187   0   6]
 [ 45   2 141  12]
 [ 23  21  10 146]]
Epoch 18
-------------------------------
loss: 0.427989  [    0/ 3200]
loss: 0.966987  [ 1600/ 3200]
loss: 0.201718  [    0/ 3200]
loss: 0.768341  [ 1600/ 3200]
loss: 0.791180  [    0/ 3200]
loss: 0.594141  [ 1600/ 3200]
loss: 0.456338  [    0/ 3200]
loss: 0.577213  [ 1600/ 3200]
loss: 0.750964  [    0/ 3200]
loss: 0.643455  [ 1600/ 3200]
loss: 0.872426  [    0/ 3200]
loss: 0.362016  [ 1600/ 3200]
loss: 1.182885  [    0/ 3200]
loss: 0.619505  [ 1600/ 3200]
loss: 0.508153  [    0/ 3200]
loss: 0.692527  [ 1600/ 3200]
loss: 0.469522  [    0/ 3200]
loss: 0.439112  [ 1600/ 3200]
loss: 0.379419  [    0/ 3200]
loss: 0.750663  [ 1600/ 3200]
loss: 0.562703  [    0/ 3200]
loss: 0.440938  [ 1600/ 3200]
loss: 0.479309  [    0/ 3200]
loss: 0.564787  [ 1600/ 3200]
loss: 0.680059  [    0/ 3200]
loss: 0.606130  [ 1600/ 3200]
loss: 0.342682  [    0/ 3200]
loss: 0.374477  [ 1600/ 3200]
loss: 0.892329  [    0/ 3200]
loss: 0.915836  [ 1600/ 3200]
loss: 0.352140  [    0/ 3200]
loss: 0.563611  [ 1600/ 3200]
loss: 0.289205  [    0/ 3200]
loss: 0.708906  [ 1600/ 3200]
loss: 0.739753  [    0/ 3200]
loss: 0.538325  [ 1600/ 3200]
loss: 0.299238  [    0/ 3200]
loss: 0.746483  [ 1600/ 3200]
loss: 0.385060  [    0/ 3200]
loss: 0.221156  [ 1600/ 3200]
loss: 0.439369  [    0/ 3200]
loss: 0.524066  [ 1600/ 3200]
loss: 0.397916  [    0/ 3200]
loss: 0.960544  [ 1600/ 3200]
loss: 0.608755  [    0/ 3200]
loss: 0.296639  [ 1600/ 3200]
loss: 0.645513  [    0/ 3200]
loss: 0.558435  [ 1600/ 3200]
loss: 0.312944  [    0/ 3200]
loss: 0.317109  [ 1600/ 3200]
loss: 0.874091  [    0/ 3200]
loss: 0.551033  [ 1600/ 3200]
loss: 0.408665  [    0/ 3200]
loss: 0.667816  [ 1600/ 3200]
loss: 1.115914  [    0/ 3200]
loss: 0.522428  [ 1600/ 3200]
loss: 0.477506  [    0/ 3200]
loss: 0.529544  [ 1600/ 3200]
loss: 0.816055  [    0/ 3200]
loss: 0.617116  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0459
F1 Score (macro-averaged): 0.6579
Accuracy: 68.1250
Confusion Matrix:
[[ 53  40  66  41]
 [ 25 162   0  13]
 [ 21   1 165  13]
 [  8   6  21 165]]
Epoch 19
-------------------------------
loss: 0.598901  [    0/ 3200]
loss: 0.718456  [ 1600/ 3200]
loss: 0.505071  [    0/ 3200]
loss: 0.966991  [ 1600/ 3200]
loss: 0.494881  [    0/ 3200]
loss: 0.622811  [ 1600/ 3200]
loss: 0.501613  [    0/ 3200]
loss: 0.808203  [ 1600/ 3200]
loss: 0.758226  [    0/ 3200]
loss: 0.608588  [ 1600/ 3200]
loss: 1.074939  [    0/ 3200]
loss: 0.613971  [ 1600/ 3200]
loss: 0.518224  [    0/ 3200]
loss: 0.546646  [ 1600/ 3200]
loss: 0.450061  [    0/ 3200]
loss: 0.980067  [ 1600/ 3200]
loss: 0.282734  [    0/ 3200]
loss: 0.429637  [ 1600/ 3200]
loss: 0.738829  [    0/ 3200]
loss: 0.539533  [ 1600/ 3200]
loss: 0.335382  [    0/ 3200]
loss: 0.482703  [ 1600/ 3200]
loss: 0.319125  [    0/ 3200]
loss: 0.559717  [ 1600/ 3200]
loss: 0.579334  [    0/ 3200]
loss: 0.934176  [ 1600/ 3200]
loss: 0.525760  [    0/ 3200]
loss: 0.667644  [ 1600/ 3200]
loss: 0.343551  [    0/ 3200]
loss: 0.255397  [ 1600/ 3200]
loss: 0.722303  [    0/ 3200]
loss: 0.748673  [ 1600/ 3200]
loss: 1.074277  [    0/ 3200]
loss: 1.014095  [ 1600/ 3200]
loss: 0.822766  [    0/ 3200]
loss: 0.403374  [ 1600/ 3200]
loss: 0.838795  [    0/ 3200]
loss: 0.367640  [ 1600/ 3200]
loss: 0.640914  [    0/ 3200]
loss: 0.579558  [ 1600/ 3200]
loss: 0.378297  [    0/ 3200]
loss: 0.236730  [ 1600/ 3200]
loss: 0.901357  [    0/ 3200]
loss: 0.598679  [ 1600/ 3200]
loss: 0.573904  [    0/ 3200]
loss: 0.782185  [ 1600/ 3200]
loss: 0.623574  [    0/ 3200]
loss: 0.693956  [ 1600/ 3200]
loss: 0.486031  [    0/ 3200]
loss: 0.368349  [ 1600/ 3200]
loss: 0.515527  [    0/ 3200]
loss: 0.477593  [ 1600/ 3200]
loss: 0.527416  [    0/ 3200]
loss: 1.137087  [ 1600/ 3200]
loss: 0.406708  [    0/ 3200]
loss: 0.406105  [ 1600/ 3200]
loss: 0.507429  [    0/ 3200]
loss: 0.381515  [ 1600/ 3200]
loss: 0.553332  [    0/ 3200]
loss: 0.809733  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0495
F1 Score (macro-averaged): 0.6848
Accuracy: 69.3750
Confusion Matrix:
[[ 90  55  23  32]
 [  7 192   0   1]
 [ 51   4 128  17]
 [ 20  25  10 145]]
Epoch 20
-------------------------------
loss: 1.136965  [    0/ 3200]
loss: 0.444598  [ 1600/ 3200]
loss: 0.519003  [    0/ 3200]
loss: 0.503471  [ 1600/ 3200]
loss: 0.679278  [    0/ 3200]
loss: 0.875459  [ 1600/ 3200]
loss: 0.515349  [    0/ 3200]
loss: 0.798449  [ 1600/ 3200]
loss: 0.419083  [    0/ 3200]
loss: 0.454401  [ 1600/ 3200]
loss: 1.100677  [    0/ 3200]
loss: 0.221957  [ 1600/ 3200]
loss: 0.275886  [    0/ 3200]
loss: 0.470116  [ 1600/ 3200]
loss: 0.598246  [    0/ 3200]
loss: 0.655391  [ 1600/ 3200]
loss: 1.089154  [    0/ 3200]
loss: 0.404779  [ 1600/ 3200]
loss: 0.937696  [    0/ 3200]
loss: 0.769279  [ 1600/ 3200]
loss: 0.621314  [    0/ 3200]
loss: 0.435317  [ 1600/ 3200]
loss: 0.603068  [    0/ 3200]
loss: 0.607500  [ 1600/ 3200]
loss: 0.635091  [    0/ 3200]
loss: 0.294819  [ 1600/ 3200]
loss: 0.928104  [    0/ 3200]
loss: 0.641275  [ 1600/ 3200]
loss: 0.364383  [    0/ 3200]
loss: 0.788466  [ 1600/ 3200]
loss: 0.373690  [    0/ 3200]
loss: 0.518875  [ 1600/ 3200]
loss: 0.396925  [    0/ 3200]
loss: 0.836750  [ 1600/ 3200]
loss: 0.923391  [    0/ 3200]
loss: 0.959914  [ 1600/ 3200]
loss: 0.550040  [    0/ 3200]
loss: 0.615495  [ 1600/ 3200]
loss: 0.361253  [    0/ 3200]
loss: 0.747743  [ 1600/ 3200]
loss: 0.735477  [    0/ 3200]
loss: 0.511428  [ 1600/ 3200]
loss: 0.362617  [    0/ 3200]
loss: 0.567492  [ 1600/ 3200]
loss: 1.007840  [    0/ 3200]
loss: 0.502278  [ 1600/ 3200]
loss: 0.598976  [    0/ 3200]
loss: 0.363380  [ 1600/ 3200]
loss: 0.477639  [    0/ 3200]
loss: 0.604450  [ 1600/ 3200]
loss: 0.454818  [    0/ 3200]
loss: 0.480237  [ 1600/ 3200]
loss: 0.709862  [    0/ 3200]
loss: 0.465168  [ 1600/ 3200]
loss: 0.821961  [    0/ 3200]
loss: 0.960360  [ 1600/ 3200]
loss: 0.791454  [    0/ 3200]
loss: 0.914023  [ 1600/ 3200]
loss: 0.624885  [    0/ 3200]
loss: 0.677788  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0473
F1 Score (macro-averaged): 0.6852
Accuracy: 68.3750
Confusion Matrix:
[[104  25  32  39]
 [ 43 137   0  20]
 [ 43   1 141  15]
 [ 21   3  11 165]]
Epoch 21
-------------------------------
loss: 0.629598  [    0/ 3200]
loss: 0.754226  [ 1600/ 3200]
loss: 0.584799  [    0/ 3200]
loss: 0.572481  [ 1600/ 3200]
loss: 0.380248  [    0/ 3200]
loss: 0.831017  [ 1600/ 3200]
loss: 0.547474  [    0/ 3200]
loss: 0.942309  [ 1600/ 3200]
loss: 0.362803  [    0/ 3200]
loss: 0.472168  [ 1600/ 3200]
loss: 0.498983  [    0/ 3200]
loss: 0.628663  [ 1600/ 3200]
loss: 0.562519  [    0/ 3200]
loss: 1.093639  [ 1600/ 3200]
loss: 0.594286  [    0/ 3200]
loss: 0.813712  [ 1600/ 3200]
loss: 0.933043  [    0/ 3200]
loss: 0.902713  [ 1600/ 3200]
loss: 0.263308  [    0/ 3200]
loss: 0.492685  [ 1600/ 3200]
loss: 0.818599  [    0/ 3200]
loss: 0.451112  [ 1600/ 3200]
loss: 0.482936  [    0/ 3200]
loss: 0.590678  [ 1600/ 3200]
loss: 0.600958  [    0/ 3200]
loss: 0.465610  [ 1600/ 3200]
loss: 0.750103  [    0/ 3200]
loss: 0.547316  [ 1600/ 3200]
loss: 0.446264  [    0/ 3200]
loss: 0.428393  [ 1600/ 3200]
loss: 0.507498  [    0/ 3200]
loss: 0.345092  [ 1600/ 3200]
loss: 0.429629  [    0/ 3200]
loss: 0.476765  [ 1600/ 3200]
loss: 0.746814  [    0/ 3200]
loss: 0.492585  [ 1600/ 3200]
loss: 0.292370  [    0/ 3200]
loss: 0.478896  [ 1600/ 3200]
loss: 0.551323  [    0/ 3200]
loss: 0.638487  [ 1600/ 3200]
loss: 0.645433  [    0/ 3200]
loss: 0.395312  [ 1600/ 3200]
loss: 0.246024  [    0/ 3200]
loss: 0.446092  [ 1600/ 3200]
loss: 0.276043  [    0/ 3200]
loss: 0.594435  [ 1600/ 3200]
loss: 0.359617  [    0/ 3200]
loss: 0.548248  [ 1600/ 3200]
loss: 0.524710  [    0/ 3200]
loss: 0.709380  [ 1600/ 3200]
loss: 0.544070  [    0/ 3200]
loss: 0.654465  [ 1600/ 3200]
loss: 0.715714  [    0/ 3200]
loss: 0.621731  [ 1600/ 3200]
loss: 0.221064  [    0/ 3200]
loss: 0.514630  [ 1600/ 3200]
loss: 0.552191  [    0/ 3200]
loss: 0.253585  [ 1600/ 3200]
loss: 0.710826  [    0/ 3200]
loss: 0.594867  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0526
F1 Score (macro-averaged): 0.6676
Accuracy: 67.5000
Confusion Matrix:
[[ 89  51  43  17]
 [ 11 186   0   3]
 [ 39   4 153   4]
 [ 50  19  19 112]]
Epoch 22
-------------------------------
loss: 0.919795  [    0/ 3200]
loss: 0.479585  [ 1600/ 3200]
loss: 0.511687  [    0/ 3200]
loss: 0.484574  [ 1600/ 3200]
loss: 1.009837  [    0/ 3200]
loss: 0.246624  [ 1600/ 3200]
loss: 0.823459  [    0/ 3200]
loss: 0.755086  [ 1600/ 3200]
loss: 0.656467  [    0/ 3200]
loss: 0.565678  [ 1600/ 3200]
loss: 0.632574  [    0/ 3200]
loss: 0.570539  [ 1600/ 3200]
loss: 0.790264  [    0/ 3200]
loss: 0.665248  [ 1600/ 3200]
loss: 0.637081  [    0/ 3200]
loss: 0.473683  [ 1600/ 3200]
loss: 0.453762  [    0/ 3200]
loss: 0.715198  [ 1600/ 3200]
loss: 0.373147  [    0/ 3200]
loss: 0.545061  [ 1600/ 3200]
loss: 1.169130  [    0/ 3200]
loss: 0.507289  [ 1600/ 3200]
loss: 0.576723  [    0/ 3200]
loss: 1.017903  [ 1600/ 3200]
loss: 0.718123  [    0/ 3200]
loss: 0.937350  [ 1600/ 3200]
loss: 0.443980  [    0/ 3200]
loss: 0.673168  [ 1600/ 3200]
loss: 0.506710  [    0/ 3200]
loss: 0.423732  [ 1600/ 3200]
loss: 0.563376  [    0/ 3200]
loss: 0.768782  [ 1600/ 3200]
loss: 0.789933  [    0/ 3200]
loss: 0.385408  [ 1600/ 3200]
loss: 0.289056  [    0/ 3200]
loss: 0.399498  [ 1600/ 3200]
loss: 0.434980  [    0/ 3200]
loss: 0.543675  [ 1600/ 3200]
loss: 0.981877  [    0/ 3200]
loss: 0.779453  [ 1600/ 3200]
loss: 1.189410  [    0/ 3200]
loss: 0.829505  [ 1600/ 3200]
loss: 0.512240  [    0/ 3200]
loss: 0.798389  [ 1600/ 3200]
loss: 0.437189  [    0/ 3200]
loss: 0.502855  [ 1600/ 3200]
loss: 0.671629  [    0/ 3200]
loss: 0.541165  [ 1600/ 3200]
loss: 0.721075  [    0/ 3200]
loss: 0.815366  [ 1600/ 3200]
loss: 0.471080  [    0/ 3200]
loss: 0.533444  [ 1600/ 3200]
loss: 0.783567  [    0/ 3200]
loss: 0.861289  [ 1600/ 3200]
loss: 0.479755  [    0/ 3200]
loss: 0.403904  [ 1600/ 3200]
loss: 0.544946  [    0/ 3200]
loss: 0.520068  [ 1600/ 3200]
loss: 0.326070  [    0/ 3200]
loss: 0.954604  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0459
F1 Score (macro-averaged): 0.6702
Accuracy: 67.6250
Confusion Matrix:
[[ 83  23  51  43]
 [ 46 132   0  22]
 [ 30   0 154  16]
 [ 11   2  15 172]]
Epoch 23
-------------------------------
loss: 0.479459  [    0/ 3200]
loss: 0.883769  [ 1600/ 3200]
loss: 0.273325  [    0/ 3200]
loss: 0.709120  [ 1600/ 3200]
loss: 0.490067  [    0/ 3200]
loss: 0.744098  [ 1600/ 3200]
loss: 0.469537  [    0/ 3200]
loss: 0.790475  [ 1600/ 3200]
loss: 0.648775  [    0/ 3200]
loss: 0.574023  [ 1600/ 3200]
loss: 0.730040  [    0/ 3200]
loss: 0.628341  [ 1600/ 3200]
loss: 0.847640  [    0/ 3200]
loss: 0.630340  [ 1600/ 3200]
loss: 0.682096  [    0/ 3200]
loss: 0.609754  [ 1600/ 3200]
loss: 0.454213  [    0/ 3200]
loss: 0.817109  [ 1600/ 3200]
loss: 0.396605  [    0/ 3200]
loss: 0.403952  [ 1600/ 3200]
loss: 0.527224  [    0/ 3200]
loss: 0.625383  [ 1600/ 3200]
loss: 1.092716  [    0/ 3200]
loss: 0.718343  [ 1600/ 3200]
loss: 0.267961  [    0/ 3200]
loss: 0.629705  [ 1600/ 3200]
loss: 0.745261  [    0/ 3200]
loss: 1.021737  [ 1600/ 3200]
loss: 0.946420  [    0/ 3200]
loss: 0.560684  [ 1600/ 3200]
loss: 0.777851  [    0/ 3200]
loss: 0.535194  [ 1600/ 3200]
loss: 0.734356  [    0/ 3200]
loss: 0.387394  [ 1600/ 3200]
loss: 0.571557  [    0/ 3200]
loss: 0.607701  [ 1600/ 3200]
loss: 0.721403  [    0/ 3200]
loss: 0.580413  [ 1600/ 3200]
loss: 0.489095  [    0/ 3200]
loss: 0.500384  [ 1600/ 3200]
loss: 0.763365  [    0/ 3200]
loss: 0.802073  [ 1600/ 3200]
loss: 0.475933  [    0/ 3200]
loss: 0.619531  [ 1600/ 3200]
loss: 0.577510  [    0/ 3200]
loss: 0.970005  [ 1600/ 3200]
loss: 0.540524  [    0/ 3200]
loss: 0.839187  [ 1600/ 3200]
loss: 0.507742  [    0/ 3200]
loss: 0.476419  [ 1600/ 3200]
loss: 1.032694  [    0/ 3200]
loss: 0.634506  [ 1600/ 3200]
loss: 0.873045  [    0/ 3200]
loss: 0.433502  [ 1600/ 3200]
loss: 0.430436  [    0/ 3200]
loss: 0.379904  [ 1600/ 3200]
loss: 0.758140  [    0/ 3200]
loss: 0.526071  [ 1600/ 3200]
loss: 0.556772  [    0/ 3200]
loss: 0.608062  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0512
F1 Score (macro-averaged): 0.6637
Accuracy: 65.7500
Confusion Matrix:
[[115  33  21  31]
 [ 34 157   0   9]
 [ 61   1 124  14]
 [ 45  16   9 130]]
Epoch 24
-------------------------------
loss: 0.500326  [    0/ 3200]
loss: 0.832929  [ 1600/ 3200]
loss: 0.555999  [    0/ 3200]
loss: 0.861512  [ 1600/ 3200]
loss: 0.314431  [    0/ 3200]
loss: 0.518129  [ 1600/ 3200]
loss: 0.373012  [    0/ 3200]
loss: 0.570180  [ 1600/ 3200]
loss: 0.534087  [    0/ 3200]
loss: 0.455155  [ 1600/ 3200]
loss: 0.663133  [    0/ 3200]
loss: 0.298508  [ 1600/ 3200]
loss: 0.831521  [    0/ 3200]
loss: 0.547308  [ 1600/ 3200]
loss: 0.546978  [    0/ 3200]
loss: 0.308195  [ 1600/ 3200]
loss: 0.525493  [    0/ 3200]
loss: 0.424091  [ 1600/ 3200]
loss: 0.441735  [    0/ 3200]
loss: 0.440927  [ 1600/ 3200]
loss: 0.431317  [    0/ 3200]
loss: 0.645202  [ 1600/ 3200]
loss: 0.444748  [    0/ 3200]
loss: 0.813807  [ 1600/ 3200]
loss: 0.643700  [    0/ 3200]
loss: 0.687214  [ 1600/ 3200]
loss: 0.254206  [    0/ 3200]
loss: 0.513156  [ 1600/ 3200]
loss: 0.602469  [    0/ 3200]
loss: 0.438540  [ 1600/ 3200]
loss: 0.527234  [    0/ 3200]
loss: 0.684869  [ 1600/ 3200]
loss: 0.626880  [    0/ 3200]
loss: 0.256087  [ 1600/ 3200]
loss: 0.569918  [    0/ 3200]
loss: 0.605966  [ 1600/ 3200]
loss: 0.562360  [    0/ 3200]
loss: 0.591942  [ 1600/ 3200]
loss: 0.564084  [    0/ 3200]
loss: 0.567458  [ 1600/ 3200]
loss: 0.637328  [    0/ 3200]
loss: 0.501975  [ 1600/ 3200]
loss: 0.728366  [    0/ 3200]
loss: 0.596612  [ 1600/ 3200]
loss: 0.690888  [    0/ 3200]
loss: 0.576557  [ 1600/ 3200]
loss: 0.405776  [    0/ 3200]
loss: 0.397024  [ 1600/ 3200]
loss: 0.572858  [    0/ 3200]
loss: 0.464016  [ 1600/ 3200]
loss: 0.304227  [    0/ 3200]
loss: 0.402810  [ 1600/ 3200]
loss: 0.733253  [    0/ 3200]
loss: 0.556647  [ 1600/ 3200]
loss: 0.270900  [    0/ 3200]
loss: 0.546180  [ 1600/ 3200]
loss: 0.579279  [    0/ 3200]
loss: 0.378619  [ 1600/ 3200]
loss: 0.637134  [    0/ 3200]
loss: 0.543219  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0471
F1 Score (macro-averaged): 0.6615
Accuracy: 67.1250
Confusion Matrix:
[[ 73  26  58  43]
 [ 37 138   0  25]
 [ 27   0 158  15]
 [ 13   2  17 168]]
Epoch 25
-------------------------------
loss: 0.588834  [    0/ 3200]
loss: 0.724144  [ 1600/ 3200]
loss: 0.342969  [    0/ 3200]
loss: 0.414122  [ 1600/ 3200]
loss: 0.742670  [    0/ 3200]
loss: 0.531977  [ 1600/ 3200]
loss: 0.610611  [    0/ 3200]
loss: 0.563670  [ 1600/ 3200]
loss: 0.398408  [    0/ 3200]
loss: 0.800756  [ 1600/ 3200]
loss: 0.587587  [    0/ 3200]
loss: 0.292557  [ 1600/ 3200]
loss: 0.509150  [    0/ 3200]
loss: 0.588049  [ 1600/ 3200]
loss: 0.473509  [    0/ 3200]
loss: 0.336031  [ 1600/ 3200]
loss: 0.219478  [    0/ 3200]
loss: 0.763603  [ 1600/ 3200]
loss: 1.048973  [    0/ 3200]
loss: 0.996360  [ 1600/ 3200]
loss: 0.468828  [    0/ 3200]
loss: 0.517484  [ 1600/ 3200]
loss: 0.618989  [    0/ 3200]
loss: 0.447251  [ 1600/ 3200]
loss: 0.525806  [    0/ 3200]
loss: 0.567583  [ 1600/ 3200]
loss: 0.986995  [    0/ 3200]
loss: 0.960851  [ 1600/ 3200]
loss: 0.755300  [    0/ 3200]
loss: 0.439117  [ 1600/ 3200]
loss: 0.433629  [    0/ 3200]
loss: 0.928519  [ 1600/ 3200]
loss: 0.695228  [    0/ 3200]
loss: 0.453249  [ 1600/ 3200]
loss: 0.560115  [    0/ 3200]
loss: 0.613317  [ 1600/ 3200]
loss: 0.766148  [    0/ 3200]
loss: 0.627788  [ 1600/ 3200]
loss: 0.643718  [    0/ 3200]
loss: 0.500236  [ 1600/ 3200]
loss: 0.645736  [    0/ 3200]
loss: 0.815575  [ 1600/ 3200]
loss: 0.579968  [    0/ 3200]
loss: 0.578780  [ 1600/ 3200]
loss: 0.327399  [    0/ 3200]
loss: 0.617279  [ 1600/ 3200]
loss: 0.539875  [    0/ 3200]
loss: 0.416314  [ 1600/ 3200]
loss: 0.573731  [    0/ 3200]
loss: 0.414900  [ 1600/ 3200]
loss: 0.758248  [    0/ 3200]
loss: 0.873981  [ 1600/ 3200]
loss: 0.456525  [    0/ 3200]
loss: 0.622525  [ 1600/ 3200]
loss: 0.707693  [    0/ 3200]
loss: 0.561998  [ 1600/ 3200]
loss: 0.483265  [    0/ 3200]
loss: 0.397225  [ 1600/ 3200]
loss: 0.318159  [    0/ 3200]
loss: 0.499802  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0478
F1 Score (macro-averaged): 0.6980
Accuracy: 71.3750
Confusion Matrix:
[[ 78  49  31  42]
 [  3 194   0   3]
 [ 30  10 143  17]
 [ 17  16  11 156]]
Epoch 26
-------------------------------
loss: 0.436701  [    0/ 3200]
loss: 0.662280  [ 1600/ 3200]
loss: 0.449690  [    0/ 3200]
loss: 0.627970  [ 1600/ 3200]
loss: 1.150105  [    0/ 3200]
loss: 1.101192  [ 1600/ 3200]
loss: 0.742901  [    0/ 3200]
loss: 0.168186  [ 1600/ 3200]
loss: 0.613829  [    0/ 3200]
loss: 0.403719  [ 1600/ 3200]
loss: 0.385051  [    0/ 3200]
loss: 0.515041  [ 1600/ 3200]
loss: 0.391627  [    0/ 3200]
loss: 0.618190  [ 1600/ 3200]
loss: 1.355071  [    0/ 3200]
loss: 0.584588  [ 1600/ 3200]
loss: 0.370698  [    0/ 3200]
loss: 0.646236  [ 1600/ 3200]
loss: 0.451710  [    0/ 3200]
loss: 0.650771  [ 1600/ 3200]
loss: 0.997849  [    0/ 3200]
loss: 0.762714  [ 1600/ 3200]
loss: 0.395980  [    0/ 3200]
loss: 0.570085  [ 1600/ 3200]
loss: 0.708598  [    0/ 3200]
loss: 0.835515  [ 1600/ 3200]
loss: 0.462256  [    0/ 3200]
loss: 0.390202  [ 1600/ 3200]
loss: 0.659261  [    0/ 3200]
loss: 0.626990  [ 1600/ 3200]
loss: 0.725234  [    0/ 3200]
loss: 0.311083  [ 1600/ 3200]
loss: 0.595199  [    0/ 3200]
loss: 0.793179  [ 1600/ 3200]
loss: 0.391003  [    0/ 3200]
loss: 0.597100  [ 1600/ 3200]
loss: 0.689373  [    0/ 3200]
loss: 0.732484  [ 1600/ 3200]
loss: 0.577941  [    0/ 3200]
loss: 0.490337  [ 1600/ 3200]
loss: 0.602328  [    0/ 3200]
loss: 0.659704  [ 1600/ 3200]
loss: 0.770399  [    0/ 3200]
loss: 0.549627  [ 1600/ 3200]
loss: 0.589735  [    0/ 3200]
loss: 0.352201  [ 1600/ 3200]
loss: 0.610488  [    0/ 3200]
loss: 0.506384  [ 1600/ 3200]
loss: 0.588115  [    0/ 3200]
loss: 0.485976  [ 1600/ 3200]
loss: 0.839192  [    0/ 3200]
loss: 0.417065  [ 1600/ 3200]
loss: 0.923163  [    0/ 3200]
loss: 0.477258  [ 1600/ 3200]
loss: 1.038881  [    0/ 3200]
loss: 0.664817  [ 1600/ 3200]
loss: 0.570718  [    0/ 3200]
loss: 0.553276  [ 1600/ 3200]
loss: 0.649726  [    0/ 3200]
loss: 0.525633  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0479
F1 Score (macro-averaged): 0.6788
Accuracy: 68.3750
Confusion Matrix:
[[ 90  33  23  54]
 [ 22 165   0  13]
 [ 44   1 124  31]
 [ 17   6   9 168]]
Epoch 27
-------------------------------
loss: 0.380121  [    0/ 3200]
loss: 0.507663  [ 1600/ 3200]
loss: 0.502129  [    0/ 3200]
loss: 0.784597  [ 1600/ 3200]
loss: 0.507974  [    0/ 3200]
loss: 0.836187  [ 1600/ 3200]
loss: 0.668709  [    0/ 3200]
loss: 0.711914  [ 1600/ 3200]
loss: 0.243367  [    0/ 3200]
loss: 0.479231  [ 1600/ 3200]
loss: 0.392907  [    0/ 3200]
loss: 0.734104  [ 1600/ 3200]
loss: 0.214819  [    0/ 3200]
loss: 0.398370  [ 1600/ 3200]
loss: 0.535070  [    0/ 3200]
loss: 0.514811  [ 1600/ 3200]
loss: 0.457882  [    0/ 3200]
loss: 0.552377  [ 1600/ 3200]
loss: 0.482132  [    0/ 3200]
loss: 0.456903  [ 1600/ 3200]
loss: 0.671932  [    0/ 3200]
loss: 0.720652  [ 1600/ 3200]
loss: 0.500936  [    0/ 3200]
loss: 0.369269  [ 1600/ 3200]
loss: 0.420864  [    0/ 3200]
loss: 0.322047  [ 1600/ 3200]
loss: 0.440637  [    0/ 3200]
loss: 0.449526  [ 1600/ 3200]
loss: 0.567224  [    0/ 3200]
loss: 0.521412  [ 1600/ 3200]
loss: 0.642076  [    0/ 3200]
loss: 0.546755  [ 1600/ 3200]
loss: 0.655447  [    0/ 3200]
loss: 0.809552  [ 1600/ 3200]
loss: 0.550188  [    0/ 3200]
loss: 0.550717  [ 1600/ 3200]
loss: 0.340247  [    0/ 3200]
loss: 0.418976  [ 1600/ 3200]
loss: 0.279736  [    0/ 3200]
loss: 0.462609  [ 1600/ 3200]
loss: 0.837518  [    0/ 3200]
loss: 0.536775  [ 1600/ 3200]
loss: 0.270703  [    0/ 3200]
loss: 0.636373  [ 1600/ 3200]
loss: 0.313393  [    0/ 3200]
loss: 0.413276  [ 1600/ 3200]
loss: 0.500691  [    0/ 3200]
loss: 0.638148  [ 1600/ 3200]
loss: 0.595889  [    0/ 3200]
loss: 0.492950  [ 1600/ 3200]
loss: 0.904134  [    0/ 3200]
loss: 0.696125  [ 1600/ 3200]
loss: 0.601816  [    0/ 3200]
loss: 0.982277  [ 1600/ 3200]
loss: 0.488847  [    0/ 3200]
loss: 0.428967  [ 1600/ 3200]
loss: 0.652395  [    0/ 3200]
loss: 0.420871  [ 1600/ 3200]
loss: 0.287543  [    0/ 3200]
loss: 0.665490  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0518
F1 Score (macro-averaged): 0.6254
Accuracy: 64.8750
Confusion Matrix:
[[ 49  35  85  31]
 [ 38 132   2  28]
 [ 12   1 176  11]
 [ 13   3  22 162]]
Epoch 28
-------------------------------
loss: 0.142762  [    0/ 3200]
loss: 0.530283  [ 1600/ 3200]
loss: 0.456221  [    0/ 3200]
loss: 0.338083  [ 1600/ 3200]
loss: 0.424948  [    0/ 3200]
loss: 0.823607  [ 1600/ 3200]
loss: 0.361855  [    0/ 3200]
loss: 0.804950  [ 1600/ 3200]
loss: 0.923446  [    0/ 3200]
loss: 0.350189  [ 1600/ 3200]
loss: 0.356616  [    0/ 3200]
loss: 0.614466  [ 1600/ 3200]
loss: 0.450236  [    0/ 3200]
loss: 0.577976  [ 1600/ 3200]
loss: 0.879865  [    0/ 3200]
loss: 0.524073  [ 1600/ 3200]
loss: 0.510084  [    0/ 3200]
loss: 0.234738  [ 1600/ 3200]
loss: 0.906599  [    0/ 3200]
loss: 0.842924  [ 1600/ 3200]
loss: 0.329490  [    0/ 3200]
loss: 0.518590  [ 1600/ 3200]
loss: 0.423967  [    0/ 3200]
loss: 0.467490  [ 1600/ 3200]
loss: 0.441237  [    0/ 3200]
loss: 0.517290  [ 1600/ 3200]
loss: 0.463737  [    0/ 3200]
loss: 0.489820  [ 1600/ 3200]
loss: 0.763110  [    0/ 3200]
loss: 0.534509  [ 1600/ 3200]
loss: 0.855553  [    0/ 3200]
loss: 0.824792  [ 1600/ 3200]
loss: 0.327254  [    0/ 3200]
loss: 0.759240  [ 1600/ 3200]
loss: 0.577843  [    0/ 3200]
loss: 0.746359  [ 1600/ 3200]
loss: 0.954400  [    0/ 3200]
loss: 0.414877  [ 1600/ 3200]
loss: 0.710464  [    0/ 3200]
loss: 0.298022  [ 1600/ 3200]
loss: 0.605946  [    0/ 3200]
loss: 0.602069  [ 1600/ 3200]
loss: 0.356592  [    0/ 3200]
loss: 0.442572  [ 1600/ 3200]
loss: 0.455193  [    0/ 3200]
loss: 0.475463  [ 1600/ 3200]
loss: 0.348884  [    0/ 3200]
loss: 0.482007  [ 1600/ 3200]
loss: 0.789196  [    0/ 3200]
loss: 0.914644  [ 1600/ 3200]
loss: 0.317191  [    0/ 3200]
loss: 0.582897  [ 1600/ 3200]
loss: 0.310918  [    0/ 3200]
loss: 0.674255  [ 1600/ 3200]
loss: 1.066737  [    0/ 3200]
loss: 0.512118  [ 1600/ 3200]
loss: 0.810455  [    0/ 3200]
loss: 0.509738  [ 1600/ 3200]
loss: 0.304228  [    0/ 3200]
loss: 0.507671  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0492
F1 Score (macro-averaged): 0.6809
Accuracy: 67.6250
Confusion Matrix:
[[112  24  28  36]
 [ 52 130   0  18]
 [ 47   1 139  13]
 [ 26   3  11 160]]
Epoch 29
-------------------------------
loss: 0.518198  [    0/ 3200]
loss: 0.572688  [ 1600/ 3200]
loss: 0.984017  [    0/ 3200]
loss: 0.282903  [ 1600/ 3200]
loss: 0.312883  [    0/ 3200]
loss: 0.430915  [ 1600/ 3200]
loss: 0.696827  [    0/ 3200]
loss: 0.370360  [ 1600/ 3200]
loss: 0.546361  [    0/ 3200]
loss: 0.627705  [ 1600/ 3200]
loss: 0.471750  [    0/ 3200]
loss: 0.739450  [ 1600/ 3200]
loss: 0.341394  [    0/ 3200]
loss: 0.677945  [ 1600/ 3200]
loss: 0.379789  [    0/ 3200]
loss: 0.395040  [ 1600/ 3200]
loss: 0.618536  [    0/ 3200]
loss: 0.493901  [ 1600/ 3200]
loss: 0.716804  [    0/ 3200]
loss: 0.472236  [ 1600/ 3200]
loss: 0.506573  [    0/ 3200]
loss: 0.529034  [ 1600/ 3200]
loss: 0.558902  [    0/ 3200]
loss: 0.707163  [ 1600/ 3200]
loss: 0.250885  [    0/ 3200]
loss: 0.746116  [ 1600/ 3200]
loss: 0.752536  [    0/ 3200]
loss: 0.300595  [ 1600/ 3200]
loss: 0.720439  [    0/ 3200]
loss: 0.460155  [ 1600/ 3200]
loss: 0.668210  [    0/ 3200]
loss: 0.443206  [ 1600/ 3200]
loss: 0.721698  [    0/ 3200]
loss: 0.561423  [ 1600/ 3200]
loss: 0.405961  [    0/ 3200]
loss: 0.317551  [ 1600/ 3200]
loss: 0.665825  [    0/ 3200]
loss: 0.484317  [ 1600/ 3200]
loss: 0.525959  [    0/ 3200]
loss: 0.353802  [ 1600/ 3200]
loss: 0.353542  [    0/ 3200]
loss: 0.686817  [ 1600/ 3200]
loss: 0.483453  [    0/ 3200]
loss: 0.447083  [ 1600/ 3200]
loss: 0.412039  [    0/ 3200]
loss: 0.739070  [ 1600/ 3200]
loss: 0.586307  [    0/ 3200]
loss: 0.525340  [ 1600/ 3200]
loss: 0.293638  [    0/ 3200]
loss: 0.362155  [ 1600/ 3200]
loss: 0.975157  [    0/ 3200]
loss: 1.037731  [ 1600/ 3200]
loss: 0.717211  [    0/ 3200]
loss: 0.364284  [ 1600/ 3200]
loss: 0.852117  [    0/ 3200]
loss: 0.374754  [ 1600/ 3200]
loss: 0.403149  [    0/ 3200]
loss: 0.328440  [ 1600/ 3200]
loss: 0.460418  [    0/ 3200]
loss: 0.156891  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0475
F1 Score (macro-averaged): 0.6718
Accuracy: 67.8750
Confusion Matrix:
[[ 77  34  58  31]
 [ 32 157   0  11]
 [ 29   0 162   9]
 [ 27   8  18 147]]
Epoch 30
-------------------------------
loss: 0.423707  [    0/ 3200]
loss: 0.328357  [ 1600/ 3200]
loss: 0.569860  [    0/ 3200]
loss: 0.370793  [ 1600/ 3200]
loss: 0.375528  [    0/ 3200]
loss: 0.926477  [ 1600/ 3200]
loss: 0.510396  [    0/ 3200]
loss: 0.420584  [ 1600/ 3200]
loss: 0.317260  [    0/ 3200]
loss: 0.790435  [ 1600/ 3200]
loss: 0.718053  [    0/ 3200]
loss: 0.656977  [ 1600/ 3200]
loss: 0.936746  [    0/ 3200]
loss: 0.395971  [ 1600/ 3200]
loss: 0.818199  [    0/ 3200]
loss: 0.564684  [ 1600/ 3200]
loss: 0.302225  [    0/ 3200]
loss: 0.401494  [ 1600/ 3200]
loss: 0.515144  [    0/ 3200]
loss: 0.694858  [ 1600/ 3200]
loss: 0.824841  [    0/ 3200]
loss: 0.453978  [ 1600/ 3200]
loss: 0.848685  [    0/ 3200]
loss: 0.538521  [ 1600/ 3200]
loss: 0.459397  [    0/ 3200]
loss: 0.811361  [ 1600/ 3200]
loss: 0.538932  [    0/ 3200]
loss: 0.624336  [ 1600/ 3200]
loss: 0.261756  [    0/ 3200]
loss: 0.687624  [ 1600/ 3200]
loss: 0.524622  [    0/ 3200]
loss: 0.235325  [ 1600/ 3200]
loss: 1.202442  [    0/ 3200]
loss: 0.554708  [ 1600/ 3200]
loss: 0.475882  [    0/ 3200]
loss: 0.479630  [ 1600/ 3200]
loss: 0.390558  [    0/ 3200]
loss: 0.503463  [ 1600/ 3200]
loss: 0.441337  [    0/ 3200]
loss: 0.489477  [ 1600/ 3200]
loss: 0.357347  [    0/ 3200]
loss: 0.370170  [ 1600/ 3200]
loss: 0.572010  [    0/ 3200]
loss: 0.363843  [ 1600/ 3200]
loss: 0.448381  [    0/ 3200]
loss: 0.699327  [ 1600/ 3200]
loss: 0.485601  [    0/ 3200]
loss: 0.640990  [ 1600/ 3200]
loss: 0.369092  [    0/ 3200]
loss: 0.766990  [ 1600/ 3200]
loss: 0.572780  [    0/ 3200]
loss: 0.683018  [ 1600/ 3200]
loss: 0.597767  [    0/ 3200]
loss: 0.502044  [ 1600/ 3200]
loss: 0.529500  [    0/ 3200]
loss: 0.756466  [ 1600/ 3200]
loss: 0.213313  [    0/ 3200]
loss: 0.522023  [ 1600/ 3200]
loss: 1.381031  [    0/ 3200]
loss: 0.319367  [ 1600/ 3200]
Validation Set Evaluation:
Loss: 0.0493
F1 Score (macro-averaged): 0.6648
Accuracy: 68.0000
Confusion Matrix:
[[ 64  35  73  28]
 [ 28 157   2  13]
 [ 19   0 175   6]
 [ 19   8  25 148]]
Test Set Evaluation for the Best Model:
Loss: 0.0487
F1 Score (macro-averaged): 0.6906
Accuracy: 69.7674
Confusion Matrix:
[[139  32  86  67]
 [ 31 250   8   8]
 [ 25   9 306  16]
 [ 56  14  64 265]]

Γραφήματα επιδόσεων #GPU: activated

def plot_results(loss_values, accuracy_values, f1_values, best_epoch, best_test_loss, best_test_f1, best_test_acc):
    epochs = range(1, len(loss_values) + 1)

    # Create subplots with 1 row and 3 columns
    fig, axs = plt.subplots(1, 3, figsize=(16, 4))

    # Plot loss
    axs[0].plot(epochs, loss_values, label='Loss')
    axs[0].axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')
    axs[0].set_title('Training Loss over Epochs')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].legend()

    # Plot accuracy
    axs[1].plot(epochs, accuracy_values, label='Accuracy')
    axs[1].axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')
    axs[1].set_title('Training Accuracy over Epochs')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel('Accuracy')
    axs[1].legend()

    # Plot F1 score
    axs[2].plot(epochs, f1_values, label='F1 Score')
    axs[2].axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')
    axs[2].set_title('Training F1 Score (Macro-averaged) over Epochs')
    axs[2].set_xlabel('Epoch')
    axs[2].set_ylabel('F1 Score')
    axs[2].legend()

    # Adjust the spacing between subplots
    plt.tight_layout()

    # Print the evaluation scores for the best model on the test set
    print("Test Set Evaluation for the Best Model:")
    print(f"Loss: {best_test_loss:.4f}")
    print(f"F1 Score (macro-averaged): {best_test_f1:.4f}")
    print(f"Accuracy: {100*best_test_acc:.4f}")

    # Display the plots
    plt.show()
def plot_confusion_matrix(confusion_matrix, labels):
    classes = np.arange(confusion_matrix.shape[0])
    cmap = plt.cm.Blues
    plt.figure(figsize=(10, 10))
    plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    fmt = 'd'
    thresh = confusion_matrix.max() / 2.
    for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):
        plt.text(j, i, format(confusion_matrix[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if confusion_matrix[i, j] > thresh else "black")

    plt.show()


# index of the best F1 score
best_f1_index = f1_values.index(max(f1_values))

# Retrieve the evaluation scores for the best model
best_test_loss = loss_values[best_f1_index]
best_test_f1 = f1_values[best_f1_index]
best_test_acc = accuracy_values[best_f1_index]
best_test_cm = cm[best_f1_index]

# Plot
plot_results(loss_values, accuracy_values, f1_values, best_f1_index+1, best_test_loss, best_test_f1, best_test_acc)

Test Set Evaluation for the Best Model:
Loss: 0.0456
F1 Score (macro-averaged): 0.7168
Accuracy: 71.3750


labels = label_encoder.classes_
plot_confusion_matrix(best_test_cm, labels)


Παρατηρούμε ότι το βέλτιστο μοντέλο απο άποψη ακρίβειας και f1 score εντοπίζεται στις 7 εποχές. Προφανώς με τη χρήση GPU επιτεύχθηκαν καλύτερα αποτελέσματα νωρίτερα κα έπειτα έγινε ένα over-fitting καθώς το μοντέλο άρχισε να υπερεκπαιδεύται. Αυτό είναι εμφανές και απο το training loss και απο τις μετρικές του f1/accuracy οι οποίες δεν ξεπέρασαν το μέγιστο τους για τις 7 εποχές. Συγκρίνοντας τον confusion matrix με τον αντίστοιχο της τελευταίας εποχής:

[[ 64 35 73 28]

[ 28 157 2 13]

[ 19 0 175 6]

[ 19 8 25 148]]

είναι εμφανές ότι οι βέλτιστες προβέψεις είναι μεγαλύτερης ακρίβειας με μεγάλη βελτίωση στα είδη των blues και classical και rock ωστόσο με μία μείωση της απόδοσης στο hiphop.

Να σημειωθεί ωστόσο ότι ο καλύτερος πίνακας δεν ταυτίζεται με το καλύτερο accuracy και F1 καθώς ήταν:

[[139 32 86 67]

[ 31 250 8 8]

[ 25 9 306 16]

[ 56 14 64 265]]

Ερώτημα 2: Convolutional Neural Network
Στο ερώτημα αυτό θα χρησιμοποιήσουμε τα mel-spectrograms σαν εισόδους σε Συνελικτικά Νευρωνικά Δίκτυα με σκοπό την ταξινόμηση μουσικού είδους. (θα πρέπει να εργάζεστε με τον ίδιο τρόπο χρησιμοποιώντας το validation set για να βρείτε το κατάλληλο στιγμιότυπο)

Βήμα 1: Φόρτωση δεδομένων (spectrograms)

Ακολουθήστε την διαδικασία του ερωτήματος 1.1, αλλά αυτή τη φορά για τα melgrams. Οπτικοποιέιστε ένα τυχαίο melgram απο κάθε κλάση.

#Training set

file_path_1 = '/content/drive/MyDrive/Ν124/music_genre_data_di/train/melgrams/X.npy'
file_path_2 = '/content/drive/MyDrive/Ν124/music_genre_data_di/train/melgrams/labels.npy'
x_tr = np.load(file_path_1)
y_tr = np.load(file_path_2)

#Validation set

file_path_3 = '/content/drive/MyDrive/Ν124/music_genre_data_di/val/melgrams/X.npy'
file_path_4 = '/content/drive/MyDrive/Ν124/music_genre_data_di/val/melgrams/labels.npy'
x_val = np.load(file_path_3)
y_val= np.load(file_path_4)

#Test set
file_path_5 = '/content/drive/MyDrive/Ν124/music_genre_data_di/test/melgrams/X.npy'
file_path_6 = '/content/drive/MyDrive/Ν124/music_genre_data_di/test/melgrams/labels.npy'
x_test = np.load(file_path_5)
y_test = np.load(file_path_6)
from sklearn.preprocessing import LabelEncoder

#encoding
label_encoder = LabelEncoder()

y_tr_en = label_encoder.fit_transform(y_tr)
y_val_en = label_encoder.transform(y_val)
y_test_en = label_encoder.transform(y_test)

#checking if there is something miss labeled in the original set
print("Original labels:", np.unique(y_tr))

#matching labels to their encodings
print("Mapping of classes to integers:")
for class_name, class_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):
    print(f"{class_name} : {class_label}")
Original labels: ['blues' 'classical' 'hiphop' 'rock_metal_hardrock']
Mapping of classes to integers:
blues : 0
classical : 1
hiphop : 2
rock_metal_hardrock : 3

from torch.utils.data import TensorDataset, DataLoader


# np.arrays to PyTorch tensors
x_tr_tensor = torch.Tensor(x_tr)
y_tr_tensor = torch.LongTensor(y_tr_en)
x_val_tensor = torch.Tensor(x_val)
y_val_tensor = torch.LongTensor(y_val_en)
x_test_tensor = torch.Tensor(x_test)
y_test_tensor = torch.LongTensor(y_test_en)


# TensorDataset for each data group
train_dataset = TensorDataset(x_tr_tensor, y_tr_tensor)
val_dataset = TensorDataset(x_val_tensor, y_val_tensor)
test_dataset = TensorDataset(x_test_tensor, y_test_tensor)

#Data loading
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

print("Training Tensor:", x_tr_tensor)
Training Tensor: tensor([[[-2.0399e+01, -2.0120e+01, -2.0196e+01,  ..., -6.9165e+01,
          -6.9209e+01, -7.2156e+01],
         [-3.0799e+01, -1.5767e+01, -1.4065e+01,  ..., -7.1162e+01,
          -7.3414e+01, -7.5681e+01],
         [-2.3480e+01, -1.7996e+01, -1.2131e+01,  ..., -7.7995e+01,
          -8.0000e+01, -7.8732e+01],
         ...,
         [-2.6255e+01, -2.4029e+01, -1.2750e+01,  ..., -6.9557e+01,
          -7.3153e+01, -7.4345e+01],
         [-2.6146e+01, -2.0127e+01, -2.1848e+01,  ..., -4.5236e+01,
          -4.6414e+01, -5.1094e+01],
         [-1.0735e+01, -9.4343e+00, -1.0928e+01,  ..., -4.4839e+01,
          -5.0423e+01, -5.4092e+01]],

        [[-3.6618e+01, -3.8268e+01, -4.2845e+01,  ..., -6.1433e+01,
          -6.3394e+01, -6.6473e+01],
         [-3.4438e+01, -4.2406e+01, -5.0727e+01,  ..., -7.9745e+01,
          -8.0000e+01, -7.2698e+01],
         [-3.3998e+01, -4.2218e+01, -4.7256e+01,  ..., -7.3999e+01,
          -7.8155e+01, -7.3787e+01],
         ...,
         [-3.2833e+01, -2.9796e+01, -2.4821e+01,  ..., -4.0404e+01,
          -4.2112e+01, -4.4268e+01],
         [-3.3049e+01, -3.2185e+01, -2.6311e+01,  ..., -3.8693e+01,
          -3.6329e+01, -4.2876e+01],
         [-3.6228e+01, -4.4933e+01, -3.6491e+01,  ..., -4.2039e+01,
          -4.2960e+01, -4.7276e+01]],

        [[-3.1698e+01, -3.3928e+01, -3.4067e+01,  ..., -6.0808e+01,
          -6.4420e+01, -6.4838e+01],
         [-3.3415e+01, -3.9467e+01, -3.6882e+01,  ..., -7.2039e+01,
          -7.1391e+01, -7.0332e+01],
         [-3.3057e+01, -3.9942e+01, -3.7770e+01,  ..., -6.5041e+01,
          -7.5340e+01, -7.1083e+01],
         ...,
         [-3.4907e+01, -2.7916e+01, -2.6396e+01,  ..., -4.5058e+01,
          -5.1895e+01, -7.1712e+01],
         [-2.8142e+01, -2.3493e+01, -2.2061e+01,  ..., -4.9591e+01,
          -7.0192e+01, -7.0003e+01],
         [-2.3006e+01, -2.0625e+01, -2.0844e+01,  ..., -5.8185e+01,
          -7.1588e+01, -6.8896e+01]],

        ...,

        [[-3.6221e+01, -1.9509e+01, -1.6100e+01,  ..., -5.1805e+01,
          -5.1217e+01, -5.2210e+01],
         [-5.3645e+00, -9.5367e-07, -3.2866e+00,  ..., -3.7594e+01,
          -4.1622e+01, -3.9361e+01],
         [-9.1992e+00, -5.8593e+00, -8.6563e+00,  ..., -3.2212e+01,
          -4.4538e+01, -4.3719e+01],
         ...,
         [-4.1448e+01, -3.6312e+01, -1.6366e+01,  ..., -4.8684e+01,
          -5.1525e+01, -5.0758e+01],
         [-9.0713e+00, -9.2255e-01, -4.5855e-01,  ..., -4.5836e+01,
          -4.4922e+01, -4.4859e+01],
         [-1.1706e+01, -1.2475e+01, -1.3900e+01,  ..., -5.2575e+01,
          -5.1445e+01, -4.6171e+01]],

        [[-3.3013e+01, -2.8460e+01, -2.0462e+01,  ..., -4.1700e+01,
          -4.5639e+01, -4.6596e+01],
         [-4.1131e+01, -2.9711e+01, -3.0453e+01,  ..., -3.9314e+01,
          -3.9690e+01, -4.0964e+01],
         [-4.4992e+01, -3.9088e+01, -3.1019e+01,  ..., -4.0768e+01,
          -3.8552e+01, -4.5365e+01],
         ...,
         [-2.7414e+01, -1.1377e+01, -9.7939e+00,  ..., -5.3530e+01,
          -5.5092e+01, -5.3078e+01],
         [-1.7139e+01, -8.9199e+00, -1.4887e+01,  ..., -4.6752e+01,
          -4.8339e+01, -4.9951e+01],
         [-2.2342e+01, -2.7399e+01, -1.4623e+01,  ..., -5.2719e+01,
          -5.1274e+01, -5.1172e+01]],

        [[-2.9652e+01, -2.7745e+01, -2.3671e+01,  ..., -5.9037e+01,
          -6.1849e+01, -6.1056e+01],
         [-6.4505e+01, -5.4369e+01, -3.6825e+01,  ..., -5.1895e+01,
          -5.7945e+01, -6.1140e+01],
         [-5.0938e+01, -4.1578e+01, -3.1290e+01,  ..., -5.1858e+01,
          -6.2036e+01, -6.5546e+01],
         ...,
         [-4.3903e+01, -2.3412e+01, -1.3282e+01,  ..., -5.2904e+01,
          -6.0881e+01, -5.6210e+01],
         [-4.0654e+01, -2.4978e+01, -1.1507e+01,  ..., -4.7927e+01,
          -5.5770e+01, -5.7356e+01],
         [-2.5562e+01, -1.9104e+01, -1.5347e+01,  ..., -4.5333e+01,
          -5.0945e+01, -5.2474e+01]]])



# unique class labels
unique_labels = np.unique(y_tr)

# random melgram from each class
fig, axs = plt.subplots(len(unique_labels), figsize=(8, 8))

for i, label in enumerate(unique_labels):
    # indices of melgrams belonging to the current class
    class_indices = np.where(y_tr == label)[0]

    #random melgram from the current class
    random_index = random.choice(class_indices)
    random_melgram = x_tr[random_index]

    # Plot
    axs[i].imshow(random_melgram, origin='lower', cmap='inferno')
    axs[i].set_title(label)
    axs[i].axis('off')

plt.tight_layout()
plt.show()


Βήμα 2: Ορισμός Νευρωνικού Δικτύου

Ορίστε ένα Συνελικτικό Νευρωνικό Δίκτυο το οποίο να αποτελείται από

• Ακολουθία τεσσάρων συνελικτικών επιπέδων, με kernel size 5, ώστε να επιτυγχάνεται η εξής ακολουθία καναλιών: 1, 16, 32, 64, 128

• Η έξοδος του τελευταίου συνελικτικού επιπέδου να εισέρχεται σε ένα πλήρως συνδεδεμένο νευρωνικό δίκτυο 5 επιπέδων με αριθμό νευρώνων: x (διάσταση εξόδου συνελικτικού δικτύου), 1024, 256, 32, out_dim

Επιλέγω να μην προχωρήσω με lenet καθώς πιστεύω οτι τα melgrams είναι λίγο complicated για τη συγκεκριμένη μέθοδο.

class CNN(nn.Module):
    def __init__(self, output_dim):
        super(CNN, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=5)

        # Fully connected (dense) layers
        self.fc1 = nn.Linear(128, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 32)
        self.fc4 = nn.Linear(32, output_dim)

    def forward(self, x):
        # Convolutional layers
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))

        # Flatten the tensor for the fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)

        return x

output_dim = 4

# instance of the CNN model
net = CNN(output_dim)


device = 'cuda' if torch.cuda.is_available() else 'cpu'
net = net.to(device)

#  loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=1e-3)

Βήμα 3: Εκπαίδευση δικτύου

Αλλάξτε όπου χρειάζεται, και τρέξτε, την διαδικασία εκπαίδευσης και αξιολόγησης ώστε να μπορεί να εκπαιδευτεί και το νέο νευρωνικό δίκτυο. Τι παρατηρείτε; Μπορεί να εκπαιδευτεί το δίκτυο;

from sklearn.metrics import f1_score

def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()  # Set the model to training mode
    for batch, (X, y) in enumerate(dataloader):
        X = X.to(device)
        y = y.to(device)

        optimizer.zero_grad()
        pred = model(X)
        loss = loss_fn(pred, y)

        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            current = batch * len(X)
            print(f"Loss: {loss.item():.4f}  [{current}/{size}]")

def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    model.eval()  # Set the model to evaluation mode
    test_loss, correct = 0, 0
    y_true, y_pred = [], []

    with torch.no_grad():
        for X, y in dataloader:
            X = X.to(device)
            y = y.to(device)

            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).sum().item()

            # Collect true labels and predicted labels for F1 score calculation
            y_true.extend(y.tolist())
            y_pred.extend(pred.argmax(1).tolist())

    test_loss /= size
    correct /= size
    accuracy = 100 * correct

    # Calculate F1 score
    f1 = f1_score(y_true, y_pred, average='macro')

    return test_loss, accuracy, f1

num_epochs = 10
for t in range(num_epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_loader, net, loss_fn, optimizer)
    test_loop(test_loader, net, loss_fn)
print("Done!")

Epoch 1
-------------------------------

---------------------------------------------------------------------------
RuntimeError Traceback (most recent call last)
<ipython-input-22-d7e9f571d54f> in <cell line: 2>() 2 for t in range(num_epochs): 3 print(f"Epoch {t+1}\n-------------------------------") ----> 4 train_loop(train_loader, net, loss_fn, optimizer) 5 test_loop(test_loader, net, loss_fn) 6 print("Done!")
<ipython-input-21-0794db9efa4b> in train_loop(dataloader, model, loss_fn, optimizer) 9 10 optimizer.zero_grad() ---> 11 pred = model(X) 12 loss = loss_fn(pred, y) 13
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs) 1499 or _global_backward_pre_hooks or _global_backward_hooks 1500 or _global_forward_hooks or _global_forward_pre_hooks): -> 1501 return forward_call(*args, **kwargs) 1502 # Do not call functions when jit is used 1503 full_backward_hooks, non_full_backward_hooks = [], []
<ipython-input-20-769885441eb5> in forward(self, x) 17 def forward(self, x): 18 # Convolutional layers ---> 19 x = F.relu(self.conv1(x)) 20 x = F.relu(self.conv2(x)) 21 x = F.relu(self.conv3(x))
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs) 1499 or _global_backward_pre_hooks or _global_backward_hooks 1500 or _global_forward_hooks or _global_forward_pre_hooks): -> 1501 return forward_call(*args, **kwargs) 1502 # Do not call functions when jit is used 1503 full_backward_hooks, non_full_backward_hooks = [], []
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py in forward(self, input) 461 462 def forward(self, input: Tensor) -> Tensor: --> 463 return self._conv_forward(input, self.weight, self.bias) 464 465 class Conv3d(_ConvNd):
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias) 457 weight, bias, self.stride, 458 _pair(0), self.dilation, self.groups) --> 459 return F.conv2d(input, weight, bias, self.stride, 460 self.padding, self.dilation, self.groups) 461
RuntimeError: Given groups=1, weight of size [16, 1, 5, 5], expected input[1, 16, 21, 128] to have 1 channels, but got 16 channels instead
Λόγω αρχιτεκτονικής του CNN αντιμετωπίζουμε πρόβλημα στην είσοδο καθώς δεν υπάρχει συνοχή με το μέγεθος των melgrams. Όπως βλέπουμε και στο “untimeError: Given groups=1, weight of size [16, 1, 5, 5], expected input[1, 16, 21, 128] to have 1 channels, but got 16 channels instead” η αναμενόμενη είσοδος του δικτύου είναι 1 κανάλι ενώ τα δεδομένα μας έχουν 16. Αυτό μπορεί να διορθωθεί είτε ρυθμίζοντας την είσοδο να ξεκινάει με 16 κόμβους είτε ρυθμίζοντας τα δεδομένα να ανταποκρίνονται στην είσοδο αυτή.

Βήμα 4: Pooling and padding

Ενσωματώστε στα συνελικτικά επίπεδα max pooling με kerne size 2, και padding μεγέθους 2. Σχολιάστε την χρησιμότητα των δύο αυτών στοιχείων. Τι επίδοση πετυχαίνετε;

class CNN_pp(nn.Module):
    def __init__(self, output_dim):
        super(CNN_pp, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)

        # Fully connected (dense) layers
        self.fc1 = nn.Linear(4096, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 32)
        self.fc4 = nn.Linear(32, output_dim)

    def forward(self, x):
        # Convolutional layers
        x = torch.relu(self.conv1(x))
        x = self.maxpool1(x)
        x = torch.relu(self.conv2(x))
        x = self.maxpool2(x)
        x = torch.relu(self.conv3(x))
        x = self.maxpool3(x)
        x = torch.relu(self.conv4(x))

        # Flatten the tensor for the fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)

        return x


device = 'cuda' if torch.cuda.is_available() else 'cpu'
net = CNN_pp(output_dim=4).to(device)

#  loss function and the optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=1e-3)


x_tr_tensor = torch.Tensor(x_tr)
y_tr_tensor = torch.LongTensor(y_tr_en)
x_val_tensor = torch.Tensor(x_val)
y_val_tensor = torch.LongTensor(y_val_en)
x_test_tensor = torch.Tensor(x_test)
y_test_tensor = torch.LongTensor(y_test_en)

# Reshape
x_tr_tensor = x_tr_tensor.unsqueeze(1)
x_val_tensor = x_val_tensor.unsqueeze(1)
x_test_tensor = x_test_tensor.unsqueeze(1)

#ensorDataset
train_dataset = TensorDataset(x_tr_tensor, y_tr_tensor)
val_dataset = TensorDataset(x_val_tensor, y_val_tensor)
test_dataset = TensorDataset(x_test_tensor, y_test_tensor)


# DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
num_epochs = 30

loss_values = []
accuracy_values = []
f1_values = []

for t in range(num_epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_loader, net, loss_fn, optimizer)
    loss, accuracy, f1 = test_loop(test_loader, net, loss_fn)
    loss_values.append(loss)
    accuracy_values.append(accuracy)
    f1_values.append(f1)


print("Done!")


Epoch 1
-------------------------------
Loss: 1.3687  [0/3200]
Loss: 1.3403  [1600/3200]
Epoch 2
-------------------------------
Loss: 1.0408  [0/3200]
Loss: 0.5099  [1600/3200]
Epoch 3
-------------------------------
Loss: 0.6743  [0/3200]
Loss: 0.5617  [1600/3200]
Epoch 4
-------------------------------
Loss: 0.8739  [0/3200]
Loss: 0.6452  [1600/3200]
Epoch 5
-------------------------------
Loss: 0.5401  [0/3200]
Loss: 0.4103  [1600/3200]
Epoch 6
-------------------------------
Loss: 0.7094  [0/3200]
Loss: 0.3338  [1600/3200]
Epoch 7
-------------------------------
Loss: 0.3439  [0/3200]
Loss: 0.1714  [1600/3200]
Epoch 8
-------------------------------
Loss: 0.5574  [0/3200]
Loss: 0.5209  [1600/3200]
Epoch 9
-------------------------------
Loss: 0.2955  [0/3200]
Loss: 0.3907  [1600/3200]
Epoch 10
-------------------------------
Loss: 0.2174  [0/3200]
Loss: 0.4491  [1600/3200]
Epoch 11
-------------------------------
Loss: 0.0689  [0/3200]
Loss: 0.3575  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.2543  [0/3200]
Loss: 0.2774  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.3964  [0/3200]
Loss: 0.5980  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.2144  [0/3200]
Loss: 0.2252  [1600/3200]
Epoch 15
-------------------------------
Loss: 0.2286  [0/3200]
Loss: 0.5377  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.2715  [0/3200]
Loss: 0.0486  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.1561  [0/3200]
Loss: 0.2469  [1600/3200]
Epoch 18
-------------------------------
Loss: 0.2097  [0/3200]
Loss: 0.1108  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.1432  [0/3200]
Loss: 0.0026  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.1215  [0/3200]
Loss: 0.0842  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.0063  [0/3200]
Loss: 0.0451  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.0161  [0/3200]
Loss: 0.0484  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.0980  [0/3200]
Loss: 0.0017  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.1103  [0/3200]
Loss: 0.0224  [1600/3200]
Epoch 25
-------------------------------
Loss: 0.0447  [0/3200]
Loss: 0.0035  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.1779  [0/3200]
Loss: 0.0172  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.0004  [0/3200]
Loss: 0.0008  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.0186  [0/3200]
Loss: 0.0910  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.0230  [0/3200]
Loss: 0.0412  [1600/3200]
Epoch 30
-------------------------------
Loss: 0.0339  [0/3200]
Loss: 0.0000  [1600/3200]
Done!

import matplotlib.pyplot as plt

def plot_results(loss_values, accuracy_values, f1_values):
    epochs = range(1, len(loss_values) + 1)

    # Create subplots with 1 row and 3 columns
    fig, axs = plt.subplots(1, 3, figsize=(16, 4))

    # Plot loss
    axs[0].plot(epochs, loss_values, label='Loss')
    axs[0].set_title('Training Loss over Epochs')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].legend()

    # Plot accuracy
    axs[1].plot(epochs, accuracy_values, label='Accuracy')
    axs[1].set_title('Training Accuracy over Epochs')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel('Accuracy')
    axs[1].legend()

    # Plot F1 score
    axs[2].plot(epochs, f1_values, label='F1 Score')
    axs[2].set_title('Training F1 Score (Macro-averaged) over Epochs')
    axs[2].set_xlabel('Epoch')
    axs[2].set_ylabel('F1 Score')
    axs[2].legend()


    # Adjust the spacing between subplots
    plt.tight_layout()

    # Display the plots
    plt.show()

#  the results
plot_results(loss_values, accuracy_values, f1_values)

Το pooling & padding κάνουν τα representations των δεδομένων μικρότερα και πιο εύκολα διαχειρίσημα, λειτουργούν μεμονομένα για κάθε activation map. Ουσιαστικά μειώνουν τις διαστάσεις και τη χωρική ποικιλομορφία με αποτέλεσμα, κρατώντας τα σημαντικότερα χαρακτηριστικά, να εξασφαλίζουν μεγαλύτερη, γρηγορότερη και ευκολότερη (με μικρότερη ανάγκη σε computation) εκμάθηση.

Τα αποτελέσματα του μοντέλου υποδεικνύουν μία απόδοση της τάξης του 70% με ένα αυξανόμενο training loss μετά απο την 10η epoch γεγονός που επισημαίνει over fitting.

Βήμα 5: Αλγόριθμοι βελτιστοποίησης

Υπάρχουν διαφορετικοί αλγόριθμοι βελτιστοποίησης ενός Νευρωνικού δικτύου. Δοκιμάστε ένα σύνολο από optimizers που αναφέρονται εδώ, και φτιάξτε ένα πινακάκι που στις στήλες θα περιέχει τους αλγόριθμους και στις γραμμές τις μετρικές accuracy και f1. Τι διαφορές παρατηρείτε στην επίδοση;

optimizers = ['Adam', 'SGD', 'Adagrad', 'AdamW', 'Adamax', 'NAdam', 'Rprop']
num_optimizers = len(optimizers)

best_accuracy = np.zeros(num_optimizers)
best_f1_score = np.zeros(num_optimizers)

for i, optimizer_name in enumerate(optimizers):
    print(f"Training with optimizer: {optimizer_name}")

    # Create a new instance of the model
    net = CNN_pp(output_dim=4).to(device)

    # Define the optimizer based on the optimizer name
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(net.parameters(), lr=1e-3)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)
    elif optimizer_name == 'Adagrad':
        optimizer = optim.Adagrad(net.parameters(), lr=0.01)
    elif optimizer_name == 'AdamW':
        optimizer = optim.AdamW(net.parameters(), lr=1e-3)
    elif optimizer_name == 'SparseAdam':
        optimizer = optim.SparseAdam(net.parameters(), lr=1e-3)
    elif optimizer_name == 'Adamax':
        optimizer = optim.Adamax(net.parameters(), lr=1e-3)
    elif optimizer_name == 'NAdam':
        optimizer = optim.NAdam(net.parameters(), lr=1e-3)
    elif optimizer_name == 'Rprop':
        optimizer = optim.Rprop(net.parameters())
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    loss_values = []
    accuracy_values = []
    f1_values = []

    for t in range(num_epochs):
        print(f"Epoch {t+1}\n-------------------------------")
        train_loop(train_loader, net, loss_fn, optimizer)
        loss, accuracy, f1 = test_loop(test_loader, net, loss_fn)
        loss_values.append(loss)
        accuracy_values.append(accuracy)
        f1_values.append(f1)



        # Update best accuracy and F1 score if the current epoch has higher values
        if accuracy > best_accuracy[i]:
            best_accuracy[i] = accuracy
        if f1 > best_f1_score[i]:
            best_f1_score[i] = f1

    print(f"Training with optimizer {optimizer_name} completed!")
    print("======================================")

# Create a DataFrame to store the results
results_df = pd.DataFrame(index=['Accuracy', 'F1 Score'], columns=optimizers)
results_df.index.name = 'Metrics'

# Populate the DataFrame with accuracy and F1 score values
results_df.loc['Accuracy'] = best_accuracy
results_df.loc['F1 Score'] = best_f1_score*100

Training with optimizer: Adam
Epoch 1
-------------------------------
Loss: 1.3854  [0/3200]
Loss: 1.3498  [1600/3200]
Epoch 2
-------------------------------
Loss: 1.0423  [0/3200]
Loss: 0.6102  [1600/3200]
Epoch 3
-------------------------------
Loss: 0.8159  [0/3200]
Loss: 0.9058  [1600/3200]
Epoch 4
-------------------------------
Loss: 1.2136  [0/3200]
Loss: 0.6411  [1600/3200]
Epoch 5
-------------------------------
Loss: 0.5958  [0/3200]
Loss: 0.6878  [1600/3200]
Epoch 6
-------------------------------
Loss: 0.3081  [0/3200]
Loss: 1.2732  [1600/3200]
Epoch 7
-------------------------------
Loss: 0.5349  [0/3200]
Loss: 0.8935  [1600/3200]
Epoch 8
-------------------------------
Loss: 0.2974  [0/3200]
Loss: 0.5040  [1600/3200]
Epoch 9
-------------------------------
Loss: 0.6809  [0/3200]
Loss: 0.4246  [1600/3200]
Epoch 10
-------------------------------
Loss: 0.0453  [0/3200]
Loss: 0.6138  [1600/3200]
Epoch 11
-------------------------------
Loss: 0.2450  [0/3200]
Loss: 0.1821  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.1321  [0/3200]
Loss: 0.3227  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.0514  [0/3200]
Loss: 0.1265  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.0037  [0/3200]
Loss: 0.1360  [1600/3200]
Epoch 15
-------------------------------
Loss: 0.1095  [0/3200]
Loss: 0.0520  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.1050  [0/3200]
Loss: 0.0675  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.1156  [0/3200]
Loss: 0.0410  [1600/3200]
Epoch 18
-------------------------------
Loss: 0.1095  [0/3200]
Loss: 0.0628  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.0154  [0/3200]
Loss: 0.1586  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.0211  [0/3200]
Loss: 0.1589  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.0208  [0/3200]
Loss: 0.0004  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.0584  [0/3200]
Loss: 0.0038  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.0004  [0/3200]
Loss: 0.1995  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.0040  [0/3200]
Loss: 0.0107  [1600/3200]
Epoch 25
-------------------------------
Loss: 1.7694  [0/3200]
Loss: 0.0405  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.0320  [0/3200]
Loss: 0.0105  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.0099  [0/3200]
Loss: 0.0008  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.0008  [0/3200]
Loss: 0.0748  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.0407  [0/3200]
Loss: 0.2180  [1600/3200]
Epoch 30
-------------------------------
Loss: 0.0100  [0/3200]
Loss: 0.0117  [1600/3200]
Training with optimizer Adam completed!
======================================
Training with optimizer: SGD
Epoch 1
-------------------------------
Loss: 1.4139  [0/3200]
Loss: 1.4231  [1600/3200]
Epoch 2
-------------------------------
Loss: 1.3720  [0/3200]
Loss: 1.3806  [1600/3200]
Epoch 3
-------------------------------
Loss: 1.3012  [0/3200]
Loss: 1.3844  [1600/3200]
Epoch 4
-------------------------------
Loss: 1.3798  [0/3200]
Loss: 1.3555  [1600/3200]
Epoch 5
-------------------------------
Loss: 1.3022  [0/3200]
Loss: 1.3958  [1600/3200]
Epoch 6
-------------------------------
Loss: 1.3842  [0/3200]
Loss: 1.1228  [1600/3200]
Epoch 7
-------------------------------
Loss: 1.4535  [0/3200]
Loss: 1.3178  [1600/3200]
Epoch 8
-------------------------------
Loss: 1.2432  [0/3200]
Loss: 1.0531  [1600/3200]
Epoch 9
-------------------------------
Loss: 1.0004  [0/3200]
Loss: 1.4640  [1600/3200]
Epoch 10
-------------------------------
Loss: 1.3143  [0/3200]
Loss: 1.3898  [1600/3200]
Epoch 11
-------------------------------
Loss: 1.3349  [0/3200]
Loss: 0.9378  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.9950  [0/3200]
Loss: 0.6772  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.5580  [0/3200]
Loss: 1.0755  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.9925  [0/3200]
Loss: 1.0841  [1600/3200]
Epoch 15
-------------------------------
Loss: 1.3707  [0/3200]
Loss: 1.0125  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.7037  [0/3200]
Loss: 0.6236  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.6834  [0/3200]
Loss: 0.6005  [1600/3200]
Epoch 18
-------------------------------
Loss: 1.0579  [0/3200]
Loss: 1.1703  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.4433  [0/3200]
Loss: 0.5855  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.8455  [0/3200]
Loss: 0.6275  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.9273  [0/3200]
Loss: 0.5612  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.4358  [0/3200]
Loss: 0.4515  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.4206  [0/3200]
Loss: 0.4548  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.4826  [0/3200]
Loss: 0.5445  [1600/3200]
Epoch 25
-------------------------------
Loss: 0.3259  [0/3200]
Loss: 0.7409  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.6010  [0/3200]
Loss: 0.2363  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.5401  [0/3200]
Loss: 0.4991  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.1653  [0/3200]
Loss: 0.8922  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.2769  [0/3200]
Loss: 0.5342  [1600/3200]
Epoch 30
-------------------------------
Loss: 0.4252  [0/3200]
Loss: 0.2210  [1600/3200]
Training with optimizer SGD completed!
======================================
Training with optimizer: Adagrad
Epoch 1
-------------------------------
Loss: 1.4216  [0/3200]
Loss: 1.4055  [1600/3200]
Epoch 2
-------------------------------
Loss: 1.2372  [0/3200]
Loss: 1.3615  [1600/3200]
Epoch 3
-------------------------------
Loss: 0.7423  [0/3200]
Loss: 0.6930  [1600/3200]
Epoch 4
-------------------------------
Loss: 0.9452  [0/3200]
Loss: 0.6348  [1600/3200]
Epoch 5
-------------------------------
Loss: 0.6392  [0/3200]
Loss: 0.9929  [1600/3200]
Epoch 6
-------------------------------
Loss: 0.3655  [0/3200]
Loss: 0.6415  [1600/3200]
Epoch 7
-------------------------------
Loss: 0.1869  [0/3200]
Loss: 0.4421  [1600/3200]
Epoch 8
-------------------------------
Loss: 0.2708  [0/3200]
Loss: 0.4646  [1600/3200]
Epoch 9
-------------------------------
Loss: 0.1971  [0/3200]
Loss: 0.1956  [1600/3200]
Epoch 10
-------------------------------
Loss: 0.2531  [0/3200]
Loss: 0.3257  [1600/3200]
Epoch 11
-------------------------------
Loss: 0.1142  [0/3200]
Loss: 0.0993  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.0831  [0/3200]
Loss: 0.0614  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.0109  [0/3200]
Loss: 0.0591  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.0048  [0/3200]
Loss: 0.0027  [1600/3200]
Epoch 15
-------------------------------
Loss: 0.0058  [0/3200]
Loss: 0.0020  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0034  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.0008  [0/3200]
Loss: 0.0006  [1600/3200]
Epoch 18
-------------------------------
Loss: 0.0011  [0/3200]
Loss: 0.0008  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.0004  [0/3200]
Loss: 0.0013  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0013  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0005  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.0008  [0/3200]
Loss: 0.0001  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0001  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.0011  [0/3200]
Loss: 0.0004  [1600/3200]
Epoch 25
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0008  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0002  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0003  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0003  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Epoch 30
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0005  [1600/3200]
Training with optimizer Adagrad completed!
======================================
Training with optimizer: AdamW
Epoch 1
-------------------------------
Loss: 1.4293  [0/3200]
Loss: 1.3756  [1600/3200]
Epoch 2
-------------------------------
Loss: 1.0445  [0/3200]
Loss: 0.7725  [1600/3200]
Epoch 3
-------------------------------
Loss: 0.8469  [0/3200]
Loss: 0.9428  [1600/3200]
Epoch 4
-------------------------------
Loss: 0.5925  [0/3200]
Loss: 1.0585  [1600/3200]
Epoch 5
-------------------------------
Loss: 0.6873  [0/3200]
Loss: 1.0126  [1600/3200]
Epoch 6
-------------------------------
Loss: 0.3892  [0/3200]
Loss: 0.3065  [1600/3200]
Epoch 7
-------------------------------
Loss: 0.7152  [0/3200]
Loss: 0.3302  [1600/3200]
Epoch 8
-------------------------------
Loss: 0.3403  [0/3200]
Loss: 0.7261  [1600/3200]
Epoch 9
-------------------------------
Loss: 0.2552  [0/3200]
Loss: 0.4708  [1600/3200]
Epoch 10
-------------------------------
Loss: 0.1230  [0/3200]
Loss: 0.1862  [1600/3200]
Epoch 11
-------------------------------
Loss: 0.0630  [0/3200]
Loss: 0.0326  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.2349  [0/3200]
Loss: 0.2575  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.1140  [0/3200]
Loss: 0.1724  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.2228  [0/3200]
Loss: 0.1402  [1600/3200]
Epoch 15
-------------------------------
Loss: 0.1205  [0/3200]
Loss: 0.1280  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.0032  [0/3200]
Loss: 0.0518  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.1953  [0/3200]
Loss: 0.2203  [1600/3200]
Epoch 18
-------------------------------
Loss: 0.0258  [0/3200]
Loss: 0.0026  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.0017  [0/3200]
Loss: 0.0835  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.0845  [0/3200]
Loss: 0.0202  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.0049  [0/3200]
Loss: 0.0319  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.1856  [0/3200]
Loss: 0.0048  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.0159  [0/3200]
Loss: 1.2474  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.0060  [0/3200]
Loss: 0.0002  [1600/3200]
Epoch 25
-------------------------------
Loss: 0.0353  [0/3200]
Loss: 0.0227  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.0282  [0/3200]
Loss: 0.0266  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.0035  [0/3200]
Loss: 0.2115  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.0802  [0/3200]
Loss: 0.0078  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.0177  [0/3200]
Loss: 0.0087  [1600/3200]
Epoch 30
-------------------------------
Loss: 0.0043  [0/3200]
Loss: 0.0049  [1600/3200]
Training with optimizer AdamW completed!
======================================
Training with optimizer: Adamax
Epoch 1
-------------------------------
Loss: 1.3796  [0/3200]
Loss: 1.2322  [1600/3200]
Epoch 2
-------------------------------
Loss: 1.0428  [0/3200]
Loss: 0.5514  [1600/3200]
Epoch 3
-------------------------------
Loss: 0.9006  [0/3200]
Loss: 0.9530  [1600/3200]
Epoch 4
-------------------------------
Loss: 0.9614  [0/3200]
Loss: 0.6436  [1600/3200]
Epoch 5
-------------------------------
Loss: 0.6381  [0/3200]
Loss: 0.4443  [1600/3200]
Epoch 6
-------------------------------
Loss: 0.5359  [0/3200]
Loss: 0.7718  [1600/3200]
Epoch 7
-------------------------------
Loss: 0.3078  [0/3200]
Loss: 0.3412  [1600/3200]
Epoch 8
-------------------------------
Loss: 0.3368  [0/3200]
Loss: 0.8629  [1600/3200]
Epoch 9
-------------------------------
Loss: 0.2356  [0/3200]
Loss: 0.3088  [1600/3200]
Epoch 10
-------------------------------
Loss: 0.0831  [0/3200]
Loss: 0.2544  [1600/3200]
Epoch 11
-------------------------------
Loss: 0.0932  [0/3200]
Loss: 0.5058  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.1682  [0/3200]
Loss: 0.0572  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.5940  [0/3200]
Loss: 0.3154  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.0129  [0/3200]
Loss: 0.0977  [1600/3200]
Epoch 15
-------------------------------
Loss: 0.0145  [0/3200]
Loss: 0.0147  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.0067  [0/3200]
Loss: 0.0210  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.1418  [0/3200]
Loss: 0.0058  [1600/3200]
Epoch 18
-------------------------------
Loss: 0.0101  [0/3200]
Loss: 0.0012  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.0014  [0/3200]
Loss: 0.0005  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.0214  [0/3200]
Loss: 0.0046  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.0055  [0/3200]
Loss: 0.0007  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.0055  [0/3200]
Loss: 0.0062  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0001  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0002  [1600/3200]
Epoch 25
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]
Epoch 30
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]
Training with optimizer Adamax completed!
======================================
Training with optimizer: NAdam
Epoch 1
-------------------------------
Loss: 1.4303  [0/3200]
Loss: 1.1347  [1600/3200]
Epoch 2
-------------------------------
Loss: 0.9017  [0/3200]
Loss: 0.9135  [1600/3200]
Epoch 3
-------------------------------
Loss: 1.1566  [0/3200]
Loss: 0.7889  [1600/3200]
Epoch 4
-------------------------------
Loss: 1.0084  [0/3200]
Loss: 0.6057  [1600/3200]
Epoch 5
-------------------------------
Loss: 0.5579  [0/3200]
Loss: 0.3749  [1600/3200]
Epoch 6
-------------------------------
Loss: 0.2830  [0/3200]
Loss: 0.3481  [1600/3200]
Epoch 7
-------------------------------
Loss: 0.2356  [0/3200]
Loss: 0.2312  [1600/3200]
Epoch 8
-------------------------------
Loss: 0.1189  [0/3200]
Loss: 0.2817  [1600/3200]
Epoch 9
-------------------------------
Loss: 0.3269  [0/3200]
Loss: 0.1409  [1600/3200]
Epoch 10
-------------------------------
Loss: 0.1154  [0/3200]
Loss: 0.1056  [1600/3200]
Epoch 11
-------------------------------
Loss: 0.1030  [0/3200]
Loss: 0.0141  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.0825  [0/3200]
Loss: 0.3886  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.0320  [0/3200]
Loss: 0.1555  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.0152  [0/3200]
Loss: 0.0008  [1600/3200]
Epoch 15
-------------------------------
Loss: 0.0602  [0/3200]
Loss: 0.2877  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.0151  [0/3200]
Loss: 0.0650  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.1813  [0/3200]
Loss: 0.0416  [1600/3200]
Epoch 18
-------------------------------
Loss: 0.0011  [0/3200]
Loss: 0.0022  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.0705  [0/3200]
Loss: 0.0013  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.0079  [0/3200]
Loss: 0.1526  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.0667  [0/3200]
Loss: 0.0080  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.0022  [0/3200]
Loss: 0.3028  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.0095  [0/3200]
Loss: 0.1107  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.0392  [0/3200]
Loss: 0.2812  [1600/3200]
Epoch 25
-------------------------------
Loss: 0.0290  [0/3200]
Loss: 0.0011  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.0089  [0/3200]
Loss: 0.0151  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.0007  [0/3200]
Loss: 0.0001  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.0008  [0/3200]
Loss: 0.0042  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.0057  [0/3200]
Loss: 0.0026  [1600/3200]
Epoch 30
-------------------------------
Loss: 0.0239  [0/3200]
Loss: 0.0148  [1600/3200]
Training with optimizer NAdam completed!
======================================
Training with optimizer: Rprop
Epoch 1
-------------------------------
Loss: 1.3661  [0/3200]
Loss: 0.8348  [1600/3200]
Epoch 2
-------------------------------
Loss: 0.9668  [0/3200]
Loss: 0.9090  [1600/3200]
Epoch 3
-------------------------------
Loss: 0.9755  [0/3200]
Loss: 0.5853  [1600/3200]
Epoch 4
-------------------------------
Loss: 0.9354  [0/3200]
Loss: 0.6807  [1600/3200]
Epoch 5
-------------------------------
Loss: 0.9559  [0/3200]
Loss: 3.5589  [1600/3200]
Epoch 6
-------------------------------
Loss: 11.2873  [0/3200]
Loss: 1.1461  [1600/3200]
Epoch 7
-------------------------------
Loss: 0.7920  [0/3200]
Loss: 0.6833  [1600/3200]
Epoch 8
-------------------------------
Loss: 1.3251  [0/3200]
Loss: 1.1744  [1600/3200]
Epoch 9
-------------------------------
Loss: 1.9050  [0/3200]
Loss: 2.2030  [1600/3200]
Epoch 10
-------------------------------
Loss: 0.7505  [0/3200]
Loss: 0.7182  [1600/3200]
Epoch 11
-------------------------------
Loss: 0.9337  [0/3200]
Loss: 0.8465  [1600/3200]
Epoch 12
-------------------------------
Loss: 0.9420  [0/3200]
Loss: 1.4514  [1600/3200]
Epoch 13
-------------------------------
Loss: 0.9907  [0/3200]
Loss: 9.1591  [1600/3200]
Epoch 14
-------------------------------
Loss: 0.5760  [0/3200]
Loss: 7.7272  [1600/3200]
Epoch 15
-------------------------------
Loss: 0.6709  [0/3200]
Loss: 0.8907  [1600/3200]
Epoch 16
-------------------------------
Loss: 0.6522  [0/3200]
Loss: 0.7571  [1600/3200]
Epoch 17
-------------------------------
Loss: 0.5530  [0/3200]
Loss: 0.7473  [1600/3200]
Epoch 18
-------------------------------
Loss: 0.7926  [0/3200]
Loss: 1.8730  [1600/3200]
Epoch 19
-------------------------------
Loss: 0.7376  [0/3200]
Loss: 0.5596  [1600/3200]
Epoch 20
-------------------------------
Loss: 0.4930  [0/3200]
Loss: 0.7247  [1600/3200]
Epoch 21
-------------------------------
Loss: 0.8551  [0/3200]
Loss: 0.6746  [1600/3200]
Epoch 22
-------------------------------
Loss: 0.8424  [0/3200]
Loss: 0.7751  [1600/3200]
Epoch 23
-------------------------------
Loss: 0.8969  [0/3200]
Loss: 0.6952  [1600/3200]
Epoch 24
-------------------------------
Loss: 0.5957  [0/3200]
Loss: 0.7913  [1600/3200]
Epoch 25
-------------------------------
Loss: 1.0558  [0/3200]
Loss: 0.5001  [1600/3200]
Epoch 26
-------------------------------
Loss: 0.8675  [0/3200]
Loss: 25.0199  [1600/3200]
Epoch 27
-------------------------------
Loss: 0.9021  [0/3200]
Loss: 0.7596  [1600/3200]
Epoch 28
-------------------------------
Loss: 0.3566  [0/3200]
Loss: 0.8132  [1600/3200]
Epoch 29
-------------------------------
Loss: 0.5265  [0/3200]
Loss: 669.5748  [1600/3200]
Epoch 30
-------------------------------
Loss: 1.1464  [0/3200]
Loss: 20.1637  [1600/3200]
Training with optimizer Rprop completed!
======================================

# Print the results DataFrame
print("Results:")
results_df
Results:


import matplotlib.pyplot as plt

# Transpose the DataFrame to swap rows and columns
results_df_1 = results_df.transpose()

# Plot the double bar chart
fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.35
opacity = 0.8

optimizers_range = range(num_optimizers)

accuracy_bars = ax.bar(optimizers_range, results_df_1['Accuracy'], bar_width, alpha=opacity, label='Accuracy')
f1_bars = ax.bar([x + bar_width for x in optimizers_range], results_df_1['F1 Score'], bar_width, alpha=opacity, label='F1 Score')

# Add labels, title, and ticks
ax.set_xlabel('Optimizer')
ax.set_ylabel('Scores')
ax.set_title('Accuracy and F1 Score for each Optimizer')
ax.set_xticks([r + bar_width / 2 for r in optimizers_range])
ax.set_xticklabels(optimizers)
ax.legend()

# Adjust the y-axis limits to ensure all data points are visible
min_value = min(results_df_1.values.flatten())
max_value = max(results_df_1.values.flatten())
padding = 0.05 * (max_value - min_value)  # Add a small padding to the limits
ax.set_ylim(min_value - padding, max_value + padding)

# Display the plot
plt.tight_layout()
plt.show()





Παρατηρούμε ότι οι Adam-based επιδόσεις (Adam, Adagrad, AdamW, Adamax, NAdam) είναι σχετικά κοντά σε απόδοση με βέλτιστη την Adamax με 75.29% Accuracy και 75.63% f1 score. Έπειτα έχουμε ενα χαηλότερο ποσοστό για Rprop optimizer και ενα πολύ χαμηλό για SGD. Απο μία σύντομη διερεύνηση ο Adamax βελτιστοποιεί τον απλο Adam optimizer ξεπερνόντας τους περιορισμούς του δεύτερου απο άποψη μνήμης και σύγκλισης. Eπεκτείνει την έννοια του υπολογισμού εκθετικά αποσβεννύμενων μέσων όρων των παρελθόντων κλίσεων και τετραγώνων κλίσεων. Η κύρια διαφορά στον Adamax είναι ο τρόπος με τον οποίο υπολογίζεται το βήμα ενημέρωσης για τις παραμέτρους. Στον Adam, το βήμα ενημέρωσης υπολογίζεται χρησιμοποιώντας την ανά χείρας διαίρεση του εκθετικά αποσβεννύμενου μέσου όρου των παρελθόντων κλίσεων με την τετραγωνική ρίζα του εκθετικά αποσβεννύμενου μέσου όρου των παρελθόντων τετραγώνων κλίσεων. Στον Adamax, η διαίρεση αντικαθίσταται από την επιλογή του μέγιστου μεταξύ του εκθετικά αποσβεννύμενου

Ερώτημα 3: Improving Performance
Βήμα 1: Reproducibility

Για να βελτιώσουμε την απόδοση του δικτύου θα πρέπει να δοκιμάσουμε διάφορες τεχνικές.Για να είμαστε σίγουροι αν μια τεχνική βελτιώνει την απόδοση θα πρέπει να εκπαιδεύουμετο δίκτυο υπό ακριβώς τις ίδιες συνθήκες. Αυτό σημαίνει πως η αρχικοποίηση των βαρών και η σειρά των δεδομένων θα πρέπει να είναι κάθε φορά ίδια, ώστε η επίδοση του δικτύουνα μην εξαρτάται από κάποιο καλύτερο σημείο αρχικοποίησης ή κάποια ευνοϊκότερηδιαχείριση των δεδομένων.

Για τον λόγο αυτό θα πρέπει να κάνετε “seed” όλες τις απαραίτητες βιβλιοθήκες και αλγορίθμους. Συμβουλευτείτε το ακόλουθο άρθρο και κάντε τις απαραίτητες αλλαγές στον κώδικά σας. Δοκιμάστε να τρέξετε 2 φορές την ίδια ακριβώς διαδικασία εκπαίδευσης. Θα πρέπει να πετυχαίνετε ακριβώς το ίδιο loss σε κάθε εποχή του train και τις ίδιες επιδόσεις στο test set.

class CNN_pp2(nn.Module):
    def __init__(self, output_dim):
        super(CNN_pp2, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)

        # Fully connected (dense) layers
        self.fc1 = nn.Linear(4096, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 32)
        self.fc4 = nn.Linear(32, output_dim)

    def forward(self, x):
        # Convolutional layers
        x = torch.relu(self.conv1(x))
        x = self.maxpool1(x)
        x = torch.relu(self.conv2(x))
        x = self.maxpool2(x)
        x = torch.relu(self.conv3(x))
        x = self.maxpool3(x)
        x = torch.relu(self.conv4(x))

        # Flatten the tensor for the fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)

        return x


device = 'cuda' if torch.cuda.is_available() else 'cpu'

def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()  # Set the model to training mode
    for batch, (X, y) in enumerate(dataloader):
        X = X.to(device)
        y = y.to(device)

        optimizer.zero_grad()
        pred = model(X)
        loss = loss_fn(pred, y)

        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            current = batch * len(X)
            print(f"Loss: {loss.item():.4f}  [{current}/{size}]")

def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    model.eval()  # Set the model to evaluation mode
    test_loss, correct = 0, 0
    y_true, y_pred = [], []

    with torch.no_grad():
        for X, y in dataloader:
            X = X.to(device)
            y = y.to(device)

            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).sum().item()

            # Collect true labels and predicted labels for F1 score calculation
            y_true.extend(y.tolist())
            y_pred.extend(pred.argmax(1).tolist())

    test_loss /= size
    correct /= size
    accuracy = 100 * correct

    # Calculate F1 score
    f1 = f1_score(y_true, y_pred, average='macro')

    return test_loss, accuracy, f1
import random
import torch.backends.cudnn as cudnn
import os
from numpy.random import MT19937
from numpy.random import RandomState, SeedSequence

SEED = 12345
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

disable_cudnn = False
if disable_cudnn:
    torch.backends.cudnn.enabled = False
else:
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# Set the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Create the model
model = CNN_pp2(output_dim=4).to(device)
model.to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adamax(model.parameters(), lr=1e-3)
print(optimizer)

Adamax (
Parameter Group 0
    betas: (0.9, 0.999)
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

# Training and validation
for epoch in range(30):
    print(f"Epoch {epoch + 1}\n-------------------------------")
    train_loop(train_loader, model, loss_fn, optimizer)
    test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
    print(f"\nValidation Error: \n- Loss: {test_loss:.4f}\n- Accuracy: {accuracy:.2f}%\n- F1 Score: {f1:.4f}\n")


Epoch 1
-------------------------------
Loss: 1.3716  [0/3200]
Loss: 1.3818  [1600/3200]

Validation Error: 
- Loss: 0.0616
- Accuracy: 51.62%
- F1 Score: 0.4590

Epoch 2
-------------------------------
Loss: 1.0170  [0/3200]
Loss: 0.9707  [1600/3200]

Validation Error: 
- Loss: 0.0626
- Accuracy: 54.50%
- F1 Score: 0.5331

Epoch 3
-------------------------------
Loss: 0.6618  [0/3200]
Loss: 0.7981  [1600/3200]

Validation Error: 
- Loss: 0.0455
- Accuracy: 69.12%
- F1 Score: 0.6837

Epoch 4
-------------------------------
Loss: 0.4836  [0/3200]
Loss: 0.4983  [1600/3200]

Validation Error: 
- Loss: 0.0417
- Accuracy: 73.25%
- F1 Score: 0.7268

Epoch 5
-------------------------------
Loss: 0.4320  [0/3200]
Loss: 0.4848  [1600/3200]

Validation Error: 
- Loss: 0.0404
- Accuracy: 75.00%
- F1 Score: 0.7385

Epoch 6
-------------------------------
Loss: 0.7810  [0/3200]
Loss: 0.3889  [1600/3200]

Validation Error: 
- Loss: 0.0400
- Accuracy: 76.88%
- F1 Score: 0.7680

Epoch 7
-------------------------------
Loss: 0.5740  [0/3200]
Loss: 0.2804  [1600/3200]

Validation Error: 
- Loss: 0.0407
- Accuracy: 73.88%
- F1 Score: 0.7387

Epoch 8
-------------------------------
Loss: 0.2541  [0/3200]
Loss: 0.3954  [1600/3200]

Validation Error: 
- Loss: 0.0403
- Accuracy: 75.62%
- F1 Score: 0.7562

Epoch 9
-------------------------------
Loss: 0.3891  [0/3200]
Loss: 0.1329  [1600/3200]

Validation Error: 
- Loss: 0.0419
- Accuracy: 76.38%
- F1 Score: 0.7584

Epoch 10
-------------------------------
Loss: 0.2734  [0/3200]
Loss: 0.4373  [1600/3200]

Validation Error: 
- Loss: 0.0446
- Accuracy: 76.38%
- F1 Score: 0.7655

Epoch 11
-------------------------------
Loss: 0.1688  [0/3200]
Loss: 0.0772  [1600/3200]

Validation Error: 
- Loss: 0.0579
- Accuracy: 73.88%
- F1 Score: 0.7427

Epoch 12
-------------------------------
Loss: 0.0712  [0/3200]
Loss: 0.1368  [1600/3200]

Validation Error: 
- Loss: 0.0625
- Accuracy: 73.75%
- F1 Score: 0.7353

Epoch 13
-------------------------------
Loss: 0.0235  [0/3200]
Loss: 0.1591  [1600/3200]

Validation Error: 
- Loss: 0.0576
- Accuracy: 76.62%
- F1 Score: 0.7647

Epoch 14
-------------------------------
Loss: 0.0480  [0/3200]
Loss: 0.0485  [1600/3200]

Validation Error: 
- Loss: 0.0595
- Accuracy: 75.12%
- F1 Score: 0.7479

Epoch 15
-------------------------------
Loss: 0.0372  [0/3200]
Loss: 0.0135  [1600/3200]

Validation Error: 
- Loss: 0.0745
- Accuracy: 73.25%
- F1 Score: 0.7260

Epoch 16
-------------------------------
Loss: 0.0150  [0/3200]
Loss: 0.0014  [1600/3200]

Validation Error: 
- Loss: 0.0631
- Accuracy: 74.00%
- F1 Score: 0.7358

Epoch 17
-------------------------------
Loss: 0.0202  [0/3200]
Loss: 0.0009  [1600/3200]

Validation Error: 
- Loss: 0.0880
- Accuracy: 76.38%
- F1 Score: 0.7613

Epoch 18
-------------------------------
Loss: 0.0022  [0/3200]
Loss: 0.0005  [1600/3200]

Validation Error: 
- Loss: 0.1079
- Accuracy: 74.62%
- F1 Score: 0.7465

Epoch 19
-------------------------------
Loss: 0.0026  [0/3200]
Loss: 0.0163  [1600/3200]

Validation Error: 
- Loss: 0.0839
- Accuracy: 75.00%
- F1 Score: 0.7481

Epoch 20
-------------------------------
Loss: 0.0024  [0/3200]
Loss: 0.0038  [1600/3200]

Validation Error: 
- Loss: 0.0913
- Accuracy: 76.62%
- F1 Score: 0.7656

Epoch 21
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0037  [1600/3200]

Validation Error: 
- Loss: 0.0969
- Accuracy: 76.25%
- F1 Score: 0.7620

Epoch 22
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0004  [1600/3200]

Validation Error: 
- Loss: 0.1010
- Accuracy: 76.88%
- F1 Score: 0.7673

Epoch 23
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error: 
- Loss: 0.1058
- Accuracy: 76.75%
- F1 Score: 0.7662

Epoch 24
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error: 
- Loss: 0.1084
- Accuracy: 76.75%
- F1 Score: 0.7657

Epoch 25
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1152
- Accuracy: 77.25%
- F1 Score: 0.7708

Epoch 26
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1203
- Accuracy: 77.00%
- F1 Score: 0.7680

Epoch 27
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1250
- Accuracy: 77.12%
- F1 Score: 0.7690

Epoch 28
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error: 
- Loss: 0.1324
- Accuracy: 77.50%
- F1 Score: 0.7726

Epoch 29
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1389
- Accuracy: 77.75%
- F1 Score: 0.7751

Epoch 30
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1430
- Accuracy: 77.38%
- F1 Score: 0.7723


# Training and validation
for epoch in range(30):
    print(f"Epoch {epoch + 1}\n-------------------------------")
    train_loop(train_loader, model, loss_fn, optimizer)
    test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
    print(f"\nValidation Error: \n- Loss: {test_loss:.4f}\n- Accuracy: {accuracy:.2f}%\n- F1 Score: {f1:.4f}\n")

Epoch 1
-------------------------------
Loss: 1.3716  [0/3200]
Loss: 1.3818  [1600/3200]

Validation Error: 
- Loss: 0.0616
- Accuracy: 51.62%
- F1 Score: 0.4590

Epoch 2
-------------------------------
Loss: 1.0170  [0/3200]
Loss: 0.9707  [1600/3200]

Validation Error: 
- Loss: 0.0626
- Accuracy: 54.50%
- F1 Score: 0.5331

Epoch 3
-------------------------------
Loss: 0.6618  [0/3200]
Loss: 0.7981  [1600/3200]

Validation Error: 
- Loss: 0.0455
- Accuracy: 69.12%
- F1 Score: 0.6837

Epoch 4
-------------------------------
Loss: 0.4836  [0/3200]
Loss: 0.4983  [1600/3200]

Validation Error: 
- Loss: 0.0417
- Accuracy: 73.25%
- F1 Score: 0.7268

Epoch 5
-------------------------------
Loss: 0.4320  [0/3200]
Loss: 0.4848  [1600/3200]

Validation Error: 
- Loss: 0.0404
- Accuracy: 75.00%
- F1 Score: 0.7385

Epoch 6
-------------------------------
Loss: 0.7810  [0/3200]
Loss: 0.3889  [1600/3200]

Validation Error: 
- Loss: 0.0400
- Accuracy: 76.88%
- F1 Score: 0.7680

Epoch 7
-------------------------------
Loss: 0.5740  [0/3200]
Loss: 0.2804  [1600/3200]

Validation Error: 
- Loss: 0.0407
- Accuracy: 73.88%
- F1 Score: 0.7387

Epoch 8
-------------------------------
Loss: 0.2541  [0/3200]
Loss: 0.3954  [1600/3200]

Validation Error: 
- Loss: 0.0403
- Accuracy: 75.62%
- F1 Score: 0.7562

Epoch 9
-------------------------------
Loss: 0.3891  [0/3200]
Loss: 0.1329  [1600/3200]

Validation Error: 
- Loss: 0.0419
- Accuracy: 76.38%
- F1 Score: 0.7584

Epoch 10
-------------------------------
Loss: 0.2734  [0/3200]
Loss: 0.4373  [1600/3200]

Validation Error: 
- Loss: 0.0446
- Accuracy: 76.38%
- F1 Score: 0.7655

Epoch 11
-------------------------------
Loss: 0.1688  [0/3200]
Loss: 0.0772  [1600/3200]

Validation Error: 
- Loss: 0.0579
- Accuracy: 73.88%
- F1 Score: 0.7427

Epoch 12
-------------------------------
Loss: 0.0712  [0/3200]
Loss: 0.1368  [1600/3200]

Validation Error: 
- Loss: 0.0625
- Accuracy: 73.75%
- F1 Score: 0.7353

Epoch 13
-------------------------------
Loss: 0.0235  [0/3200]
Loss: 0.1591  [1600/3200]

Validation Error: 
- Loss: 0.0576
- Accuracy: 76.62%
- F1 Score: 0.7647

Epoch 14
-------------------------------
Loss: 0.0480  [0/3200]
Loss: 0.0485  [1600/3200]

Validation Error: 
- Loss: 0.0595
- Accuracy: 75.12%
- F1 Score: 0.7479

Epoch 15
-------------------------------
Loss: 0.0372  [0/3200]
Loss: 0.0135  [1600/3200]

Validation Error: 
- Loss: 0.0745
- Accuracy: 73.25%
- F1 Score: 0.7260

Epoch 16
-------------------------------
Loss: 0.0150  [0/3200]
Loss: 0.0014  [1600/3200]

Validation Error: 
- Loss: 0.0631
- Accuracy: 74.00%
- F1 Score: 0.7358

Epoch 17
-------------------------------
Loss: 0.0202  [0/3200]
Loss: 0.0009  [1600/3200]

Validation Error: 
- Loss: 0.0880
- Accuracy: 76.38%
- F1 Score: 0.7613

Epoch 18
-------------------------------
Loss: 0.0022  [0/3200]
Loss: 0.0005  [1600/3200]

Validation Error: 
- Loss: 0.1079
- Accuracy: 74.62%
- F1 Score: 0.7465

Epoch 19
-------------------------------
Loss: 0.0026  [0/3200]
Loss: 0.0163  [1600/3200]

Validation Error: 
- Loss: 0.0839
- Accuracy: 75.00%
- F1 Score: 0.7481

Epoch 20
-------------------------------
Loss: 0.0024  [0/3200]
Loss: 0.0038  [1600/3200]

Validation Error: 
- Loss: 0.0913
- Accuracy: 76.62%
- F1 Score: 0.7656

Epoch 21
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0037  [1600/3200]

Validation Error: 
- Loss: 0.0969
- Accuracy: 76.25%
- F1 Score: 0.7620

Epoch 22
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0004  [1600/3200]

Validation Error: 
- Loss: 0.1010
- Accuracy: 76.88%
- F1 Score: 0.7673

Epoch 23
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error: 
- Loss: 0.1058
- Accuracy: 76.75%
- F1 Score: 0.7662

Epoch 24
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error: 
- Loss: 0.1084
- Accuracy: 76.75%
- F1 Score: 0.7657

Epoch 25
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1152
- Accuracy: 77.25%
- F1 Score: 0.7708

Epoch 26
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1203
- Accuracy: 77.00%
- F1 Score: 0.7680

Epoch 27
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1250
- Accuracy: 77.12%
- F1 Score: 0.7690

Epoch 28
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error: 
- Loss: 0.1324
- Accuracy: 77.50%
- F1 Score: 0.7726

Epoch 29
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1389
- Accuracy: 77.75%
- F1 Score: 0.7751

Epoch 30
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error: 
- Loss: 0.1430
- Accuracy: 77.38%
- F1 Score: 0.7723


Βήμα 2: Activation functions

Το Νευρωνικό Δίκτυό που έχουμε φτιάξει μέχρι στιγμής εφαρμόζει αποκλειστικά γραμμικούς μετασχηματισμούς στα δεδομένα. Για να μπορέσει το δίκτυο να “μάθει” πιο σύνθετες συσχετίσεις στα δεδομένα θα πρέπει να εισάγουμε μη γραμμικές συναρτήσεις ενεργοποίησης (non-linear activation functions). Την συμπεριφορά αυτή θα μας την δώσουν οι μη γραμμικές συναρτήσεις ενεργοποίησης.

Δοκιμάστε διάφορες συναρτήσεις ενεργοποίησης από αυτήν την λίστα, τοποθετώντας τες σε κάθε συνελικτικό επίπεδο (μετά την πράξη της συνέλιξης και πριν το pooling), και σε κάθε είσοδο των γραμμικών επιπέδων. Να φτιάξετε ένα πινακάκι που να περιέχει στις γραμμές τα activation functions που δοκιμάσατε και τις μετρικές f1 και accuracy στις στήλες.

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import f1_score
import pandas as pd
import numpy as np



# Activation functions
activation_functions = {
    'ReLU': nn.ReLU(),
    'ELU': nn.ELU(),
    'Softplus': nn.Softplus(),
    'Mish': nn.Mish(),
    'PReLU': nn.PReLU()
}

#  initialize the model's weights
def weights_init(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight.data)

# Function for training
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()  # Set the model to training mode
    for batch, (X, y) in enumerate(dataloader):
        X = X.to(device)
        y = y.to(device)

        optimizer.zero_grad()
        pred = model(X)
        loss = loss_fn(pred, y)

        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            current = batch * len(X)
            print(f"Loss: {loss.item():.4f}  [{current}/{size}]")

# Function for testing
def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    model.eval()
    test_loss, correct = 0, 0
    y_true, y_pred = [], []

    with torch.no_grad():
        for X, y in dataloader:
            X = X.to(device)
            y = y.to(device)

            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).sum().item()

            #  F1 score calculation
            y_true.extend(y.tolist())
            y_pred.extend(pred.argmax(1).tolist())

    test_loss /= size
    correct /= size
    accuracy = 100 * correct

    # Calculate F1 score
    f1 = f1_score(y_true, y_pred, average='macro')

    return test_loss, accuracy, f1

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# the CNN model
class CNN_pp(nn.Module):
    def __init__(self, output_dim, activation):
        super(CNN_pp, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)

        # Fully connected (dense) layers
        self.fc1 = nn.Linear(4096, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 32)
        self.fc4 = nn.Linear(32, output_dim)

        # Activation function
        self.activation = activation

    def forward(self, x):
        # Convolutional layers
        x = self.activation(self.conv1(x))
        x = self.maxpool1(x)
        x = self.activation(self.conv2(x))
        x = self.maxpool2(x)
        x = self.activation(self.conv3(x))
        x = self.maxpool3(x)
        x = self.activation(self.conv4(x))

        # Flatten the tensor for the fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        x = self.fc4(x)

        return x

# Training and evaluating the model for each activation function
for activation_name, activation in activation_functions.items():
    # Set the random seed for reproducibility
    SEED = 12345
    torch.manual_seed(SEED)
    np.random.seed(SEED)
    random.seed(SEED)
    rs = RandomState(MT19937(SeedSequence(SEED)))
    torch.cuda.manual_seed_all(SEED)

    # Initialize the model with the same starting weights
    model = CNN_pp(output_dim=4, activation=activation)
    model.apply(weights_init)
    model.to(device)
    print("Model Structure:\n---------------------\n", model)
    print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    optimizer = optim.Adamax(model.parameters(), lr=1e-3)
    print(optimizer)

    best_accuracy = 0.0
    best_f1 = 0.0

    # Training loop
    for epoch in range(30):
        print(f"Epoch {epoch + 1}\n-------------------------------")
        train_loop(train_loader, model, loss_fn, optimizer)
        test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
        print(f"\nValidation Error:\n- Loss: {test_loss:.4f}\n- Accuracy: {accuracy:.2f}%\n- F1 Score: {f1:.4f}\n")

        # Update the best accuracy and F1 score
        if accuracy > best_accuracy:
            best_accuracy = accuracy
        if f1 > best_f1:
            best_f1 = f1

    # Store the results in the DataFrame
    results_df = results_df.append({'Activation': activation_name, 'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1},
                                   ignore_index=True)

Model Structure:
---------------------
 CNN_pp(
  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (fc1): Linear(in_features=4096, out_features=1024, bias=True)
  (fc2): Linear(in_features=1024, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=32, bias=True)
  (fc4): Linear(in_features=32, out_features=4, bias=True)
  (activation): ReLU()
)
Trainable parameters: 4,735,524
Adamax (
Parameter Group 0
    betas: (0.9, 0.999)
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Epoch 1
-------------------------------
Loss: 1.9396  [0/3200]
Loss: 1.1945  [1600/3200]

Validation Error:
- Loss: 0.0679
- Accuracy: 46.00%
- F1 Score: 0.3956

Epoch 2
-------------------------------
Loss: 1.0151  [0/3200]
Loss: 0.7397  [1600/3200]

Validation Error:
- Loss: 0.0564
- Accuracy: 58.50%
- F1 Score: 0.5528

Epoch 3
-------------------------------
Loss: 0.9236  [0/3200]
Loss: 0.5102  [1600/3200]

Validation Error:
- Loss: 0.0443
- Accuracy: 71.12%
- F1 Score: 0.7026

Epoch 4
-------------------------------
Loss: 0.7396  [0/3200]
Loss: 0.7759  [1600/3200]

Validation Error:
- Loss: 0.0457
- Accuracy: 70.25%
- F1 Score: 0.6943

Epoch 5
-------------------------------
Loss: 0.5116  [0/3200]
Loss: 0.9652  [1600/3200]

Validation Error:
- Loss: 0.0451
- Accuracy: 71.00%
- F1 Score: 0.7103

Epoch 6
-------------------------------
Loss: 0.4307  [0/3200]
Loss: 0.2552  [1600/3200]

Validation Error:
- Loss: 0.0472
- Accuracy: 71.25%
- F1 Score: 0.6991

Epoch 7
-------------------------------
Loss: 0.3202  [0/3200]
Loss: 0.4060  [1600/3200]

Validation Error:
- Loss: 0.0421
- Accuracy: 73.88%
- F1 Score: 0.7314

Epoch 8
-------------------------------
Loss: 0.3626  [0/3200]
Loss: 0.1121  [1600/3200]

Validation Error:
- Loss: 0.0414
- Accuracy: 76.62%
- F1 Score: 0.7668

Epoch 9
-------------------------------
Loss: 0.0884  [0/3200]
Loss: 0.3647  [1600/3200]

Validation Error:
- Loss: 0.0508
- Accuracy: 73.00%
- F1 Score: 0.7267

Epoch 10
-------------------------------
Loss: 0.2143  [0/3200]
Loss: 0.1326  [1600/3200]

Validation Error:
- Loss: 0.0476
- Accuracy: 75.88%
- F1 Score: 0.7553

Epoch 11
-------------------------------
Loss: 0.0364  [0/3200]
Loss: 0.1491  [1600/3200]

Validation Error:
- Loss: 0.0553
- Accuracy: 73.50%
- F1 Score: 0.7369

Epoch 12
-------------------------------
Loss: 0.2314  [0/3200]
Loss: 0.0470  [1600/3200]

Validation Error:
- Loss: 0.0572
- Accuracy: 73.38%
- F1 Score: 0.7315

Epoch 13
-------------------------------
Loss: 0.0572  [0/3200]
Loss: 0.0170  [1600/3200]

Validation Error:
- Loss: 0.0703
- Accuracy: 73.50%
- F1 Score: 0.7278

Epoch 14
-------------------------------
Loss: 0.0393  [0/3200]
Loss: 0.0059  [1600/3200]

Validation Error:
- Loss: 0.0948
- Accuracy: 71.00%
- F1 Score: 0.7029

Epoch 15
-------------------------------
Loss: 0.0667  [0/3200]
Loss: 0.0019  [1600/3200]

Validation Error:
- Loss: 0.0824
- Accuracy: 72.75%
- F1 Score: 0.7212

Epoch 16
-------------------------------
Loss: 0.0434  [0/3200]
Loss: 0.0151  [1600/3200]

Validation Error:
- Loss: 0.0662
- Accuracy: 75.88%
- F1 Score: 0.7578

Epoch 17
-------------------------------
Loss: 0.0136  [0/3200]
Loss: 0.0024  [1600/3200]

Validation Error:
- Loss: 0.0774
- Accuracy: 75.88%
- F1 Score: 0.7599

Epoch 18
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0021  [1600/3200]

Validation Error:
- Loss: 0.0863
- Accuracy: 73.75%
- F1 Score: 0.7405

Epoch 19
-------------------------------
Loss: 0.0011  [0/3200]
Loss: 0.1104  [1600/3200]

Validation Error:
- Loss: 0.0850
- Accuracy: 74.62%
- F1 Score: 0.7491

Epoch 20
-------------------------------
Loss: 0.0016  [0/3200]
Loss: 0.0036  [1600/3200]

Validation Error:
- Loss: 0.1008
- Accuracy: 72.12%
- F1 Score: 0.7230

Epoch 21
-------------------------------
Loss: 0.0514  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0828
- Accuracy: 75.88%
- F1 Score: 0.7583

Epoch 22
-------------------------------
Loss: 0.0005  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0881
- Accuracy: 75.88%
- F1 Score: 0.7590

Epoch 23
-------------------------------
Loss: 0.0016  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0917
- Accuracy: 75.88%
- F1 Score: 0.7584

Epoch 24
-------------------------------
Loss: 0.0004  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.0949
- Accuracy: 76.00%
- F1 Score: 0.7590

Epoch 25
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0973
- Accuracy: 76.12%
- F1 Score: 0.7602

Epoch 26
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0002  [1600/3200]

Validation Error:
- Loss: 0.0997
- Accuracy: 76.25%
- F1 Score: 0.7617

Epoch 27
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.1024
- Accuracy: 76.12%
- F1 Score: 0.7604

Epoch 28
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.1040
- Accuracy: 76.75%
- F1 Score: 0.7662

Epoch 29
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.1061
- Accuracy: 76.62%
- F1 Score: 0.7654

Epoch 30
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.1084
- Accuracy: 76.25%
- F1 Score: 0.7619

Model Structure:
---------------------
 CNN_pp(
  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (fc1): Linear(in_features=4096, out_features=1024, bias=True)
  (fc2): Linear(in_features=1024, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=32, bias=True)
  (fc4): Linear(in_features=32, out_features=4, bias=True)
  (activation): ELU(alpha=1.0)
)
Trainable parameters: 4,735,524
Adamax (
Parameter Group 0
    betas: (0.9, 0.999)
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Epoch 1
-------------------------------
Loss: 1.9717  [0/3200]

<ipython-input-36-0ca721a378ed>:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Activation': activation_name, 'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1},

Loss: 1.2160  [1600/3200]

Validation Error:
- Loss: 0.0718
- Accuracy: 45.75%
- F1 Score: 0.4029

Epoch 2
-------------------------------
Loss: 1.1865  [0/3200]
Loss: 0.9238  [1600/3200]

Validation Error:
- Loss: 0.0598
- Accuracy: 58.75%
- F1 Score: 0.5643

Epoch 3
-------------------------------
Loss: 0.9210  [0/3200]
Loss: 0.7373  [1600/3200]

Validation Error:
- Loss: 0.0546
- Accuracy: 64.62%
- F1 Score: 0.6332

Epoch 4
-------------------------------
Loss: 0.9476  [0/3200]
Loss: 0.7366  [1600/3200]

Validation Error:
- Loss: 0.0532
- Accuracy: 61.88%
- F1 Score: 0.6172

Epoch 5
-------------------------------
Loss: 0.7978  [0/3200]
Loss: 1.1900  [1600/3200]

Validation Error:
- Loss: 0.0444
- Accuracy: 71.12%
- F1 Score: 0.6831

Epoch 6
-------------------------------
Loss: 0.7220  [0/3200]
Loss: 0.2980  [1600/3200]

Validation Error:
- Loss: 0.0420
- Accuracy: 72.88%
- F1 Score: 0.7273

Epoch 7
-------------------------------
Loss: 0.4424  [0/3200]
Loss: 0.5028  [1600/3200]

Validation Error:
- Loss: 0.0405
- Accuracy: 76.12%
- F1 Score: 0.7606

Epoch 8
-------------------------------
Loss: 1.0744  [0/3200]
Loss: 0.6385  [1600/3200]

Validation Error:
- Loss: 0.0420
- Accuracy: 73.88%
- F1 Score: 0.7439

Epoch 9
-------------------------------
Loss: 0.3403  [0/3200]
Loss: 0.6707  [1600/3200]

Validation Error:
- Loss: 0.0409
- Accuracy: 74.88%
- F1 Score: 0.7523

Epoch 10
-------------------------------
Loss: 0.7311  [0/3200]
Loss: 0.2579  [1600/3200]

Validation Error:
- Loss: 0.0322
- Accuracy: 81.12%
- F1 Score: 0.8104

Epoch 11
-------------------------------
Loss: 0.3325  [0/3200]
Loss: 0.4489  [1600/3200]

Validation Error:
- Loss: 0.0310
- Accuracy: 80.50%
- F1 Score: 0.8076

Epoch 12
-------------------------------
Loss: 0.2836  [0/3200]
Loss: 0.1893  [1600/3200]

Validation Error:
- Loss: 0.0364
- Accuracy: 76.62%
- F1 Score: 0.7678

Epoch 13
-------------------------------
Loss: 0.4252  [0/3200]
Loss: 0.4812  [1600/3200]

Validation Error:
- Loss: 0.0449
- Accuracy: 73.88%
- F1 Score: 0.7344

Epoch 14
-------------------------------
Loss: 0.3469  [0/3200]
Loss: 0.2345  [1600/3200]

Validation Error:
- Loss: 0.0372
- Accuracy: 78.00%
- F1 Score: 0.7787

Epoch 15
-------------------------------
Loss: 0.0979  [0/3200]
Loss: 0.3823  [1600/3200]

Validation Error:
- Loss: 0.0387
- Accuracy: 80.75%
- F1 Score: 0.8087

Epoch 16
-------------------------------
Loss: 0.0527  [0/3200]
Loss: 0.0239  [1600/3200]

Validation Error:
- Loss: 0.0429
- Accuracy: 79.12%
- F1 Score: 0.7925

Epoch 17
-------------------------------
Loss: 0.0635  [0/3200]
Loss: 0.3871  [1600/3200]

Validation Error:
- Loss: 0.0632
- Accuracy: 77.00%
- F1 Score: 0.7626

Epoch 18
-------------------------------
Loss: 0.0503  [0/3200]
Loss: 0.0512  [1600/3200]

Validation Error:
- Loss: 0.0724
- Accuracy: 72.88%
- F1 Score: 0.7230

Epoch 19
-------------------------------
Loss: 0.0304  [0/3200]
Loss: 0.0058  [1600/3200]

Validation Error:
- Loss: 0.0628
- Accuracy: 78.88%
- F1 Score: 0.7867

Epoch 20
-------------------------------
Loss: 0.0113  [0/3200]
Loss: 0.0036  [1600/3200]

Validation Error:
- Loss: 0.0696
- Accuracy: 77.88%
- F1 Score: 0.7775

Epoch 21
-------------------------------
Loss: 0.0095  [0/3200]
Loss: 0.0039  [1600/3200]

Validation Error:
- Loss: 0.0699
- Accuracy: 77.75%
- F1 Score: 0.7704

Epoch 22
-------------------------------
Loss: 0.0009  [0/3200]
Loss: 0.0004  [1600/3200]

Validation Error:
- Loss: 0.0719
- Accuracy: 79.38%
- F1 Score: 0.7909

Epoch 23
-------------------------------
Loss: 0.0016  [0/3200]
Loss: 0.0002  [1600/3200]

Validation Error:
- Loss: 0.0761
- Accuracy: 79.62%
- F1 Score: 0.7919

Epoch 24
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0776
- Accuracy: 79.38%
- F1 Score: 0.7901

Epoch 25
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0803
- Accuracy: 79.88%
- F1 Score: 0.7947

Epoch 26
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0824
- Accuracy: 79.75%
- F1 Score: 0.7937

Epoch 27
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0842
- Accuracy: 79.50%
- F1 Score: 0.7919

Epoch 28
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0005  [1600/3200]

Validation Error:
- Loss: 0.0871
- Accuracy: 79.88%
- F1 Score: 0.7954

Epoch 29
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.0893
- Accuracy: 80.25%
- F1 Score: 0.7997

Epoch 30
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.0916
- Accuracy: 79.88%
- F1 Score: 0.7957

Model Structure:
---------------------
 CNN_pp(
  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (fc1): Linear(in_features=4096, out_features=1024, bias=True)
  (fc2): Linear(in_features=1024, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=32, bias=True)
  (fc4): Linear(in_features=32, out_features=4, bias=True)
  (activation): Softplus(beta=1, threshold=20)
)
Trainable parameters: 4,735,524
Adamax (
Parameter Group 0
    betas: (0.9, 0.999)
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Epoch 1
-------------------------------
Loss: 2.3827  [0/3200]

<ipython-input-36-0ca721a378ed>:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Activation': activation_name, 'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1},

Loss: 1.3388  [1600/3200]

Validation Error:
- Loss: 0.0588
- Accuracy: 62.75%
- F1 Score: 0.6285

Epoch 2
-------------------------------
Loss: 0.8874  [0/3200]
Loss: 0.7700  [1600/3200]

Validation Error:
- Loss: 0.0527
- Accuracy: 65.50%
- F1 Score: 0.6500

Epoch 3
-------------------------------
Loss: 0.6121  [0/3200]
Loss: 0.6199  [1600/3200]

Validation Error:
- Loss: 0.0468
- Accuracy: 71.25%
- F1 Score: 0.7161

Epoch 4
-------------------------------
Loss: 0.8731  [0/3200]
Loss: 0.6772  [1600/3200]

Validation Error:
- Loss: 0.0425
- Accuracy: 71.38%
- F1 Score: 0.7207

Epoch 5
-------------------------------
Loss: 0.4906  [0/3200]
Loss: 1.1625  [1600/3200]

Validation Error:
- Loss: 0.0473
- Accuracy: 68.00%
- F1 Score: 0.6683

Epoch 6
-------------------------------
Loss: 0.6351  [0/3200]
Loss: 0.6465  [1600/3200]

Validation Error:
- Loss: 0.0387
- Accuracy: 74.50%
- F1 Score: 0.7248

Epoch 7
-------------------------------
Loss: 0.3080  [0/3200]
Loss: 0.4467  [1600/3200]

Validation Error:
- Loss: 0.0364
- Accuracy: 77.50%
- F1 Score: 0.7641

Epoch 8
-------------------------------
Loss: 0.9113  [0/3200]
Loss: 0.5801  [1600/3200]

Validation Error:
- Loss: 0.0467
- Accuracy: 69.75%
- F1 Score: 0.7015

Epoch 9
-------------------------------
Loss: 0.4705  [0/3200]
Loss: 0.7238  [1600/3200]

Validation Error:
- Loss: 0.0374
- Accuracy: 75.62%
- F1 Score: 0.7586

Epoch 10
-------------------------------
Loss: 0.6398  [0/3200]
Loss: 0.2699  [1600/3200]

Validation Error:
- Loss: 0.0361
- Accuracy: 77.25%
- F1 Score: 0.7691

Epoch 11
-------------------------------
Loss: 0.3345  [0/3200]
Loss: 0.4701  [1600/3200]

Validation Error:
- Loss: 0.0360
- Accuracy: 76.38%
- F1 Score: 0.7680

Epoch 12
-------------------------------
Loss: 0.3247  [0/3200]
Loss: 0.3103  [1600/3200]

Validation Error:
- Loss: 0.0390
- Accuracy: 75.25%
- F1 Score: 0.7442

Epoch 13
-------------------------------
Loss: 0.2725  [0/3200]
Loss: 0.6906  [1600/3200]

Validation Error:
- Loss: 0.0444
- Accuracy: 71.00%
- F1 Score: 0.6899

Epoch 14
-------------------------------
Loss: 0.5613  [0/3200]
Loss: 0.3689  [1600/3200]

Validation Error:
- Loss: 0.0467
- Accuracy: 76.00%
- F1 Score: 0.7550

Epoch 15
-------------------------------
Loss: 0.3767  [0/3200]
Loss: 0.3995  [1600/3200]

Validation Error:
- Loss: 0.0364
- Accuracy: 79.50%
- F1 Score: 0.7933

Epoch 16
-------------------------------
Loss: 0.1791  [0/3200]
Loss: 0.2595  [1600/3200]

Validation Error:
- Loss: 0.0376
- Accuracy: 79.00%
- F1 Score: 0.7887

Epoch 17
-------------------------------
Loss: 0.2332  [0/3200]
Loss: 0.2722  [1600/3200]

Validation Error:
- Loss: 0.0429
- Accuracy: 79.25%
- F1 Score: 0.7902

Epoch 18
-------------------------------
Loss: 0.1856  [0/3200]
Loss: 0.1244  [1600/3200]

Validation Error:
- Loss: 0.0518
- Accuracy: 74.38%
- F1 Score: 0.7454

Epoch 19
-------------------------------
Loss: 0.1861  [0/3200]
Loss: 0.4300  [1600/3200]

Validation Error:
- Loss: 0.0474
- Accuracy: 77.12%
- F1 Score: 0.7648

Epoch 20
-------------------------------
Loss: 0.1489  [0/3200]
Loss: 0.1427  [1600/3200]

Validation Error:
- Loss: 0.0578
- Accuracy: 77.88%
- F1 Score: 0.7767

Epoch 21
-------------------------------
Loss: 0.0980  [0/3200]
Loss: 0.0100  [1600/3200]

Validation Error:
- Loss: 0.0598
- Accuracy: 76.88%
- F1 Score: 0.7717

Epoch 22
-------------------------------
Loss: 0.0251  [0/3200]
Loss: 0.0090  [1600/3200]

Validation Error:
- Loss: 0.0646
- Accuracy: 78.00%
- F1 Score: 0.7774

Epoch 23
-------------------------------
Loss: 0.0498  [0/3200]
Loss: 0.0514  [1600/3200]

Validation Error:
- Loss: 0.0757
- Accuracy: 77.12%
- F1 Score: 0.7679

Epoch 24
-------------------------------
Loss: 0.0423  [0/3200]
Loss: 0.0050  [1600/3200]

Validation Error:
- Loss: 0.0653
- Accuracy: 73.38%
- F1 Score: 0.7326

Epoch 25
-------------------------------
Loss: 0.0864  [0/3200]
Loss: 0.0142  [1600/3200]

Validation Error:
- Loss: 0.0888
- Accuracy: 76.00%
- F1 Score: 0.7620

Epoch 26
-------------------------------
Loss: 0.0068  [0/3200]
Loss: 0.0032  [1600/3200]

Validation Error:
- Loss: 0.0967
- Accuracy: 76.62%
- F1 Score: 0.7646

Epoch 27
-------------------------------
Loss: 0.0051  [0/3200]
Loss: 0.0002  [1600/3200]

Validation Error:
- Loss: 0.1145
- Accuracy: 74.88%
- F1 Score: 0.7454

Epoch 28
-------------------------------
Loss: 0.0027  [0/3200]
Loss: 0.0071  [1600/3200]

Validation Error:
- Loss: 0.1058
- Accuracy: 76.50%
- F1 Score: 0.7613

Epoch 29
-------------------------------
Loss: 0.0012  [0/3200]
Loss: 0.0008  [1600/3200]

Validation Error:
- Loss: 0.1192
- Accuracy: 75.88%
- F1 Score: 0.7536

Epoch 30
-------------------------------
Loss: 0.1701  [0/3200]
Loss: 0.0397  [1600/3200]

Validation Error:
- Loss: 0.0966
- Accuracy: 76.25%
- F1 Score: 0.7624

Model Structure:
---------------------
 CNN_pp(
  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (fc1): Linear(in_features=4096, out_features=1024, bias=True)
  (fc2): Linear(in_features=1024, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=32, bias=True)
  (fc4): Linear(in_features=32, out_features=4, bias=True)
  (activation): Mish()
)
Trainable parameters: 4,735,524
Adamax (
Parameter Group 0
    betas: (0.9, 0.999)
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Epoch 1
-------------------------------
Loss: 1.7966  [0/3200]

<ipython-input-36-0ca721a378ed>:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Activation': activation_name, 'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1},

Loss: 1.0830  [1600/3200]

Validation Error:
- Loss: 0.0508
- Accuracy: 68.75%
- F1 Score: 0.6902

Epoch 2
-------------------------------
Loss: 0.7942  [0/3200]
Loss: 0.8267  [1600/3200]

Validation Error:
- Loss: 0.0601
- Accuracy: 61.75%
- F1 Score: 0.5392

Epoch 3
-------------------------------
Loss: 0.8235  [0/3200]
Loss: 0.5221  [1600/3200]

Validation Error:
- Loss: 0.0392
- Accuracy: 74.25%
- F1 Score: 0.7409

Epoch 4
-------------------------------
Loss: 0.6060  [0/3200]
Loss: 0.5591  [1600/3200]

Validation Error:
- Loss: 0.0371
- Accuracy: 77.50%
- F1 Score: 0.7741

Epoch 5
-------------------------------
Loss: 0.4149  [0/3200]
Loss: 0.9940  [1600/3200]

Validation Error:
- Loss: 0.0412
- Accuracy: 74.38%
- F1 Score: 0.7469

Epoch 6
-------------------------------
Loss: 0.4629  [0/3200]
Loss: 0.2017  [1600/3200]

Validation Error:
- Loss: 0.0464
- Accuracy: 73.38%
- F1 Score: 0.7224

Epoch 7
-------------------------------
Loss: 0.1756  [0/3200]
Loss: 0.3165  [1600/3200]

Validation Error:
- Loss: 0.0413
- Accuracy: 76.75%
- F1 Score: 0.7634

Epoch 8
-------------------------------
Loss: 0.6365  [0/3200]
Loss: 0.3122  [1600/3200]

Validation Error:
- Loss: 0.0363
- Accuracy: 78.50%
- F1 Score: 0.7850

Epoch 9
-------------------------------
Loss: 0.0850  [0/3200]
Loss: 0.2018  [1600/3200]

Validation Error:
- Loss: 0.0391
- Accuracy: 77.88%
- F1 Score: 0.7765

Epoch 10
-------------------------------
Loss: 0.3474  [0/3200]
Loss: 0.1738  [1600/3200]

Validation Error:
- Loss: 0.0451
- Accuracy: 77.62%
- F1 Score: 0.7736

Epoch 11
-------------------------------
Loss: 0.0761  [0/3200]
Loss: 0.0905  [1600/3200]

Validation Error:
- Loss: 0.0423
- Accuracy: 80.25%
- F1 Score: 0.7997

Epoch 12
-------------------------------
Loss: 0.1090  [0/3200]
Loss: 0.0151  [1600/3200]

Validation Error:
- Loss: 0.0594
- Accuracy: 76.12%
- F1 Score: 0.7563

Epoch 13
-------------------------------
Loss: 0.0357  [0/3200]
Loss: 0.0228  [1600/3200]

Validation Error:
- Loss: 0.0614
- Accuracy: 76.25%
- F1 Score: 0.7617

Epoch 14
-------------------------------
Loss: 0.0113  [0/3200]
Loss: 0.0018  [1600/3200]

Validation Error:
- Loss: 0.0671
- Accuracy: 76.50%
- F1 Score: 0.7614

Epoch 15
-------------------------------
Loss: 0.0719  [0/3200]
Loss: 0.0692  [1600/3200]

Validation Error:
- Loss: 0.0767
- Accuracy: 74.25%
- F1 Score: 0.7470

Epoch 16
-------------------------------
Loss: 0.0017  [0/3200]
Loss: 0.0343  [1600/3200]

Validation Error:
- Loss: 0.0808
- Accuracy: 75.12%
- F1 Score: 0.7547

Epoch 17
-------------------------------
Loss: 0.0493  [0/3200]
Loss: 0.0049  [1600/3200]

Validation Error:
- Loss: 0.0834
- Accuracy: 76.00%
- F1 Score: 0.7587

Epoch 18
-------------------------------
Loss: 0.0108  [0/3200]
Loss: 0.0013  [1600/3200]

Validation Error:
- Loss: 0.0731
- Accuracy: 78.88%
- F1 Score: 0.7883

Epoch 19
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0021  [1600/3200]

Validation Error:
- Loss: 0.0873
- Accuracy: 77.75%
- F1 Score: 0.7797

Epoch 20
-------------------------------
Loss: 0.0104  [0/3200]
Loss: 0.0017  [1600/3200]

Validation Error:
- Loss: 0.0842
- Accuracy: 75.75%
- F1 Score: 0.7550

Epoch 21
-------------------------------
Loss: 0.0961  [0/3200]
Loss: 0.0002  [1600/3200]

Validation Error:
- Loss: 0.0817
- Accuracy: 77.38%
- F1 Score: 0.7724

Epoch 22
-------------------------------
Loss: 0.0003  [0/3200]
Loss: 0.0003  [1600/3200]

Validation Error:
- Loss: 0.0744
- Accuracy: 75.75%
- F1 Score: 0.7574

Epoch 23
-------------------------------
Loss: 0.0181  [0/3200]
Loss: 0.0006  [1600/3200]

Validation Error:
- Loss: 0.0826
- Accuracy: 78.25%
- F1 Score: 0.7815

Epoch 24
-------------------------------
Loss: 0.0008  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0840
- Accuracy: 78.50%
- F1 Score: 0.7845

Epoch 25
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0002  [1600/3200]

Validation Error:
- Loss: 0.0874
- Accuracy: 78.12%
- F1 Score: 0.7809

Epoch 26
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0004  [1600/3200]

Validation Error:
- Loss: 0.0893
- Accuracy: 78.88%
- F1 Score: 0.7884

Epoch 27
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0926
- Accuracy: 78.00%
- F1 Score: 0.7797

Epoch 28
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.0936
- Accuracy: 78.75%
- F1 Score: 0.7870

Epoch 29
-------------------------------
Loss: 0.0002  [0/3200]
Loss: 0.0000  [1600/3200]

Validation Error:
- Loss: 0.0955
- Accuracy: 78.38%
- F1 Score: 0.7832

Epoch 30
-------------------------------
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.0973
- Accuracy: 78.50%
- F1 Score: 0.7848

Model Structure:
---------------------
 CNN_pp(
  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (fc1): Linear(in_features=4096, out_features=1024, bias=True)
  (fc2): Linear(in_features=1024, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=32, bias=True)
  (fc4): Linear(in_features=32, out_features=4, bias=True)
  (activation): PReLU(num_parameters=1)
)
Trainable parameters: 4,735,525
Adamax (
Parameter Group 0
    betas: (0.9, 0.999)
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Epoch 1
-------------------------------
Loss: 2.4804  [0/3200]

<ipython-input-36-0ca721a378ed>:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Activation': activation_name, 'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1},

Loss: 1.2475  [1600/3200]

Validation Error:
- Loss: 0.0578
- Accuracy: 59.38%
- F1 Score: 0.5439

Epoch 2
-------------------------------
Loss: 0.9295  [0/3200]
Loss: 0.6773  [1600/3200]

Validation Error:
- Loss: 0.0573
- Accuracy: 56.38%
- F1 Score: 0.5124

Epoch 3
-------------------------------
Loss: 0.7258  [0/3200]
Loss: 0.5259  [1600/3200]

Validation Error:
- Loss: 0.0478
- Accuracy: 66.88%
- F1 Score: 0.6643

Epoch 4
-------------------------------
Loss: 0.7762  [0/3200]
Loss: 0.4876  [1600/3200]

Validation Error:
- Loss: 0.0409
- Accuracy: 73.25%
- F1 Score: 0.7308

Epoch 5
-------------------------------
Loss: 0.4174  [0/3200]
Loss: 1.1014  [1600/3200]

Validation Error:
- Loss: 0.0387
- Accuracy: 73.75%
- F1 Score: 0.7372

Epoch 6
-------------------------------
Loss: 0.3934  [0/3200]
Loss: 0.1522  [1600/3200]

Validation Error:
- Loss: 0.0488
- Accuracy: 70.12%
- F1 Score: 0.6724

Epoch 7
-------------------------------
Loss: 0.2369  [0/3200]
Loss: 0.3219  [1600/3200]

Validation Error:
- Loss: 0.0396
- Accuracy: 76.38%
- F1 Score: 0.7596

Epoch 8
-------------------------------
Loss: 0.2214  [0/3200]
Loss: 0.1503  [1600/3200]

Validation Error:
- Loss: 0.0507
- Accuracy: 71.75%
- F1 Score: 0.7195

Epoch 9
-------------------------------
Loss: 0.1149  [0/3200]
Loss: 0.1139  [1600/3200]

Validation Error:
- Loss: 0.0512
- Accuracy: 76.62%
- F1 Score: 0.7647

Epoch 10
-------------------------------
Loss: 0.0452  [0/3200]
Loss: 0.2908  [1600/3200]

Validation Error:
- Loss: 0.0649
- Accuracy: 71.12%
- F1 Score: 0.6950

Epoch 11
-------------------------------
Loss: 0.0336  [0/3200]
Loss: 0.4016  [1600/3200]

Validation Error:
- Loss: 0.0609
- Accuracy: 74.50%
- F1 Score: 0.7422

Epoch 12
-------------------------------
Loss: 0.0653  [0/3200]
Loss: 0.0013  [1600/3200]

Validation Error:
- Loss: 0.0629
- Accuracy: 74.75%
- F1 Score: 0.7440

Epoch 13
-------------------------------
Loss: 0.0069  [0/3200]
Loss: 0.0197  [1600/3200]

Validation Error:
- Loss: 0.0899
- Accuracy: 75.75%
- F1 Score: 0.7582

Epoch 14
-------------------------------
Loss: 0.0045  [0/3200]
Loss: 0.0316  [1600/3200]

Validation Error:
- Loss: 0.0768
- Accuracy: 76.75%
- F1 Score: 0.7690

Epoch 15
-------------------------------
Loss: 0.0013  [0/3200]
Loss: 0.0081  [1600/3200]

Validation Error:
- Loss: 0.0965
- Accuracy: 71.00%
- F1 Score: 0.7076

Epoch 16
-------------------------------
Loss: 0.0418  [0/3200]
Loss: 0.0034  [1600/3200]

Validation Error:
- Loss: 0.0981
- Accuracy: 72.75%
- F1 Score: 0.7308

Epoch 17
-------------------------------
Loss: 0.0091  [0/3200]
Loss: 0.0033  [1600/3200]

Validation Error:
- Loss: 0.1008
- Accuracy: 72.25%
- F1 Score: 0.7242

Epoch 18
-------------------------------
Loss: 0.0058  [0/3200]
Loss: 0.0642  [1600/3200]

Validation Error:
- Loss: 0.0941
- Accuracy: 73.12%
- F1 Score: 0.7343

Epoch 19
-------------------------------
Loss: 0.0020  [0/3200]
Loss: 0.0054  [1600/3200]

Validation Error:
- Loss: 0.0870
- Accuracy: 75.62%
- F1 Score: 0.7552

Epoch 20
-------------------------------
Loss: 0.0030  [0/3200]
Loss: 0.2333  [1600/3200]

Validation Error:
- Loss: 0.0941
- Accuracy: 73.00%
- F1 Score: 0.7303

Epoch 21
-------------------------------
Loss: 0.0023  [0/3200]
Loss: 0.2052  [1600/3200]

Validation Error:
- Loss: 0.0992
- Accuracy: 72.88%
- F1 Score: 0.7312

Epoch 22
-------------------------------
Loss: 0.0205  [0/3200]
Loss: 0.0018  [1600/3200]

Validation Error:
- Loss: 0.1185
- Accuracy: 70.12%
- F1 Score: 0.7004

Epoch 23
-------------------------------
Loss: 0.4958  [0/3200]
Loss: 0.0014  [1600/3200]

Validation Error:
- Loss: 0.1066
- Accuracy: 71.38%
- F1 Score: 0.7041

Epoch 24
-------------------------------
Loss: 0.0045  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.1156
- Accuracy: 71.38%
- F1 Score: 0.7073

Epoch 25
-------------------------------
Loss: 0.0008  [0/3200]
Loss: 0.0780  [1600/3200]

Validation Error:
- Loss: 0.1073
- Accuracy: 74.75%
- F1 Score: 0.7466

Epoch 26
-------------------------------
Loss: 0.0000  [0/3200]
Loss: 0.0035  [1600/3200]

Validation Error:
- Loss: 0.1140
- Accuracy: 71.25%
- F1 Score: 0.7066

Epoch 27
-------------------------------
Loss: 0.0009  [0/3200]
Loss: 0.0801  [1600/3200]

Validation Error:
- Loss: 0.1081
- Accuracy: 73.75%
- F1 Score: 0.7335

Epoch 28
-------------------------------
Loss: 0.0086  [0/3200]
Loss: 0.0026  [1600/3200]

Validation Error:
- Loss: 0.0944
- Accuracy: 74.25%
- F1 Score: 0.7399

Epoch 29
-------------------------------
Loss: 0.0008  [0/3200]
Loss: 0.0006  [1600/3200]

Validation Error:
- Loss: 0.0903
- Accuracy: 75.50%
- F1 Score: 0.7535

Epoch 30
-------------------------------
Loss: 0.0054  [0/3200]
Loss: 0.0001  [1600/3200]

Validation Error:
- Loss: 0.1058
- Accuracy: 74.12%
- F1 Score: 0.7352


<ipython-input-36-0ca721a378ed>:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Activation': activation_name, 'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1},

results_df


Βήμα 4: Learning rate scheduler

Το learning rate είναι μία κρίσιμη μεταβλητή του αλγόριθμου βελτιστοποίησης η οποία επηρεάζει άμεσα το στιγμιότυπο στο οποίο τελικά θα καταλήξει το μοντέλο μας. Μέχρι στιγμής έχουμε χρησιμοποιήσει ένα στατικό learning rate. Παρ’ όλα αυτά το learning rate που έχουμε επιλέξει δεν είναι η καλύτερη επιλογή για όλα τα βήματα εκτέλεσης, αφού πιθανόν να είναι μικρό για τα αρχικά βήματα στα οποία επιθυμούμε μεγαλύτερες αναπροσαρμογές των βαρών, και μεγάλο για τα τελικά βήματα στα οποία έχουμε προσεγγίσει ένα καλό στιγμιότυπο του μοντέλου και δεν θέλουμε να απομακρυνθούμε αρκετά από αυτό.

Υπάρχει η δυνατότητα να χρησιμοποιήσουμε learning rate schedulers οι οποίοι αναπροσαρμόζουν δυναμικά το learning rate ανάλογα με το ιστορικό των losses. Εισάγετε στην διαδικασία εκπαίδευσής σας διαφορετικούς schedulers (από εδώ), δίνοντας στους schedulers σαν όρισμα verbose=True ώστε να έχετε εποπτεία των δυναμικών αλλαγών που γίνονται. Τι αποδόσεις πετυχαίνετε;

from sklearn.metrics import f1_score
import torch.optim.lr_scheduler as lr_scheduler

# Define the list of learning rates and corresponding scheduler names
lr = 2e-3
scheduler_names = ['CosineAnnealingLR', 'OneCycleLR', 'LambdaLR', 'PolynomialLR', 'MultiplicativeLR']

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=['Scheduler', 'Learning Rate', 'Best Accuracy', 'Best F1 Score'])

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Iterate over the scheduler names
for scheduler_name in scheduler_names:
    # Set the random seeds for reproducibility inside the loop
    torch.manual_seed(SEED)
    random.seed(SEED)
    np.random.seed(SEED)
    torch.cuda.manual_seed_all(SEED)

    # Initialize the model with Mish activation
    model = CNN_pp(output_dim=4, activation=nn.Mish()).to(device)
    model.apply(weights_init)  # Initialize the weights

    # Create the optimizer with Adamax
    optimizer = optim.Adamax(model.parameters(), lr=lr)

    # Create the scheduler based on the scheduler name
    if scheduler_name == 'CosineAnnealingLR':
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=lr, verbose=True)
    elif scheduler_name == 'OneCycleLR':
        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=30, steps_per_epoch=len(train_loader), verbose=True)
    elif scheduler_name == 'LambdaLR':
        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch, verbose=True)
    elif scheduler_name == 'PolynomialLR':
        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9, verbose=True)
    elif scheduler_name == 'MultiplicativeLR':
        scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.9, verbose=True)

    # Train the model
    print(f"Training with {scheduler_name} and learning rate {lr}...")
    best_accuracy = 0.0
    best_f1 = 0.0
    for epoch in range(30):
        print(f"Epoch {epoch+1}/{30}")
        train_loop(train_loader, model, loss_fn, optimizer)
        test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
        print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

        # Update the best accuracy and F1 score
        if accuracy > best_accuracy:
            best_accuracy = accuracy
        if f1 > best_f1:
            best_f1 = f1

        scheduler.step()  # Update the learning rate

    # Add the results to the DataFrame
    results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,
                                    'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1}, ignore_index=True)

Adjusting learning rate of group 0 to 2.0000e-03.
Training with CosineAnnealingLR and learning rate 0.002...
Epoch 1/30
Loss: 1.8130  [0/3200]
Loss: 1.0179  [1600/3200]
Validation Loss: 0.0557, Accuracy: 65.00%, F1 Score: 0.6353
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 2/30
Loss: 0.8962  [0/3200]
Loss: 0.7813  [1600/3200]
Validation Loss: 0.0485, Accuracy: 64.00%, F1 Score: 0.6225
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 3/30
Loss: 0.6768  [0/3200]
Loss: 0.4570  [1600/3200]
Validation Loss: 0.0374, Accuracy: 75.75%, F1 Score: 0.7553
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 4/30
Loss: 0.3430  [0/3200]
Loss: 0.3478  [1600/3200]
Validation Loss: 0.0405, Accuracy: 75.62%, F1 Score: 0.7450
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 5/30
Loss: 0.4716  [0/3200]
Loss: 0.5937  [1600/3200]
Validation Loss: 0.0342, Accuracy: 78.00%, F1 Score: 0.7836
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 6/30
Loss: 0.6305  [0/3200]
Loss: 0.3271  [1600/3200]
Validation Loss: 0.0432, Accuracy: 75.00%, F1 Score: 0.7378
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 7/30
Loss: 0.3963  [0/3200]
Loss: 0.4495  [1600/3200]
Validation Loss: 0.0377, Accuracy: 77.25%, F1 Score: 0.7641
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 8/30
Loss: 0.2799  [0/3200]
Loss: 0.3085  [1600/3200]
Validation Loss: 0.0395, Accuracy: 76.62%, F1 Score: 0.7598
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 9/30
Loss: 0.3736  [0/3200]
Loss: 0.2404  [1600/3200]
Validation Loss: 0.0411, Accuracy: 76.38%, F1 Score: 0.7568
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 10/30
Loss: 0.2664  [0/3200]
Loss: 0.1837  [1600/3200]
Validation Loss: 0.0414, Accuracy: 78.88%, F1 Score: 0.7906
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 11/30
Loss: 0.2292  [0/3200]
Loss: 0.0488  [1600/3200]
Validation Loss: 0.0679, Accuracy: 71.25%, F1 Score: 0.7089
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 12/30
Loss: 0.1257  [0/3200]
Loss: 0.1383  [1600/3200]
Validation Loss: 0.0572, Accuracy: 76.00%, F1 Score: 0.7462
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 13/30
Loss: 0.0381  [0/3200]
Loss: 0.1475  [1600/3200]
Validation Loss: 0.0563, Accuracy: 76.12%, F1 Score: 0.7593
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 14/30
Loss: 0.0210  [0/3200]
Loss: 0.1179  [1600/3200]
Validation Loss: 0.0617, Accuracy: 73.62%, F1 Score: 0.7243
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 15/30
Loss: 0.0223  [0/3200]
Loss: 0.0165  [1600/3200]
Validation Loss: 0.0631, Accuracy: 77.38%, F1 Score: 0.7759
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 16/30
Loss: 0.0830  [0/3200]
Loss: 0.0136  [1600/3200]
Validation Loss: 0.0763, Accuracy: 76.25%, F1 Score: 0.7553
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 17/30
Loss: 0.0879  [0/3200]
Loss: 0.0427  [1600/3200]
Validation Loss: 0.0763, Accuracy: 76.62%, F1 Score: 0.7621
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 18/30
Loss: 0.0028  [0/3200]
Loss: 0.0087  [1600/3200]
Validation Loss: 0.0829, Accuracy: 75.25%, F1 Score: 0.7460
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 19/30
Loss: 0.0119  [0/3200]
Loss: 0.0359  [1600/3200]
Validation Loss: 0.0670, Accuracy: 77.00%, F1 Score: 0.7676
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 20/30
Loss: 0.0006  [0/3200]
Loss: 0.0033  [1600/3200]
Validation Loss: 0.0894, Accuracy: 77.50%, F1 Score: 0.7733
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 21/30
Loss: 0.0093  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0886, Accuracy: 76.25%, F1 Score: 0.7570
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 22/30
Loss: 0.0067  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0926, Accuracy: 75.38%, F1 Score: 0.7539
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 23/30
Loss: 0.0078  [0/3200]
Loss: 0.0077  [1600/3200]
Validation Loss: 0.0727, Accuracy: 77.00%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 24/30
Loss: 0.0269  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.1159, Accuracy: 74.00%, F1 Score: 0.7409
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 25/30
Loss: 0.0004  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.1051, Accuracy: 75.75%, F1 Score: 0.7540
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 26/30
Loss: 0.0002  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.1021, Accuracy: 76.38%, F1 Score: 0.7662
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 27/30
Loss: 0.0000  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0979, Accuracy: 73.88%, F1 Score: 0.7366
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 28/30
Loss: 0.0013  [0/3200]
Loss: 0.0075  [1600/3200]
Validation Loss: 0.1035, Accuracy: 74.50%, F1 Score: 0.7500
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 29/30
Loss: 0.0037  [0/3200]
Loss: 0.0023  [1600/3200]
Validation Loss: 0.1030, Accuracy: 74.88%, F1 Score: 0.7441
Adjusting learning rate of group 0 to 2.0000e-03.
Epoch 30/30
Loss: 0.0006  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0874, Accuracy: 76.25%, F1 Score: 0.7559
Adjusting learning rate of group 0 to 2.0000e-03.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with OneCycleLR and learning rate 0.002...
Epoch 1/30
Loss: 1.8130  [0/3200]

<ipython-input-37-54628fe8881b>:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,

Loss: 1.0082  [1600/3200]
Validation Loss: 0.0518, Accuracy: 64.12%, F1 Score: 0.6167
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/30
Loss: 0.8653  [0/3200]
Loss: 0.8663  [1600/3200]
Validation Loss: 0.0503, Accuracy: 64.62%, F1 Score: 0.6125
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/30
Loss: 0.5948  [0/3200]
Loss: 0.5597  [1600/3200]
Validation Loss: 0.0416, Accuracy: 72.38%, F1 Score: 0.7242
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/30
Loss: 0.5668  [0/3200]
Loss: 0.5498  [1600/3200]
Validation Loss: 0.0410, Accuracy: 75.25%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/30
Loss: 0.5309  [0/3200]
Loss: 0.6688  [1600/3200]
Validation Loss: 0.0378, Accuracy: 76.00%, F1 Score: 0.7489
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/30
Loss: 1.0985  [0/3200]
Loss: 0.4645  [1600/3200]
Validation Loss: 0.0377, Accuracy: 76.88%, F1 Score: 0.7669
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/30
Loss: 0.5276  [0/3200]
Loss: 0.5884  [1600/3200]
Validation Loss: 0.0364, Accuracy: 79.62%, F1 Score: 0.7930
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/30
Loss: 0.3510  [0/3200]
Loss: 0.5044  [1600/3200]
Validation Loss: 0.0362, Accuracy: 78.00%, F1 Score: 0.7828
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/30
Loss: 0.4413  [0/3200]
Loss: 0.4679  [1600/3200]
Validation Loss: 0.0340, Accuracy: 80.62%, F1 Score: 0.8038
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/30
Loss: 0.3946  [0/3200]
Loss: 0.4947  [1600/3200]
Validation Loss: 0.0414, Accuracy: 75.62%, F1 Score: 0.7606
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/30
Loss: 0.5328  [0/3200]
Loss: 0.1810  [1600/3200]
Validation Loss: 0.0371, Accuracy: 78.12%, F1 Score: 0.7759
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/30
Loss: 0.3081  [0/3200]
Loss: 0.1696  [1600/3200]
Validation Loss: 0.0383, Accuracy: 77.75%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/30
Loss: 0.3587  [0/3200]
Loss: 0.3402  [1600/3200]
Validation Loss: 0.0429, Accuracy: 75.00%, F1 Score: 0.7422
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/30
Loss: 0.5925  [0/3200]
Loss: 0.2783  [1600/3200]
Validation Loss: 0.0482, Accuracy: 70.25%, F1 Score: 0.7055
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/30
Loss: 0.5891  [0/3200]
Loss: 0.4305  [1600/3200]
Validation Loss: 0.0342, Accuracy: 80.00%, F1 Score: 0.8002
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/30
Loss: 0.3292  [0/3200]
Loss: 0.4663  [1600/3200]
Validation Loss: 0.0345, Accuracy: 80.75%, F1 Score: 0.8050
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/30
Loss: 0.2252  [0/3200]
Loss: 0.2396  [1600/3200]
Validation Loss: 0.0350, Accuracy: 80.38%, F1 Score: 0.8003
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/30
Loss: 0.3002  [0/3200]
Loss: 0.1919  [1600/3200]
Validation Loss: 0.0355, Accuracy: 80.12%, F1 Score: 0.8025
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/30
Loss: 0.2526  [0/3200]
Loss: 0.2975  [1600/3200]
Validation Loss: 0.0375, Accuracy: 80.75%, F1 Score: 0.8071
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/30
Loss: 0.3383  [0/3200]
Loss: 0.0821  [1600/3200]
Validation Loss: 0.0353, Accuracy: 80.88%, F1 Score: 0.8076
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/30
Loss: 0.1798  [0/3200]
Loss: 0.4491  [1600/3200]
Validation Loss: 0.0419, Accuracy: 76.25%, F1 Score: 0.7468
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/30
Loss: 0.3717  [0/3200]
Loss: 0.4203  [1600/3200]
Validation Loss: 0.0412, Accuracy: 78.12%, F1 Score: 0.7821
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/30
Loss: 0.3322  [0/3200]
Loss: 0.0484  [1600/3200]
Validation Loss: 0.0399, Accuracy: 79.62%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/30
Loss: 0.1613  [0/3200]
Loss: 0.3998  [1600/3200]
Validation Loss: 0.0398, Accuracy: 77.62%, F1 Score: 0.7728
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/30
Loss: 0.0968  [0/3200]
Loss: 0.2833  [1600/3200]
Validation Loss: 0.0406, Accuracy: 77.25%, F1 Score: 0.7701
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/30
Loss: 0.1591  [0/3200]
Loss: 0.0526  [1600/3200]
Validation Loss: 0.0473, Accuracy: 76.00%, F1 Score: 0.7524
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/30
Loss: 0.1805  [0/3200]
Loss: 0.2381  [1600/3200]
Validation Loss: 0.0475, Accuracy: 74.75%, F1 Score: 0.7427
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/30
Loss: 0.1600  [0/3200]
Loss: 0.1003  [1600/3200]
Validation Loss: 0.0453, Accuracy: 75.75%, F1 Score: 0.7547
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/30
Loss: 0.1322  [0/3200]
Loss: 0.0817  [1600/3200]
Validation Loss: 0.0555, Accuracy: 75.12%, F1 Score: 0.7362
Adjusting learning rate of group 0 to 8.1231e-05.
Epoch 30/30
Loss: 0.0911  [0/3200]
Loss: 0.0679  [1600/3200]
Validation Loss: 0.0443, Accuracy: 77.88%, F1 Score: 0.7758
Adjusting learning rate of group 0 to 8.1317e-05.
Adjusting learning rate of group 0 to 2.0000e-03.
Training with LambdaLR and learning rate 0.002...
Epoch 1/30
Loss: 1.8130  [0/3200]

<ipython-input-37-54628fe8881b>:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,

Loss: 1.0179  [1600/3200]
Validation Loss: 0.0557, Accuracy: 65.00%, F1 Score: 0.6353
Adjusting learning rate of group 0 to 1.9000e-03.
Epoch 2/30
Loss: 0.8962  [0/3200]
Loss: 0.7639  [1600/3200]
Validation Loss: 0.0444, Accuracy: 69.25%, F1 Score: 0.6728
Adjusting learning rate of group 0 to 1.8050e-03.
Epoch 3/30
Loss: 0.5973  [0/3200]
Loss: 0.5990  [1600/3200]
Validation Loss: 0.0345, Accuracy: 78.62%, F1 Score: 0.7885
Adjusting learning rate of group 0 to 1.7147e-03.
Epoch 4/30
Loss: 0.2703  [0/3200]
Loss: 0.4569  [1600/3200]
Validation Loss: 0.0416, Accuracy: 75.75%, F1 Score: 0.7485
Adjusting learning rate of group 0 to 1.6290e-03.
Epoch 5/30
Loss: 0.5799  [0/3200]
Loss: 0.7134  [1600/3200]
Validation Loss: 0.0366, Accuracy: 77.00%, F1 Score: 0.7672
Adjusting learning rate of group 0 to 1.5476e-03.
Epoch 6/30
Loss: 0.8133  [0/3200]
Loss: 0.3273  [1600/3200]
Validation Loss: 0.0370, Accuracy: 76.88%, F1 Score: 0.7589
Adjusting learning rate of group 0 to 1.4702e-03.
Epoch 7/30
Loss: 0.3578  [0/3200]
Loss: 0.3514  [1600/3200]
Validation Loss: 0.0361, Accuracy: 77.62%, F1 Score: 0.7714
Adjusting learning rate of group 0 to 1.3967e-03.
Epoch 8/30
Loss: 0.1966  [0/3200]
Loss: 0.3226  [1600/3200]
Validation Loss: 0.0378, Accuracy: 77.00%, F1 Score: 0.7625
Adjusting learning rate of group 0 to 1.3268e-03.
Epoch 9/30
Loss: 0.4329  [0/3200]
Loss: 0.2031  [1600/3200]
Validation Loss: 0.0394, Accuracy: 77.75%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 1.2605e-03.
Epoch 10/30
Loss: 0.2724  [0/3200]
Loss: 0.2382  [1600/3200]
Validation Loss: 0.0403, Accuracy: 78.00%, F1 Score: 0.7807
Adjusting learning rate of group 0 to 1.1975e-03.
Epoch 11/30
Loss: 0.2352  [0/3200]
Loss: 0.0480  [1600/3200]
Validation Loss: 0.0471, Accuracy: 76.00%, F1 Score: 0.7557
Adjusting learning rate of group 0 to 1.1376e-03.
Epoch 12/30
Loss: 0.0762  [0/3200]
Loss: 0.0734  [1600/3200]
Validation Loss: 0.0584, Accuracy: 75.88%, F1 Score: 0.7588
Adjusting learning rate of group 0 to 1.0807e-03.
Epoch 13/30
Loss: 0.0063  [0/3200]
Loss: 0.0367  [1600/3200]
Validation Loss: 0.0623, Accuracy: 75.75%, F1 Score: 0.7496
Adjusting learning rate of group 0 to 1.0267e-03.
Epoch 14/30
Loss: 0.0277  [0/3200]
Loss: 0.1212  [1600/3200]
Validation Loss: 0.0612, Accuracy: 74.38%, F1 Score: 0.7464
Adjusting learning rate of group 0 to 9.7535e-04.
Epoch 15/30
Loss: 0.1218  [0/3200]
Loss: 0.0204  [1600/3200]
Validation Loss: 0.0652, Accuracy: 77.75%, F1 Score: 0.7768
Adjusting learning rate of group 0 to 9.2658e-04.
Epoch 16/30
Loss: 0.0373  [0/3200]
Loss: 0.0047  [1600/3200]
Validation Loss: 0.0791, Accuracy: 75.50%, F1 Score: 0.7527
Adjusting learning rate of group 0 to 8.8025e-04.
Epoch 17/30
Loss: 0.0047  [0/3200]
Loss: 0.0035  [1600/3200]
Validation Loss: 0.0777, Accuracy: 77.00%, F1 Score: 0.7695
Adjusting learning rate of group 0 to 8.3624e-04.
Epoch 18/30
Loss: 0.0198  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0813, Accuracy: 76.00%, F1 Score: 0.7591
Adjusting learning rate of group 0 to 7.9443e-04.
Epoch 19/30
Loss: 0.0016  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0946, Accuracy: 75.25%, F1 Score: 0.7525
Adjusting learning rate of group 0 to 7.5471e-04.
Epoch 20/30
Loss: 0.0006  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0887, Accuracy: 77.38%, F1 Score: 0.7710
Adjusting learning rate of group 0 to 7.1697e-04.
Epoch 21/30
Loss: 0.0002  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0918, Accuracy: 77.38%, F1 Score: 0.7711
Adjusting learning rate of group 0 to 6.8112e-04.
Epoch 22/30
Loss: 0.0002  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0933, Accuracy: 77.00%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 6.4707e-04.
Epoch 23/30
Loss: 0.0003  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0951, Accuracy: 77.50%, F1 Score: 0.7722
Adjusting learning rate of group 0 to 6.1471e-04.
Epoch 24/30
Loss: 0.0005  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0960, Accuracy: 77.88%, F1 Score: 0.7757
Adjusting learning rate of group 0 to 5.8398e-04.
Epoch 25/30
Loss: 0.0003  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0995, Accuracy: 77.25%, F1 Score: 0.7696
Adjusting learning rate of group 0 to 5.5478e-04.
Epoch 26/30
Loss: 0.0002  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.1007, Accuracy: 77.38%, F1 Score: 0.7710
Adjusting learning rate of group 0 to 5.2704e-04.
Epoch 27/30
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.1023, Accuracy: 77.75%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 5.0069e-04.
Epoch 28/30
Loss: 0.0000  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.1037, Accuracy: 77.38%, F1 Score: 0.7705
Adjusting learning rate of group 0 to 4.7565e-04.
Epoch 29/30
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.1060, Accuracy: 77.12%, F1 Score: 0.7681
Adjusting learning rate of group 0 to 4.5187e-04.
Epoch 30/30
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.1080, Accuracy: 77.38%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 4.2928e-04.
Adjusting learning rate of group 0 to 2.0000e-03.
Training with PolynomialLR and learning rate 0.002...
Epoch 1/30
Loss: 1.8130  [0/3200]

<ipython-input-37-54628fe8881b>:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,

Loss: 1.0179  [1600/3200]
Validation Loss: 0.0557, Accuracy: 65.00%, F1 Score: 0.6353
Adjusting learning rate of group 0 to 1.8000e-03.
Epoch 2/30
Loss: 0.8962  [0/3200]
Loss: 0.7957  [1600/3200]
Validation Loss: 0.0428, Accuracy: 71.50%, F1 Score: 0.7022
Adjusting learning rate of group 0 to 1.6200e-03.
Epoch 3/30
Loss: 0.6122  [0/3200]
Loss: 0.6183  [1600/3200]
Validation Loss: 0.0350, Accuracy: 79.00%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 1.4580e-03.
Epoch 4/30
Loss: 0.2775  [0/3200]
Loss: 0.4516  [1600/3200]
Validation Loss: 0.0408, Accuracy: 73.88%, F1 Score: 0.7219
Adjusting learning rate of group 0 to 1.3122e-03.
Epoch 5/30
Loss: 0.5173  [0/3200]
Loss: 0.6605  [1600/3200]
Validation Loss: 0.0359, Accuracy: 76.62%, F1 Score: 0.7638
Adjusting learning rate of group 0 to 1.1810e-03.
Epoch 6/30
Loss: 0.8422  [0/3200]
Loss: 0.3457  [1600/3200]
Validation Loss: 0.0350, Accuracy: 77.75%, F1 Score: 0.7727
Adjusting learning rate of group 0 to 1.0629e-03.
Epoch 7/30
Loss: 0.3696  [0/3200]
Loss: 0.4020  [1600/3200]
Validation Loss: 0.0406, Accuracy: 75.62%, F1 Score: 0.7476
Adjusting learning rate of group 0 to 9.5659e-04.
Epoch 8/30
Loss: 0.2643  [0/3200]
Loss: 0.2781  [1600/3200]
Validation Loss: 0.0385, Accuracy: 77.50%, F1 Score: 0.7686
Adjusting learning rate of group 0 to 8.6093e-04.
Epoch 9/30
Loss: 0.4522  [0/3200]
Loss: 0.2178  [1600/3200]
Validation Loss: 0.0380, Accuracy: 77.50%, F1 Score: 0.7712
Adjusting learning rate of group 0 to 7.7484e-04.
Epoch 10/30
Loss: 0.2680  [0/3200]
Loss: 0.2283  [1600/3200]
Validation Loss: 0.0408, Accuracy: 77.88%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 6.9736e-04.
Epoch 11/30
Loss: 0.2435  [0/3200]
Loss: 0.0242  [1600/3200]
Validation Loss: 0.0431, Accuracy: 77.00%, F1 Score: 0.7643
Adjusting learning rate of group 0 to 6.2762e-04.
Epoch 12/30
Loss: 0.1136  [0/3200]
Loss: 0.1008  [1600/3200]
Validation Loss: 0.0475, Accuracy: 78.50%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 5.6486e-04.
Epoch 13/30
Loss: 0.0204  [0/3200]
Loss: 0.0718  [1600/3200]
Validation Loss: 0.0501, Accuracy: 78.25%, F1 Score: 0.7831
Adjusting learning rate of group 0 to 5.0837e-04.
Epoch 14/30
Loss: 0.0306  [0/3200]
Loss: 0.0990  [1600/3200]
Validation Loss: 0.0551, Accuracy: 76.88%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 4.5754e-04.
Epoch 15/30
Loss: 0.0438  [0/3200]
Loss: 0.0397  [1600/3200]
Validation Loss: 0.0700, Accuracy: 74.88%, F1 Score: 0.7520
Adjusting learning rate of group 0 to 4.1178e-04.
Epoch 16/30
Loss: 0.1155  [0/3200]
Loss: 0.0128  [1600/3200]
Validation Loss: 0.0625, Accuracy: 76.38%, F1 Score: 0.7576
Adjusting learning rate of group 0 to 3.7060e-04.
Epoch 17/30
Loss: 0.0327  [0/3200]
Loss: 0.0136  [1600/3200]
Validation Loss: 0.0699, Accuracy: 75.38%, F1 Score: 0.7510
Adjusting learning rate of group 0 to 3.3354e-04.
Epoch 18/30
Loss: 0.0112  [0/3200]
Loss: 0.0064  [1600/3200]
Validation Loss: 0.0668, Accuracy: 76.38%, F1 Score: 0.7605
Adjusting learning rate of group 0 to 3.0019e-04.
Epoch 19/30
Loss: 0.0081  [0/3200]
Loss: 0.0073  [1600/3200]
Validation Loss: 0.0728, Accuracy: 77.25%, F1 Score: 0.7723
Adjusting learning rate of group 0 to 2.7017e-04.
Epoch 20/30
Loss: 0.0033  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0765, Accuracy: 77.25%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 2.4315e-04.
Epoch 21/30
Loss: 0.0030  [0/3200]
Loss: 0.0040  [1600/3200]
Validation Loss: 0.0806, Accuracy: 75.88%, F1 Score: 0.7563
Adjusting learning rate of group 0 to 2.1884e-04.
Epoch 22/30
Loss: 0.0059  [0/3200]
Loss: 0.0087  [1600/3200]
Validation Loss: 0.0817, Accuracy: 76.88%, F1 Score: 0.7681
Adjusting learning rate of group 0 to 1.9695e-04.
Epoch 23/30
Loss: 0.0023  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0838, Accuracy: 76.88%, F1 Score: 0.7671
Adjusting learning rate of group 0 to 1.7726e-04.
Epoch 24/30
Loss: 0.0013  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0844, Accuracy: 77.38%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 1.5953e-04.
Epoch 25/30
Loss: 0.0020  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0868, Accuracy: 77.25%, F1 Score: 0.7721
Adjusting learning rate of group 0 to 1.4358e-04.
Epoch 26/30
Loss: 0.0010  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0906, Accuracy: 76.38%, F1 Score: 0.7635
Adjusting learning rate of group 0 to 1.2922e-04.
Epoch 27/30
Loss: 0.0004  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0912, Accuracy: 77.00%, F1 Score: 0.7687
Adjusting learning rate of group 0 to 1.1630e-04.
Epoch 28/30
Loss: 0.0002  [0/3200]
Loss: 0.0029  [1600/3200]
Validation Loss: 0.0922, Accuracy: 77.00%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 1.0467e-04.
Epoch 29/30
Loss: 0.0000  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0948, Accuracy: 76.88%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 9.4203e-05.
Epoch 30/30
Loss: 0.0002  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0958, Accuracy: 77.12%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.4782e-05.
Adjusting learning rate of group 0 to 2.0000e-03.
Training with MultiplicativeLR and learning rate 0.002...
Epoch 1/30
Loss: 1.8130  [0/3200]

<ipython-input-37-54628fe8881b>:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,

Loss: 1.0179  [1600/3200]
Validation Loss: 0.0557, Accuracy: 65.00%, F1 Score: 0.6353
Adjusting learning rate of group 0 to 1.8000e-03.
Epoch 2/30
Loss: 0.8962  [0/3200]
Loss: 0.7957  [1600/3200]
Validation Loss: 0.0428, Accuracy: 71.50%, F1 Score: 0.7022
Adjusting learning rate of group 0 to 1.6200e-03.
Epoch 3/30
Loss: 0.6122  [0/3200]
Loss: 0.6183  [1600/3200]
Validation Loss: 0.0350, Accuracy: 79.00%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 1.4580e-03.
Epoch 4/30
Loss: 0.2775  [0/3200]
Loss: 0.4516  [1600/3200]
Validation Loss: 0.0408, Accuracy: 73.88%, F1 Score: 0.7219
Adjusting learning rate of group 0 to 1.3122e-03.
Epoch 5/30
Loss: 0.5173  [0/3200]
Loss: 0.6605  [1600/3200]
Validation Loss: 0.0359, Accuracy: 76.62%, F1 Score: 0.7638
Adjusting learning rate of group 0 to 1.1810e-03.
Epoch 6/30
Loss: 0.8422  [0/3200]
Loss: 0.3457  [1600/3200]
Validation Loss: 0.0350, Accuracy: 77.75%, F1 Score: 0.7727
Adjusting learning rate of group 0 to 1.0629e-03.
Epoch 7/30
Loss: 0.3696  [0/3200]
Loss: 0.4020  [1600/3200]
Validation Loss: 0.0406, Accuracy: 75.62%, F1 Score: 0.7476
Adjusting learning rate of group 0 to 9.5659e-04.
Epoch 8/30
Loss: 0.2643  [0/3200]
Loss: 0.2781  [1600/3200]
Validation Loss: 0.0385, Accuracy: 77.50%, F1 Score: 0.7686
Adjusting learning rate of group 0 to 8.6093e-04.
Epoch 9/30
Loss: 0.4522  [0/3200]
Loss: 0.2178  [1600/3200]
Validation Loss: 0.0380, Accuracy: 77.50%, F1 Score: 0.7712
Adjusting learning rate of group 0 to 7.7484e-04.
Epoch 10/30
Loss: 0.2680  [0/3200]
Loss: 0.2283  [1600/3200]
Validation Loss: 0.0408, Accuracy: 77.88%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 6.9736e-04.
Epoch 11/30
Loss: 0.2435  [0/3200]
Loss: 0.0242  [1600/3200]
Validation Loss: 0.0431, Accuracy: 77.00%, F1 Score: 0.7643
Adjusting learning rate of group 0 to 6.2762e-04.
Epoch 12/30
Loss: 0.1136  [0/3200]
Loss: 0.1008  [1600/3200]
Validation Loss: 0.0475, Accuracy: 78.50%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 5.6486e-04.
Epoch 13/30
Loss: 0.0204  [0/3200]
Loss: 0.0718  [1600/3200]
Validation Loss: 0.0501, Accuracy: 78.25%, F1 Score: 0.7831
Adjusting learning rate of group 0 to 5.0837e-04.
Epoch 14/30
Loss: 0.0306  [0/3200]
Loss: 0.0990  [1600/3200]
Validation Loss: 0.0551, Accuracy: 76.88%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 4.5754e-04.
Epoch 15/30
Loss: 0.0438  [0/3200]
Loss: 0.0397  [1600/3200]
Validation Loss: 0.0700, Accuracy: 74.88%, F1 Score: 0.7520
Adjusting learning rate of group 0 to 4.1178e-04.
Epoch 16/30
Loss: 0.1155  [0/3200]
Loss: 0.0128  [1600/3200]
Validation Loss: 0.0625, Accuracy: 76.38%, F1 Score: 0.7576
Adjusting learning rate of group 0 to 3.7060e-04.
Epoch 17/30
Loss: 0.0327  [0/3200]
Loss: 0.0136  [1600/3200]
Validation Loss: 0.0699, Accuracy: 75.38%, F1 Score: 0.7510
Adjusting learning rate of group 0 to 3.3354e-04.
Epoch 18/30
Loss: 0.0112  [0/3200]
Loss: 0.0064  [1600/3200]
Validation Loss: 0.0668, Accuracy: 76.38%, F1 Score: 0.7605
Adjusting learning rate of group 0 to 3.0019e-04.
Epoch 19/30
Loss: 0.0081  [0/3200]
Loss: 0.0073  [1600/3200]
Validation Loss: 0.0728, Accuracy: 77.25%, F1 Score: 0.7723
Adjusting learning rate of group 0 to 2.7017e-04.
Epoch 20/30
Loss: 0.0033  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0765, Accuracy: 77.25%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 2.4315e-04.
Epoch 21/30
Loss: 0.0030  [0/3200]
Loss: 0.0040  [1600/3200]
Validation Loss: 0.0806, Accuracy: 75.88%, F1 Score: 0.7563
Adjusting learning rate of group 0 to 2.1884e-04.
Epoch 22/30
Loss: 0.0059  [0/3200]
Loss: 0.0087  [1600/3200]
Validation Loss: 0.0817, Accuracy: 76.88%, F1 Score: 0.7681
Adjusting learning rate of group 0 to 1.9695e-04.
Epoch 23/30
Loss: 0.0023  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0838, Accuracy: 76.88%, F1 Score: 0.7671
Adjusting learning rate of group 0 to 1.7726e-04.
Epoch 24/30
Loss: 0.0013  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0844, Accuracy: 77.38%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 1.5953e-04.
Epoch 25/30
Loss: 0.0020  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0868, Accuracy: 77.25%, F1 Score: 0.7721
Adjusting learning rate of group 0 to 1.4358e-04.
Epoch 26/30
Loss: 0.0010  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0906, Accuracy: 76.38%, F1 Score: 0.7635
Adjusting learning rate of group 0 to 1.2922e-04.
Epoch 27/30
Loss: 0.0004  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0912, Accuracy: 77.00%, F1 Score: 0.7687
Adjusting learning rate of group 0 to 1.1630e-04.
Epoch 28/30
Loss: 0.0002  [0/3200]
Loss: 0.0029  [1600/3200]
Validation Loss: 0.0922, Accuracy: 77.00%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 1.0467e-04.
Epoch 29/30
Loss: 0.0000  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0948, Accuracy: 76.88%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 9.4203e-05.
Epoch 30/30
Loss: 0.0002  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0958, Accuracy: 77.12%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.4782e-05.

<ipython-input-37-54628fe8881b>:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,

results_df

Παρατηρούμε μια αξιόλογη απόδοση στην ακρίβεια και στο F1 με OneCycleLR scheduler, ενώ οι υπόλποποι schedulers είναι σχετικά κοντά μεταξύ τις για τις καλύτερες αποδόσεις στην ακρίβεια και στην f1.

Βήμα 5: Batch Normalization

Τα νευρωνικά δίκτυα λειτουργούν καλύτερα εάν τα δεδομένα έχουν συγκεκριμένες στατιστικές ιδιότητες. Για να πετύχουμε αυτές τις ιδιότητες συνήθως πραγματοποιούμε normalization στο input. Παρ’ όλα αυτά καθώς εφαρμόζουμε περισσότερα επίπεδα του δικτύου και καθώς ενημερώνονται τα βάρη, ενδέχεται οι στατιστικές ιδιότητες του input κάθε layer να διαφέρουν από το ένα batch στο άλλο. Αυτό δεν βοηθά τον αλγόριθμο εκπαίδευσης αφού προσπαθεί να μάθει από δεδομένα των οποίων αλλάζει λίγο η κατανομή μεταξύ των batches. Μία λύση σε αυτό το πρόβλημα είναι να εφαρμόζουμε, σε κάθε layer, normalization σε όλα τα στοιχεία του batch ώστε να πετύχουμε ίδιες ιδιότητες.

Εισάγετε λοιπόν στην αρχιτεκτονική σας BatchNorm2d layers πριν από κάθε συνάρτηση ενεργοποίησης όλων των συνελικτικών επιπέδων.

lr = 2e-3
scheduler_name = 'OneCycleLR'

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=['Scheduler', 'Learning Rate', 'Best Accuracy', 'Best F1 Score'])

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define the model with Batch Normalization and Mish activation
class CNN_pp_bn(nn.Module):
    def __init__(self, output_dim):
        super(CNN_pp_bn, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm2d(16)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm2d(32)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)
        self.bn4 = nn.BatchNorm2d(128)

        # Fully connected (dense) layers
        self.fc1 = nn.Linear(4096, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 32)
        self.fc4 = nn.Linear(32, output_dim)

    def forward(self, x):
        # Convolutional layers
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.maxpool1(x)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.maxpool2(x)
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.maxpool3(x)
        x = torch.relu(self.bn4(self.conv4(x)))

        # Flatten the tensor for the fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)

        return x

# Initialize the model
model = CNN_pp_bn(output_dim=4).to(device)
model.apply(weights_init)  # Initialize the weights

# Create the optimizer with Adamax
optimizer = optim.Adamax(model.parameters(), lr=lr)

# Create the scheduler (OneCycleLR)
scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=30, steps_per_epoch=len(train_loader), verbose=True)

# Train the model
print(f"Training with {scheduler_name} and learning rate {lr}...")
best_accuracy = 0.0
best_f1 = 0.0
for epoch in range(30):
    print(f"Epoch {epoch+1}/{30}")
    train_loop(train_loader, model, loss_fn, optimizer)
    test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
    print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

    # Update the best accuracy and F1 score
    if accuracy > best_accuracy:
        best_accuracy = accuracy
    if f1 > best_f1:
        best_f1 = f1

    scheduler.step()  # Update the learning rate

# Add the results to the DataFrame
results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,
                                'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1}, ignore_index=True)
Adjusting learning rate of group 0 to 8.0000e-05.
Training with OneCycleLR and learning rate 0.002...
Epoch 1/30
Loss: 1.3065  [0/3200]
Loss: 0.8184  [1600/3200]
Validation Loss: 0.0426, Accuracy: 72.38%, F1 Score: 0.7039
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/30
Loss: 0.7560  [0/3200]
Loss: 0.6138  [1600/3200]
Validation Loss: 0.0394, Accuracy: 75.00%, F1 Score: 0.7356
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/30
Loss: 0.3933  [0/3200]
Loss: 0.1723  [1600/3200]
Validation Loss: 0.0367, Accuracy: 76.25%, F1 Score: 0.7579
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/30
Loss: 0.2401  [0/3200]
Loss: 0.1601  [1600/3200]
Validation Loss: 0.0344, Accuracy: 79.25%, F1 Score: 0.7904
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/30
Loss: 0.2472  [0/3200]
Loss: 0.2762  [1600/3200]
Validation Loss: 0.0354, Accuracy: 78.25%, F1 Score: 0.7774
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/30
Loss: 0.2068  [0/3200]
Loss: 0.1113  [1600/3200]
Validation Loss: 0.0360, Accuracy: 77.50%, F1 Score: 0.7710
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/30
Loss: 0.2970  [0/3200]
Loss: 0.2086  [1600/3200]
Validation Loss: 0.0365, Accuracy: 77.75%, F1 Score: 0.7805
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/30
Loss: 0.0827  [0/3200]
Loss: 0.1533  [1600/3200]
Validation Loss: 0.0376, Accuracy: 78.38%, F1 Score: 0.7818
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/30
Loss: 0.0820  [0/3200]
Loss: 0.0597  [1600/3200]
Validation Loss: 0.0386, Accuracy: 76.62%, F1 Score: 0.7588
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/30
Loss: 0.0258  [0/3200]
Loss: 0.0502  [1600/3200]
Validation Loss: 0.0405, Accuracy: 76.88%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/30
Loss: 0.0699  [0/3200]
Loss: 0.0080  [1600/3200]
Validation Loss: 0.0455, Accuracy: 75.00%, F1 Score: 0.7400
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/30
Loss: 0.0169  [0/3200]
Loss: 0.0345  [1600/3200]
Validation Loss: 0.0404, Accuracy: 78.50%, F1 Score: 0.7829
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/30
Loss: 0.0233  [0/3200]
Loss: 0.0565  [1600/3200]
Validation Loss: 0.0439, Accuracy: 77.38%, F1 Score: 0.7674
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/30
Loss: 0.0165  [0/3200]
Loss: 0.0138  [1600/3200]
Validation Loss: 0.0446, Accuracy: 76.50%, F1 Score: 0.7607
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/30
Loss: 0.0125  [0/3200]
Loss: 0.0189  [1600/3200]
Validation Loss: 0.0453, Accuracy: 77.12%, F1 Score: 0.7705
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/30
Loss: 0.0094  [0/3200]
Loss: 0.1368  [1600/3200]
Validation Loss: 0.0493, Accuracy: 75.38%, F1 Score: 0.7474
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/30
Loss: 0.0070  [0/3200]
Loss: 0.0071  [1600/3200]
Validation Loss: 0.0478, Accuracy: 76.75%, F1 Score: 0.7665
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/30
Loss: 0.0533  [0/3200]
Loss: 0.0101  [1600/3200]
Validation Loss: 0.0498, Accuracy: 76.75%, F1 Score: 0.7640
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/30
Loss: 0.0048  [0/3200]
Loss: 0.0052  [1600/3200]
Validation Loss: 0.0498, Accuracy: 76.88%, F1 Score: 0.7674
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/30
Loss: 0.0043  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0628, Accuracy: 74.75%, F1 Score: 0.7437
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/30
Loss: 0.0075  [0/3200]
Loss: 0.0126  [1600/3200]
Validation Loss: 0.0550, Accuracy: 75.50%, F1 Score: 0.7481
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/30
Loss: 0.0033  [0/3200]
Loss: 0.0027  [1600/3200]
Validation Loss: 0.0548, Accuracy: 75.50%, F1 Score: 0.7487
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/30
Loss: 0.0015  [0/3200]
Loss: 0.0102  [1600/3200]
Validation Loss: 0.0567, Accuracy: 76.00%, F1 Score: 0.7550
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/30
Loss: 0.0237  [0/3200]
Loss: 0.0036  [1600/3200]
Validation Loss: 0.0548, Accuracy: 76.38%, F1 Score: 0.7604
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/30
Loss: 0.0046  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0558, Accuracy: 76.12%, F1 Score: 0.7570
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/30
Loss: 0.0005  [0/3200]
Loss: 0.0021  [1600/3200]
Validation Loss: 0.0569, Accuracy: 76.62%, F1 Score: 0.7605
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/30
Loss: 0.0009  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0609, Accuracy: 75.75%, F1 Score: 0.7525
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/30
Loss: 0.0131  [0/3200]
Loss: 0.0036  [1600/3200]
Validation Loss: 0.0569, Accuracy: 77.38%, F1 Score: 0.7715
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/30
Loss: 0.0008  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0578, Accuracy: 77.12%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 8.1231e-05.
Epoch 30/30
Loss: 0.0033  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0639, Accuracy: 75.25%, F1 Score: 0.7427
Adjusting learning rate of group 0 to 8.1317e-05.

<ipython-input-38-9cd47fe36c97>:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Scheduler': scheduler_name, 'Learning Rate': lr,

results_df

Παρατηρούμε μείωση σε σχέση με την πρότερη απόδοση. Οπότε στο επόμενο βήμα θα πειραματιστούμε και με τα δύο μοντέλα (με και χωρίς Batch Normalization).

Βήμα 6: Regularization

Δοκιμάστε να αμβλύνετε τη διαφορά του train loss απο το validation loss βάζοντας και δοκιμάζοντας διαφορετικές τιμές: (i) weight_decay στον optimizer και (ii) dropout στα linear layers. Αυξήστε τον αριθμό των εποχών από 30 σε 60 και δοκιμάστε τα (i) και (ii) μαζί και ξεχωριστά. Τι επίδοση πετυχαίνετε στο test set;

Εφαρμογή Regularization στο μοντέλο που έχει υποστεί όλα τα optimization request της άσκησης.
from sklearn.metrics import f1_score
import torch.optim.lr_scheduler as lr_scheduler

# Define the list of learning rates and corresponding scheduler names
lr = 2e-3
scheduler_name = 'OneCycleLR'
weight_decays = [0.0, 1e-4, 1e-3]
dropout_rates = [0.0, 0.2, 0.5]
epochs = 60

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=['Weight Decay', 'Dropout Rate', 'Best Accuracy', 'Best F1 Score'])

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Initialize the model with Mish activation and BatchNorm2d layers
class CNN_pp_re(nn.Module):
    def __init__(self, output_dim, dropout_rate=0.0):
        super(CNN_pp_re, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm2d(16)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm2d(32)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)
        self.bn4 = nn.BatchNorm2d(128)

        # Fully connected (dense) layers
        self.fc1 = nn.Linear(4096, 1024)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 32)
        self.fc4 = nn.Linear(32, output_dim)

    def forward(self, x):
        # Convolutional layers
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.maxpool1(x)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.maxpool2(x)
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.maxpool3(x)
        x = torch.relu(self.bn4(self.conv4(x)))

        # Flatten the tensor for the fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)

        return x

# Iterate over weight decays and dropout rates
for weight_decay in weight_decays:
    for dropout_rate in dropout_rates:
        # Set the random seeds for reproducibility within the loop
        torch.manual_seed(SEED)
        random.seed(SEED)
        np.random.seed(SEED)
        torch.cuda.manual_seed_all(SEED)

        # Initialize the model
        model = CNN_pp_re(output_dim=4, dropout_rate=dropout_rate).to(device)
        model.apply(weights_init)  # Initialize the weights

        optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=weight_decay)

        # Create the scheduler based on the scheduler name
        if scheduler_name == 'OneCycleLR':
            scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=60, steps_per_epoch=len(train_loader), verbose=True)

        # Train the model
        print(f"Training with weight decay {weight_decay} and dropout rate {dropout_rate}...")
        best_accuracy = 0.0
        best_f1 = 0.0
        for epoch in range(epochs):
            print(f"Epoch {epoch+1}/{epochs}")
            train_loop(train_loader, model, loss_fn, optimizer)
            test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
            print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

            # Update the best accuracy and F1 score
            if accuracy > best_accuracy:
                best_accuracy = accuracy
            if f1 > best_f1:
                best_f1 = f1

            scheduler.step()  # Update the learning rate

        # Add the results to the DataFrame
        results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,
                                        'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1}, ignore_index=True)

Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0 and dropout rate 0.0...
Epoch 1/60
Loss: 1.3065  [0/3200]
Loss: 0.8184  [1600/3200]
Validation Loss: 0.0426, Accuracy: 72.38%, F1 Score: 0.7039
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.7560  [0/3200]
Loss: 0.6150  [1600/3200]
Validation Loss: 0.0393, Accuracy: 75.25%, F1 Score: 0.7368
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.4039  [0/3200]
Loss: 0.1787  [1600/3200]
Validation Loss: 0.0371, Accuracy: 75.88%, F1 Score: 0.7533
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2417  [0/3200]
Loss: 0.1710  [1600/3200]
Validation Loss: 0.0342, Accuracy: 78.88%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.2641  [0/3200]
Loss: 0.2872  [1600/3200]
Validation Loss: 0.0352, Accuracy: 78.25%, F1 Score: 0.7774
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.2181  [0/3200]
Loss: 0.1146  [1600/3200]
Validation Loss: 0.0358, Accuracy: 77.38%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.2879  [0/3200]
Loss: 0.2064  [1600/3200]
Validation Loss: 0.0362, Accuracy: 78.50%, F1 Score: 0.7872
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.0771  [0/3200]
Loss: 0.1609  [1600/3200]
Validation Loss: 0.0378, Accuracy: 77.50%, F1 Score: 0.7728
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.0759  [0/3200]
Loss: 0.0551  [1600/3200]
Validation Loss: 0.0382, Accuracy: 76.62%, F1 Score: 0.7600
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.0273  [0/3200]
Loss: 0.0542  [1600/3200]
Validation Loss: 0.0403, Accuracy: 77.38%, F1 Score: 0.7744
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.0656  [0/3200]
Loss: 0.0082  [1600/3200]
Validation Loss: 0.0452, Accuracy: 75.50%, F1 Score: 0.7486
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.0158  [0/3200]
Loss: 0.0385  [1600/3200]
Validation Loss: 0.0405, Accuracy: 77.62%, F1 Score: 0.7744
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.0266  [0/3200]
Loss: 0.0484  [1600/3200]
Validation Loss: 0.0440, Accuracy: 76.62%, F1 Score: 0.7597
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.0175  [0/3200]
Loss: 0.0163  [1600/3200]
Validation Loss: 0.0443, Accuracy: 76.12%, F1 Score: 0.7582
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.0123  [0/3200]
Loss: 0.0179  [1600/3200]
Validation Loss: 0.0453, Accuracy: 77.00%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.0105  [0/3200]
Loss: 0.1339  [1600/3200]
Validation Loss: 0.0493, Accuracy: 75.38%, F1 Score: 0.7484
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.0069  [0/3200]
Loss: 0.0059  [1600/3200]
Validation Loss: 0.0481, Accuracy: 76.88%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.0387  [0/3200]
Loss: 0.0080  [1600/3200]
Validation Loss: 0.0500, Accuracy: 76.00%, F1 Score: 0.7571
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0047  [0/3200]
Loss: 0.0051  [1600/3200]
Validation Loss: 0.0505, Accuracy: 76.50%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0042  [0/3200]
Loss: 0.0021  [1600/3200]
Validation Loss: 0.0640, Accuracy: 74.25%, F1 Score: 0.7386
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.0080  [0/3200]
Loss: 0.0119  [1600/3200]
Validation Loss: 0.0550, Accuracy: 75.50%, F1 Score: 0.7486
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.0030  [0/3200]
Loss: 0.0028  [1600/3200]
Validation Loss: 0.0541, Accuracy: 76.00%, F1 Score: 0.7545
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0015  [0/3200]
Loss: 0.0100  [1600/3200]
Validation Loss: 0.0575, Accuracy: 75.38%, F1 Score: 0.7490
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0267  [0/3200]
Loss: 0.0029  [1600/3200]
Validation Loss: 0.0549, Accuracy: 76.50%, F1 Score: 0.7622
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0052  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0560, Accuracy: 75.88%, F1 Score: 0.7549
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0005  [0/3200]
Loss: 0.0026  [1600/3200]
Validation Loss: 0.0558, Accuracy: 76.38%, F1 Score: 0.7582
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0010  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0601, Accuracy: 75.88%, F1 Score: 0.7542
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0110  [0/3200]
Loss: 0.0051  [1600/3200]
Validation Loss: 0.0565, Accuracy: 76.50%, F1 Score: 0.7629
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0011  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0579, Accuracy: 76.75%, F1 Score: 0.7603
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0034  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0648, Accuracy: 75.50%, F1 Score: 0.7447
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0005  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.0626, Accuracy: 76.38%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0010  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0616, Accuracy: 76.50%, F1 Score: 0.7614
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.0024  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0642, Accuracy: 77.25%, F1 Score: 0.7703
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0004  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0642, Accuracy: 76.50%, F1 Score: 0.7608
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0001  [0/3200]
Loss: 0.0014  [1600/3200]
Validation Loss: 0.0630, Accuracy: 76.12%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0002  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0643, Accuracy: 76.00%, F1 Score: 0.7571
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0001  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0665, Accuracy: 77.12%, F1 Score: 0.7696
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0003  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0676, Accuracy: 75.88%, F1 Score: 0.7537
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0002  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0678, Accuracy: 76.12%, F1 Score: 0.7572
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0001  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0671, Accuracy: 77.25%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0004  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0678, Accuracy: 76.62%, F1 Score: 0.7630
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0002  [0/3200]
Loss: 0.0018  [1600/3200]
Validation Loss: 0.0702, Accuracy: 76.25%, F1 Score: 0.7569
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0002  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0700, Accuracy: 75.50%, F1 Score: 0.7483
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0003  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0650, Accuracy: 76.75%, F1 Score: 0.7649
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0769, Accuracy: 75.75%, F1 Score: 0.7492
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0004  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0782, Accuracy: 75.00%, F1 Score: 0.7416
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0004  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0707, Accuracy: 76.50%, F1 Score: 0.7616
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0001  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0724, Accuracy: 76.38%, F1 Score: 0.7587
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0752, Accuracy: 76.12%, F1 Score: 0.7560
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0000  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0787, Accuracy: 76.38%, F1 Score: 0.7568
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0001  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0739, Accuracy: 77.12%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0766, Accuracy: 75.88%, F1 Score: 0.7541
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0749, Accuracy: 76.75%, F1 Score: 0.7645
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0000  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0754, Accuracy: 76.75%, F1 Score: 0.7588
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0005  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0757, Accuracy: 76.62%, F1 Score: 0.7609
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0000  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0763, Accuracy: 76.38%, F1 Score: 0.7608
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0000  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0765, Accuracy: 76.00%, F1 Score: 0.7553
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0757, Accuracy: 76.38%, F1 Score: 0.7588
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0771, Accuracy: 76.50%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0780, Accuracy: 76.62%, F1 Score: 0.7608
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0 and dropout rate 0.2...
Epoch 1/60
Loss: 1.3219  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 0.9731  [1600/3200]
Validation Loss: 0.0448, Accuracy: 67.88%, F1 Score: 0.6524
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.7953  [0/3200]
Loss: 0.7954  [1600/3200]
Validation Loss: 0.0399, Accuracy: 73.75%, F1 Score: 0.7279
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.5550  [0/3200]
Loss: 0.2494  [1600/3200]
Validation Loss: 0.0375, Accuracy: 76.38%, F1 Score: 0.7518
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2338  [0/3200]
Loss: 0.1927  [1600/3200]
Validation Loss: 0.0356, Accuracy: 79.25%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.3535  [0/3200]
Loss: 0.5919  [1600/3200]
Validation Loss: 0.0364, Accuracy: 77.62%, F1 Score: 0.7683
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.4372  [0/3200]
Loss: 0.1819  [1600/3200]
Validation Loss: 0.0360, Accuracy: 78.75%, F1 Score: 0.7828
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.4163  [0/3200]
Loss: 0.2640  [1600/3200]
Validation Loss: 0.0365, Accuracy: 77.12%, F1 Score: 0.7668
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.1539  [0/3200]
Loss: 0.2232  [1600/3200]
Validation Loss: 0.0363, Accuracy: 78.62%, F1 Score: 0.7851
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.2137  [0/3200]
Loss: 0.0792  [1600/3200]
Validation Loss: 0.0367, Accuracy: 78.88%, F1 Score: 0.7827
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.0496  [0/3200]
Loss: 0.1214  [1600/3200]
Validation Loss: 0.0394, Accuracy: 76.38%, F1 Score: 0.7646
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.1813  [0/3200]
Loss: 0.0505  [1600/3200]
Validation Loss: 0.0408, Accuracy: 75.50%, F1 Score: 0.7476
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.0387  [0/3200]
Loss: 0.0649  [1600/3200]
Validation Loss: 0.0390, Accuracy: 79.75%, F1 Score: 0.7968
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.0790  [0/3200]
Loss: 0.0672  [1600/3200]
Validation Loss: 0.0426, Accuracy: 76.75%, F1 Score: 0.7597
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.1068  [0/3200]
Loss: 0.0425  [1600/3200]
Validation Loss: 0.0404, Accuracy: 77.62%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.0233  [0/3200]
Loss: 0.0424  [1600/3200]
Validation Loss: 0.0426, Accuracy: 77.75%, F1 Score: 0.7771
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.0388  [0/3200]
Loss: 0.2501  [1600/3200]
Validation Loss: 0.0477, Accuracy: 76.88%, F1 Score: 0.7625
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.0507  [0/3200]
Loss: 0.0225  [1600/3200]
Validation Loss: 0.0449, Accuracy: 78.00%, F1 Score: 0.7769
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.0823  [0/3200]
Loss: 0.0393  [1600/3200]
Validation Loss: 0.0504, Accuracy: 75.00%, F1 Score: 0.7400
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0229  [0/3200]
Loss: 0.0542  [1600/3200]
Validation Loss: 0.0477, Accuracy: 76.38%, F1 Score: 0.7637
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0160  [0/3200]
Loss: 0.0246  [1600/3200]
Validation Loss: 0.0530, Accuracy: 75.38%, F1 Score: 0.7493
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.0133  [0/3200]
Loss: 0.0318  [1600/3200]
Validation Loss: 0.0522, Accuracy: 76.25%, F1 Score: 0.7555
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.0279  [0/3200]
Loss: 0.0075  [1600/3200]
Validation Loss: 0.0504, Accuracy: 76.88%, F1 Score: 0.7665
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0064  [0/3200]
Loss: 0.0454  [1600/3200]
Validation Loss: 0.0526, Accuracy: 75.62%, F1 Score: 0.7543
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0249  [0/3200]
Loss: 0.0106  [1600/3200]
Validation Loss: 0.0509, Accuracy: 76.50%, F1 Score: 0.7623
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0122  [0/3200]
Loss: 0.0053  [1600/3200]
Validation Loss: 0.0506, Accuracy: 77.62%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0026  [0/3200]
Loss: 0.0078  [1600/3200]
Validation Loss: 0.0516, Accuracy: 77.00%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0015  [0/3200]
Loss: 0.0014  [1600/3200]
Validation Loss: 0.0554, Accuracy: 75.88%, F1 Score: 0.7546
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0174  [0/3200]
Loss: 0.0312  [1600/3200]
Validation Loss: 0.0534, Accuracy: 78.50%, F1 Score: 0.7830
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0064  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0556, Accuracy: 76.50%, F1 Score: 0.7592
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0044  [0/3200]
Loss: 0.0083  [1600/3200]
Validation Loss: 0.0581, Accuracy: 76.00%, F1 Score: 0.7505
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0027  [0/3200]
Loss: 0.0083  [1600/3200]
Validation Loss: 0.0558, Accuracy: 76.75%, F1 Score: 0.7656
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0017  [0/3200]
Loss: 0.0009  [1600/3200]
Validation Loss: 0.0562, Accuracy: 77.00%, F1 Score: 0.7682
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.0155  [0/3200]
Loss: 0.0028  [1600/3200]
Validation Loss: 0.0592, Accuracy: 76.38%, F1 Score: 0.7593
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0007  [0/3200]
Loss: 0.0023  [1600/3200]
Validation Loss: 0.0569, Accuracy: 77.38%, F1 Score: 0.7711
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0020  [0/3200]
Loss: 0.0042  [1600/3200]
Validation Loss: 0.0662, Accuracy: 74.75%, F1 Score: 0.7360
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0006  [0/3200]
Loss: 0.0028  [1600/3200]
Validation Loss: 0.0595, Accuracy: 76.38%, F1 Score: 0.7617
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0003  [0/3200]
Loss: 0.0018  [1600/3200]
Validation Loss: 0.0606, Accuracy: 76.12%, F1 Score: 0.7562
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0014  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0614, Accuracy: 77.12%, F1 Score: 0.7677
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0009  [0/3200]
Loss: 0.0029  [1600/3200]
Validation Loss: 0.0623, Accuracy: 77.38%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0003  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0666, Accuracy: 75.75%, F1 Score: 0.7463
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0007  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0615, Accuracy: 76.88%, F1 Score: 0.7646
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0003  [0/3200]
Loss: 0.1152  [1600/3200]
Validation Loss: 0.0691, Accuracy: 75.38%, F1 Score: 0.7451
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0003  [0/3200]
Loss: 0.0261  [1600/3200]
Validation Loss: 0.0663, Accuracy: 76.38%, F1 Score: 0.7569
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0008  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0607, Accuracy: 77.12%, F1 Score: 0.7701
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0015  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0668, Accuracy: 77.12%, F1 Score: 0.7652
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0006  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0621, Accuracy: 77.25%, F1 Score: 0.7689
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0009  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0634, Accuracy: 77.12%, F1 Score: 0.7684
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0023  [0/3200]
Loss: 0.0029  [1600/3200]
Validation Loss: 0.0673, Accuracy: 76.88%, F1 Score: 0.7651
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0011  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0716, Accuracy: 75.38%, F1 Score: 0.7470
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0002  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0779, Accuracy: 75.12%, F1 Score: 0.7394
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0002  [0/3200]
Loss: 0.0026  [1600/3200]
Validation Loss: 0.0680, Accuracy: 77.00%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0030  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0734, Accuracy: 75.25%, F1 Score: 0.7474
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0727, Accuracy: 75.75%, F1 Score: 0.7532
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0001  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0690, Accuracy: 77.00%, F1 Score: 0.7665
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0006  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0685, Accuracy: 77.62%, F1 Score: 0.7722
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0001  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0702, Accuracy: 76.88%, F1 Score: 0.7661
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0004  [0/3200]
Loss: 0.0027  [1600/3200]
Validation Loss: 0.0691, Accuracy: 77.75%, F1 Score: 0.7749
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0004  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0762, Accuracy: 76.25%, F1 Score: 0.7516
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0006  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0721, Accuracy: 77.12%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0005  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0697, Accuracy: 78.12%, F1 Score: 0.7778
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0 and dropout rate 0.5...
Epoch 1/60
Loss: 1.4068  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.0340  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.38%, F1 Score: 0.6587
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0309  [0/3200]
Loss: 0.9789  [1600/3200]
Validation Loss: 0.0446, Accuracy: 69.38%, F1 Score: 0.6808
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7666  [0/3200]
Loss: 0.5071  [1600/3200]
Validation Loss: 0.0384, Accuracy: 76.00%, F1 Score: 0.7459
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.3333  [0/3200]
Loss: 0.2567  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8734  [0/3200]
Loss: 0.6187  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7106  [0/3200]
Loss: 0.3061  [1600/3200]
Validation Loss: 0.0371, Accuracy: 76.62%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5991  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7522
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3038  [0/3200]
Loss: 0.3593  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.12%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.5215  [0/3200]
Loss: 0.3155  [1600/3200]
Validation Loss: 0.0339, Accuracy: 80.00%, F1 Score: 0.7938
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2824  [0/3200]
Loss: 0.2885  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.3508  [0/3200]
Loss: 0.1472  [1600/3200]
Validation Loss: 0.0332, Accuracy: 80.50%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2190  [0/3200]
Loss: 0.2166  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.62%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3937  [0/3200]
Loss: 0.1673  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.12%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.2727  [0/3200]
Loss: 0.2061  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.3493  [0/3200]
Loss: 0.3084  [1600/3200]
Validation Loss: 0.0353, Accuracy: 79.12%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.2409  [0/3200]
Loss: 0.3674  [1600/3200]
Validation Loss: 0.0375, Accuracy: 78.88%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.1645  [0/3200]
Loss: 0.1782  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.75%, F1 Score: 0.7967
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.1921  [0/3200]
Loss: 0.1559  [1600/3200]
Validation Loss: 0.0377, Accuracy: 78.62%, F1 Score: 0.7821
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.1434  [0/3200]
Loss: 0.3144  [1600/3200]
Validation Loss: 0.0373, Accuracy: 79.00%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0890  [0/3200]
Loss: 0.0579  [1600/3200]
Validation Loss: 0.0399, Accuracy: 78.62%, F1 Score: 0.7871
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1354  [0/3200]
Loss: 0.3022  [1600/3200]
Validation Loss: 0.0407, Accuracy: 79.00%, F1 Score: 0.7827
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.1071  [0/3200]
Loss: 0.0615  [1600/3200]
Validation Loss: 0.0395, Accuracy: 79.12%, F1 Score: 0.7890
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0352  [0/3200]
Loss: 0.3220  [1600/3200]
Validation Loss: 0.0435, Accuracy: 76.62%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0740  [0/3200]
Loss: 0.2077  [1600/3200]
Validation Loss: 0.0419, Accuracy: 78.25%, F1 Score: 0.7787
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.2023  [0/3200]
Loss: 0.0523  [1600/3200]
Validation Loss: 0.0411, Accuracy: 79.50%, F1 Score: 0.7937
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0448  [0/3200]
Loss: 0.0335  [1600/3200]
Validation Loss: 0.0423, Accuracy: 79.00%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0219  [0/3200]
Loss: 0.0496  [1600/3200]
Validation Loss: 0.0438, Accuracy: 78.62%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0509  [0/3200]
Loss: 0.1963  [1600/3200]
Validation Loss: 0.0468, Accuracy: 77.62%, F1 Score: 0.7695
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0238  [0/3200]
Loss: 0.0277  [1600/3200]
Validation Loss: 0.0445, Accuracy: 78.62%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0291  [0/3200]
Loss: 0.0403  [1600/3200]
Validation Loss: 0.0486, Accuracy: 78.25%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0102  [0/3200]
Loss: 0.0528  [1600/3200]
Validation Loss: 0.0491, Accuracy: 77.75%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0074  [0/3200]
Loss: 0.0048  [1600/3200]
Validation Loss: 0.0458, Accuracy: 78.75%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.1512  [0/3200]
Loss: 0.0292  [1600/3200]
Validation Loss: 0.0483, Accuracy: 79.00%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0056  [0/3200]
Loss: 0.0385  [1600/3200]
Validation Loss: 0.0464, Accuracy: 79.75%, F1 Score: 0.7953
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0606  [0/3200]
Loss: 0.0637  [1600/3200]
Validation Loss: 0.0503, Accuracy: 79.00%, F1 Score: 0.7851
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0063  [0/3200]
Loss: 0.0068  [1600/3200]
Validation Loss: 0.0493, Accuracy: 78.62%, F1 Score: 0.7839
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0141  [0/3200]
Loss: 0.0316  [1600/3200]
Validation Loss: 0.0492, Accuracy: 78.75%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0097  [0/3200]
Loss: 0.0173  [1600/3200]
Validation Loss: 0.0508, Accuracy: 78.12%, F1 Score: 0.7782
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0037  [0/3200]
Loss: 0.0083  [1600/3200]
Validation Loss: 0.0537, Accuracy: 77.88%, F1 Score: 0.7764
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0026  [0/3200]
Loss: 0.0046  [1600/3200]
Validation Loss: 0.0558, Accuracy: 76.25%, F1 Score: 0.7526
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0109  [0/3200]
Loss: 0.0125  [1600/3200]
Validation Loss: 0.0529, Accuracy: 78.88%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0034  [0/3200]
Loss: 0.0438  [1600/3200]
Validation Loss: 0.0521, Accuracy: 78.88%, F1 Score: 0.7877
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0218  [0/3200]
Loss: 0.0095  [1600/3200]
Validation Loss: 0.0549, Accuracy: 78.62%, F1 Score: 0.7822
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0015  [0/3200]
Loss: 0.0092  [1600/3200]
Validation Loss: 0.0542, Accuracy: 79.00%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0049  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0560, Accuracy: 78.88%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0007  [0/3200]
Loss: 0.0123  [1600/3200]
Validation Loss: 0.0530, Accuracy: 79.12%, F1 Score: 0.7894
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0265  [0/3200]
Loss: 0.0053  [1600/3200]
Validation Loss: 0.0540, Accuracy: 79.75%, F1 Score: 0.7965
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0613  [0/3200]
Loss: 0.0033  [1600/3200]
Validation Loss: 0.0572, Accuracy: 79.25%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0113  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0573, Accuracy: 78.50%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0049  [0/3200]
Loss: 0.0123  [1600/3200]
Validation Loss: 0.0616, Accuracy: 77.38%, F1 Score: 0.7676
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0010  [0/3200]
Loss: 0.0104  [1600/3200]
Validation Loss: 0.0618, Accuracy: 78.00%, F1 Score: 0.7729
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0017  [0/3200]
Loss: 0.0022  [1600/3200]
Validation Loss: 0.0587, Accuracy: 79.12%, F1 Score: 0.7863
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0010  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0609, Accuracy: 78.12%, F1 Score: 0.7784
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0012  [0/3200]
Loss: 0.1055  [1600/3200]
Validation Loss: 0.0580, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0078  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0573, Accuracy: 79.12%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0056  [0/3200]
Loss: 0.0032  [1600/3200]
Validation Loss: 0.0602, Accuracy: 78.62%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0039  [0/3200]
Loss: 0.0057  [1600/3200]
Validation Loss: 0.0581, Accuracy: 80.12%, F1 Score: 0.7987
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0002  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0627, Accuracy: 78.25%, F1 Score: 0.7762
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0008  [0/3200]
Loss: 0.0014  [1600/3200]
Validation Loss: 0.0612, Accuracy: 79.50%, F1 Score: 0.7922
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0011  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0644, Accuracy: 78.50%, F1 Score: 0.7800
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0001 and dropout rate 0.0...
Epoch 1/60
Loss: 1.3065  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 0.8365  [1600/3200]
Validation Loss: 0.0425, Accuracy: 71.62%, F1 Score: 0.6914
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.7424  [0/3200]
Loss: 0.6422  [1600/3200]
Validation Loss: 0.0390, Accuracy: 75.62%, F1 Score: 0.7402
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.4088  [0/3200]
Loss: 0.1768  [1600/3200]
Validation Loss: 0.0365, Accuracy: 75.88%, F1 Score: 0.7507
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2262  [0/3200]
Loss: 0.1989  [1600/3200]
Validation Loss: 0.0342, Accuracy: 78.75%, F1 Score: 0.7846
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.2603  [0/3200]
Loss: 0.3018  [1600/3200]
Validation Loss: 0.0352, Accuracy: 76.62%, F1 Score: 0.7568
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.2219  [0/3200]
Loss: 0.1131  [1600/3200]
Validation Loss: 0.0362, Accuracy: 76.62%, F1 Score: 0.7602
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.3012  [0/3200]
Loss: 0.2049  [1600/3200]
Validation Loss: 0.0359, Accuracy: 77.25%, F1 Score: 0.7745
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.0800  [0/3200]
Loss: 0.1407  [1600/3200]
Validation Loss: 0.0373, Accuracy: 76.88%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.0832  [0/3200]
Loss: 0.0581  [1600/3200]
Validation Loss: 0.0383, Accuracy: 76.62%, F1 Score: 0.7590
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.0292  [0/3200]
Loss: 0.0565  [1600/3200]
Validation Loss: 0.0405, Accuracy: 76.38%, F1 Score: 0.7637
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.0643  [0/3200]
Loss: 0.0082  [1600/3200]
Validation Loss: 0.0453, Accuracy: 74.88%, F1 Score: 0.7401
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.0161  [0/3200]
Loss: 0.0341  [1600/3200]
Validation Loss: 0.0399, Accuracy: 78.62%, F1 Score: 0.7855
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.0261  [0/3200]
Loss: 0.0541  [1600/3200]
Validation Loss: 0.0444, Accuracy: 76.25%, F1 Score: 0.7551
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.0175  [0/3200]
Loss: 0.0172  [1600/3200]
Validation Loss: 0.0455, Accuracy: 76.50%, F1 Score: 0.7608
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.0107  [0/3200]
Loss: 0.0177  [1600/3200]
Validation Loss: 0.0449, Accuracy: 76.88%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.0087  [0/3200]
Loss: 0.1389  [1600/3200]
Validation Loss: 0.0490, Accuracy: 76.38%, F1 Score: 0.7579
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.0069  [0/3200]
Loss: 0.0059  [1600/3200]
Validation Loss: 0.0473, Accuracy: 76.75%, F1 Score: 0.7660
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.0371  [0/3200]
Loss: 0.0113  [1600/3200]
Validation Loss: 0.0490, Accuracy: 76.12%, F1 Score: 0.7586
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0037  [0/3200]
Loss: 0.0054  [1600/3200]
Validation Loss: 0.0498, Accuracy: 76.50%, F1 Score: 0.7641
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0031  [0/3200]
Loss: 0.0025  [1600/3200]
Validation Loss: 0.0624, Accuracy: 74.25%, F1 Score: 0.7386
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.0066  [0/3200]
Loss: 0.0157  [1600/3200]
Validation Loss: 0.0536, Accuracy: 76.25%, F1 Score: 0.7570
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.0044  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.0538, Accuracy: 75.88%, F1 Score: 0.7532
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0018  [0/3200]
Loss: 0.0104  [1600/3200]
Validation Loss: 0.0553, Accuracy: 76.12%, F1 Score: 0.7572
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0175  [0/3200]
Loss: 0.0030  [1600/3200]
Validation Loss: 0.0548, Accuracy: 75.50%, F1 Score: 0.7507
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0051  [0/3200]
Loss: 0.0014  [1600/3200]
Validation Loss: 0.0547, Accuracy: 75.75%, F1 Score: 0.7549
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0005  [0/3200]
Loss: 0.0018  [1600/3200]
Validation Loss: 0.0557, Accuracy: 77.38%, F1 Score: 0.7679
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0011  [0/3200]
Loss: 0.0009  [1600/3200]
Validation Loss: 0.0615, Accuracy: 75.88%, F1 Score: 0.7530
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0245  [0/3200]
Loss: 0.0077  [1600/3200]
Validation Loss: 0.0576, Accuracy: 77.00%, F1 Score: 0.7668
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0009  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0578, Accuracy: 76.50%, F1 Score: 0.7570
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0021  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0606, Accuracy: 75.75%, F1 Score: 0.7480
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0004  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0625, Accuracy: 76.38%, F1 Score: 0.7594
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0011  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0607, Accuracy: 76.38%, F1 Score: 0.7593
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.0034  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0636, Accuracy: 76.88%, F1 Score: 0.7640
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0004  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0624, Accuracy: 77.50%, F1 Score: 0.7710
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0002  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0613, Accuracy: 76.75%, F1 Score: 0.7651
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0002  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0612, Accuracy: 76.50%, F1 Score: 0.7633
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0648, Accuracy: 76.75%, F1 Score: 0.7633
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0003  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0644, Accuracy: 75.75%, F1 Score: 0.7538
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0003  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0668, Accuracy: 76.38%, F1 Score: 0.7591
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0001  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0649, Accuracy: 76.25%, F1 Score: 0.7601
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0004  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0650, Accuracy: 76.62%, F1 Score: 0.7639
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0002  [0/3200]
Loss: 0.0018  [1600/3200]
Validation Loss: 0.0683, Accuracy: 75.62%, F1 Score: 0.7504
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0003  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0680, Accuracy: 75.88%, F1 Score: 0.7517
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0639, Accuracy: 76.50%, F1 Score: 0.7607
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0778, Accuracy: 75.25%, F1 Score: 0.7436
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0005  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0729, Accuracy: 75.62%, F1 Score: 0.7483
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0004  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0671, Accuracy: 76.50%, F1 Score: 0.7614
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0001  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.0685, Accuracy: 76.50%, F1 Score: 0.7611
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0686, Accuracy: 77.62%, F1 Score: 0.7722
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0001  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0721, Accuracy: 76.38%, F1 Score: 0.7570
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0001  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0688, Accuracy: 76.12%, F1 Score: 0.7552
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0680, Accuracy: 76.88%, F1 Score: 0.7660
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0002  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0688, Accuracy: 76.38%, F1 Score: 0.7604
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0704, Accuracy: 76.62%, F1 Score: 0.7587
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0005  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0690, Accuracy: 76.75%, F1 Score: 0.7634
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0000  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0667, Accuracy: 77.12%, F1 Score: 0.7715
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0002  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0713, Accuracy: 76.38%, F1 Score: 0.7596
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0003  [0/3200]
Loss: 0.0000  [1600/3200]
Validation Loss: 0.0705, Accuracy: 75.88%, F1 Score: 0.7518
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0697, Accuracy: 75.88%, F1 Score: 0.7527
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0001  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0707, Accuracy: 76.25%, F1 Score: 0.7582
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0001 and dropout rate 0.2...
Epoch 1/60
Loss: 1.3219  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 0.9897  [1600/3200]
Validation Loss: 0.0442, Accuracy: 67.88%, F1 Score: 0.6560
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.7910  [0/3200]
Loss: 0.7953  [1600/3200]
Validation Loss: 0.0397, Accuracy: 73.62%, F1 Score: 0.7292
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.5315  [0/3200]
Loss: 0.2669  [1600/3200]
Validation Loss: 0.0377, Accuracy: 74.88%, F1 Score: 0.7319
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2107  [0/3200]
Loss: 0.2214  [1600/3200]
Validation Loss: 0.0357, Accuracy: 79.62%, F1 Score: 0.7901
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.3480  [0/3200]
Loss: 0.5796  [1600/3200]
Validation Loss: 0.0372, Accuracy: 76.50%, F1 Score: 0.7526
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.4598  [0/3200]
Loss: 0.1475  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.12%, F1 Score: 0.7873
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.4423  [0/3200]
Loss: 0.2661  [1600/3200]
Validation Loss: 0.0370, Accuracy: 76.88%, F1 Score: 0.7611
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.1620  [0/3200]
Loss: 0.2322  [1600/3200]
Validation Loss: 0.0366, Accuracy: 78.00%, F1 Score: 0.7787
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.2684  [0/3200]
Loss: 0.1012  [1600/3200]
Validation Loss: 0.0364, Accuracy: 78.88%, F1 Score: 0.7836
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.0592  [0/3200]
Loss: 0.1184  [1600/3200]
Validation Loss: 0.0398, Accuracy: 76.38%, F1 Score: 0.7636
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.2276  [0/3200]
Loss: 0.0439  [1600/3200]
Validation Loss: 0.0413, Accuracy: 76.62%, F1 Score: 0.7573
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.0275  [0/3200]
Loss: 0.0768  [1600/3200]
Validation Loss: 0.0386, Accuracy: 79.25%, F1 Score: 0.7916
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.0917  [0/3200]
Loss: 0.0652  [1600/3200]
Validation Loss: 0.0431, Accuracy: 77.62%, F1 Score: 0.7682
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.0941  [0/3200]
Loss: 0.0270  [1600/3200]
Validation Loss: 0.0419, Accuracy: 77.12%, F1 Score: 0.7653
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.0248  [0/3200]
Loss: 0.0443  [1600/3200]
Validation Loss: 0.0425, Accuracy: 78.12%, F1 Score: 0.7782
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.0505  [0/3200]
Loss: 0.2799  [1600/3200]
Validation Loss: 0.0485, Accuracy: 76.38%, F1 Score: 0.7568
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.0548  [0/3200]
Loss: 0.0222  [1600/3200]
Validation Loss: 0.0451, Accuracy: 77.75%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.0680  [0/3200]
Loss: 0.0292  [1600/3200]
Validation Loss: 0.0497, Accuracy: 76.62%, F1 Score: 0.7576
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0224  [0/3200]
Loss: 0.0288  [1600/3200]
Validation Loss: 0.0484, Accuracy: 76.50%, F1 Score: 0.7642
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0094  [0/3200]
Loss: 0.0196  [1600/3200]
Validation Loss: 0.0539, Accuracy: 75.12%, F1 Score: 0.7479
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.0172  [0/3200]
Loss: 0.0552  [1600/3200]
Validation Loss: 0.0522, Accuracy: 77.00%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.0540  [0/3200]
Loss: 0.0059  [1600/3200]
Validation Loss: 0.0500, Accuracy: 77.12%, F1 Score: 0.7691
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0082  [0/3200]
Loss: 0.0609  [1600/3200]
Validation Loss: 0.0539, Accuracy: 75.12%, F1 Score: 0.7485
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0283  [0/3200]
Loss: 0.0109  [1600/3200]
Validation Loss: 0.0513, Accuracy: 76.50%, F1 Score: 0.7623
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0115  [0/3200]
Loss: 0.0067  [1600/3200]
Validation Loss: 0.0510, Accuracy: 77.88%, F1 Score: 0.7775
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0034  [0/3200]
Loss: 0.0129  [1600/3200]
Validation Loss: 0.0519, Accuracy: 76.88%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0032  [0/3200]
Loss: 0.0017  [1600/3200]
Validation Loss: 0.0556, Accuracy: 76.25%, F1 Score: 0.7584
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0277  [0/3200]
Loss: 0.0155  [1600/3200]
Validation Loss: 0.0550, Accuracy: 78.12%, F1 Score: 0.7774
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0032  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0554, Accuracy: 77.00%, F1 Score: 0.7632
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0033  [0/3200]
Loss: 0.0125  [1600/3200]
Validation Loss: 0.0593, Accuracy: 75.88%, F1 Score: 0.7495
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0021  [0/3200]
Loss: 0.0045  [1600/3200]
Validation Loss: 0.0561, Accuracy: 76.88%, F1 Score: 0.7656
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0021  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0558, Accuracy: 76.62%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.0176  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0590, Accuracy: 76.88%, F1 Score: 0.7625
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0005  [0/3200]
Loss: 0.0051  [1600/3200]
Validation Loss: 0.0565, Accuracy: 79.12%, F1 Score: 0.7882
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0050  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0658, Accuracy: 76.12%, F1 Score: 0.7509
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0002  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.0597, Accuracy: 77.25%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0004  [0/3200]
Loss: 0.0035  [1600/3200]
Validation Loss: 0.0606, Accuracy: 77.12%, F1 Score: 0.7670
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0010  [0/3200]
Loss: 0.0023  [1600/3200]
Validation Loss: 0.0637, Accuracy: 77.12%, F1 Score: 0.7657
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0004  [0/3200]
Loss: 0.0034  [1600/3200]
Validation Loss: 0.0625, Accuracy: 77.88%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0003  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0663, Accuracy: 76.50%, F1 Score: 0.7555
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0014  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0603, Accuracy: 77.88%, F1 Score: 0.7750
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0003  [0/3200]
Loss: 0.0692  [1600/3200]
Validation Loss: 0.0650, Accuracy: 76.50%, F1 Score: 0.7599
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0003  [0/3200]
Loss: 0.0067  [1600/3200]
Validation Loss: 0.0681, Accuracy: 76.88%, F1 Score: 0.7597
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0015  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0618, Accuracy: 76.75%, F1 Score: 0.7637
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0030  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0663, Accuracy: 76.12%, F1 Score: 0.7563
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0006  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0610, Accuracy: 77.62%, F1 Score: 0.7736
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0015  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0638, Accuracy: 76.88%, F1 Score: 0.7656
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0047  [0/3200]
Loss: 0.0021  [1600/3200]
Validation Loss: 0.0649, Accuracy: 77.62%, F1 Score: 0.7739
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0003  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0694, Accuracy: 76.50%, F1 Score: 0.7610
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0002  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0755, Accuracy: 75.25%, F1 Score: 0.7406
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0002  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0702, Accuracy: 76.25%, F1 Score: 0.7566
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0007  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0679, Accuracy: 77.12%, F1 Score: 0.7690
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0002  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0709, Accuracy: 76.25%, F1 Score: 0.7590
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0001  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0695, Accuracy: 76.62%, F1 Score: 0.7603
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0011  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0692, Accuracy: 77.00%, F1 Score: 0.7647
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0001  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0688, Accuracy: 77.38%, F1 Score: 0.7720
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0005  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0693, Accuracy: 77.12%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0004  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0740, Accuracy: 77.25%, F1 Score: 0.7632
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0025  [0/3200]
Loss: 0.0001  [1600/3200]
Validation Loss: 0.0693, Accuracy: 77.62%, F1 Score: 0.7723
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0003  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0683, Accuracy: 76.75%, F1 Score: 0.7658
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0001 and dropout rate 0.5...
Epoch 1/60
Loss: 1.4068  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.0254  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.75%, F1 Score: 0.6613
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0474  [0/3200]
Loss: 0.9257  [1600/3200]
Validation Loss: 0.0448, Accuracy: 68.75%, F1 Score: 0.6714
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7345  [0/3200]
Loss: 0.5598  [1600/3200]
Validation Loss: 0.0394, Accuracy: 76.25%, F1 Score: 0.7444
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2906  [0/3200]
Loss: 0.2358  [1600/3200]
Validation Loss: 0.0357, Accuracy: 77.12%, F1 Score: 0.7664
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8163  [0/3200]
Loss: 0.6394  [1600/3200]
Validation Loss: 0.0359, Accuracy: 77.25%, F1 Score: 0.7718
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7063  [0/3200]
Loss: 0.3783  [1600/3200]
Validation Loss: 0.0381, Accuracy: 75.38%, F1 Score: 0.7582
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.6770  [0/3200]
Loss: 0.3902  [1600/3200]
Validation Loss: 0.0381, Accuracy: 75.00%, F1 Score: 0.7396
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3351  [0/3200]
Loss: 0.4017  [1600/3200]
Validation Loss: 0.0361, Accuracy: 78.25%, F1 Score: 0.7736
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.4994  [0/3200]
Loss: 0.3026  [1600/3200]
Validation Loss: 0.0349, Accuracy: 79.25%, F1 Score: 0.7864
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2535  [0/3200]
Loss: 0.2667  [1600/3200]
Validation Loss: 0.0344, Accuracy: 79.62%, F1 Score: 0.7940
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.3503  [0/3200]
Loss: 0.2222  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.88%, F1 Score: 0.7839
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2080  [0/3200]
Loss: 0.2368  [1600/3200]
Validation Loss: 0.0364, Accuracy: 79.12%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3139  [0/3200]
Loss: 0.1919  [1600/3200]
Validation Loss: 0.0366, Accuracy: 78.00%, F1 Score: 0.7718
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.2748  [0/3200]
Loss: 0.1907  [1600/3200]
Validation Loss: 0.0368, Accuracy: 79.00%, F1 Score: 0.7848
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.3442  [0/3200]
Loss: 0.2352  [1600/3200]
Validation Loss: 0.0360, Accuracy: 78.38%, F1 Score: 0.7815
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.3372  [0/3200]
Loss: 0.3042  [1600/3200]
Validation Loss: 0.0373, Accuracy: 78.75%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.1343  [0/3200]
Loss: 0.1335  [1600/3200]
Validation Loss: 0.0375, Accuracy: 79.88%, F1 Score: 0.7973
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.1978  [0/3200]
Loss: 0.1412  [1600/3200]
Validation Loss: 0.0388, Accuracy: 78.50%, F1 Score: 0.7811
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0893  [0/3200]
Loss: 0.2312  [1600/3200]
Validation Loss: 0.0383, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0700  [0/3200]
Loss: 0.0658  [1600/3200]
Validation Loss: 0.0413, Accuracy: 78.25%, F1 Score: 0.7826
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1521  [0/3200]
Loss: 0.3411  [1600/3200]
Validation Loss: 0.0412, Accuracy: 78.12%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.1189  [0/3200]
Loss: 0.0309  [1600/3200]
Validation Loss: 0.0410, Accuracy: 78.25%, F1 Score: 0.7806
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0715  [0/3200]
Loss: 0.3020  [1600/3200]
Validation Loss: 0.0475, Accuracy: 75.50%, F1 Score: 0.7540
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.1193  [0/3200]
Loss: 0.2163  [1600/3200]
Validation Loss: 0.0428, Accuracy: 78.88%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.1598  [0/3200]
Loss: 0.0660  [1600/3200]
Validation Loss: 0.0424, Accuracy: 78.75%, F1 Score: 0.7859
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0466  [0/3200]
Loss: 0.0678  [1600/3200]
Validation Loss: 0.0427, Accuracy: 78.00%, F1 Score: 0.7765
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0599  [0/3200]
Loss: 0.0498  [1600/3200]
Validation Loss: 0.0452, Accuracy: 78.38%, F1 Score: 0.7824
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0635  [0/3200]
Loss: 0.1386  [1600/3200]
Validation Loss: 0.0437, Accuracy: 78.75%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0265  [0/3200]
Loss: 0.0248  [1600/3200]
Validation Loss: 0.0455, Accuracy: 79.75%, F1 Score: 0.7948
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0232  [0/3200]
Loss: 0.0594  [1600/3200]
Validation Loss: 0.0512, Accuracy: 77.38%, F1 Score: 0.7651
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0125  [0/3200]
Loss: 0.0411  [1600/3200]
Validation Loss: 0.0514, Accuracy: 76.75%, F1 Score: 0.7609
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0189  [0/3200]
Loss: 0.0047  [1600/3200]
Validation Loss: 0.0474, Accuracy: 78.25%, F1 Score: 0.7824
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.2020  [0/3200]
Loss: 0.0379  [1600/3200]
Validation Loss: 0.0476, Accuracy: 79.25%, F1 Score: 0.7915
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0132  [0/3200]
Loss: 0.0167  [1600/3200]
Validation Loss: 0.0478, Accuracy: 79.00%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0340  [0/3200]
Loss: 0.0616  [1600/3200]
Validation Loss: 0.0498, Accuracy: 78.75%, F1 Score: 0.7828
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0084  [0/3200]
Loss: 0.0021  [1600/3200]
Validation Loss: 0.0485, Accuracy: 79.75%, F1 Score: 0.7963
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0115  [0/3200]
Loss: 0.0166  [1600/3200]
Validation Loss: 0.0502, Accuracy: 78.88%, F1 Score: 0.7878
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0096  [0/3200]
Loss: 0.0151  [1600/3200]
Validation Loss: 0.0525, Accuracy: 77.75%, F1 Score: 0.7743
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0023  [0/3200]
Loss: 0.0077  [1600/3200]
Validation Loss: 0.0573, Accuracy: 76.50%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0017  [0/3200]
Loss: 0.0029  [1600/3200]
Validation Loss: 0.0597, Accuracy: 76.25%, F1 Score: 0.7522
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0219  [0/3200]
Loss: 0.0054  [1600/3200]
Validation Loss: 0.0530, Accuracy: 78.00%, F1 Score: 0.7763
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0033  [0/3200]
Loss: 0.1533  [1600/3200]
Validation Loss: 0.0526, Accuracy: 79.12%, F1 Score: 0.7904
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0185  [0/3200]
Loss: 0.0087  [1600/3200]
Validation Loss: 0.0558, Accuracy: 78.62%, F1 Score: 0.7811
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0008  [0/3200]
Loss: 0.0082  [1600/3200]
Validation Loss: 0.0533, Accuracy: 78.88%, F1 Score: 0.7845
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0076  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.0594, Accuracy: 76.75%, F1 Score: 0.7619
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0021  [0/3200]
Loss: 0.0073  [1600/3200]
Validation Loss: 0.0542, Accuracy: 79.50%, F1 Score: 0.7938
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0194  [0/3200]
Loss: 0.0041  [1600/3200]
Validation Loss: 0.0539, Accuracy: 79.62%, F1 Score: 0.7942
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0601  [0/3200]
Loss: 0.0165  [1600/3200]
Validation Loss: 0.0563, Accuracy: 79.62%, F1 Score: 0.7922
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0022  [0/3200]
Loss: 0.0024  [1600/3200]
Validation Loss: 0.0576, Accuracy: 78.88%, F1 Score: 0.7874
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0072  [0/3200]
Loss: 0.0183  [1600/3200]
Validation Loss: 0.0596, Accuracy: 77.88%, F1 Score: 0.7729
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0039  [0/3200]
Loss: 0.0038  [1600/3200]
Validation Loss: 0.0620, Accuracy: 77.50%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0048  [0/3200]
Loss: 0.0021  [1600/3200]
Validation Loss: 0.0613, Accuracy: 77.75%, F1 Score: 0.7718
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0029  [0/3200]
Loss: 0.0009  [1600/3200]
Validation Loss: 0.0616, Accuracy: 77.38%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0028  [0/3200]
Loss: 0.1026  [1600/3200]
Validation Loss: 0.0589, Accuracy: 79.50%, F1 Score: 0.7906
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0086  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0626, Accuracy: 78.12%, F1 Score: 0.7743
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0052  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0608, Accuracy: 78.25%, F1 Score: 0.7801
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0030  [0/3200]
Loss: 0.0300  [1600/3200]
Validation Loss: 0.0617, Accuracy: 78.75%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0013  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0606, Accuracy: 79.00%, F1 Score: 0.7868
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0007  [0/3200]
Loss: 0.0037  [1600/3200]
Validation Loss: 0.0653, Accuracy: 78.00%, F1 Score: 0.7735
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0005  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0620, Accuracy: 78.50%, F1 Score: 0.7815
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.001 and dropout rate 0.0...
Epoch 1/60
Loss: 1.3065  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 0.8352  [1600/3200]
Validation Loss: 0.0424, Accuracy: 72.25%, F1 Score: 0.6984
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.7371  [0/3200]
Loss: 0.6118  [1600/3200]
Validation Loss: 0.0388, Accuracy: 75.12%, F1 Score: 0.7344
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.4124  [0/3200]
Loss: 0.1752  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7546
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2251  [0/3200]
Loss: 0.1815  [1600/3200]
Validation Loss: 0.0340, Accuracy: 79.62%, F1 Score: 0.7941
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.2541  [0/3200]
Loss: 0.2972  [1600/3200]
Validation Loss: 0.0346, Accuracy: 78.50%, F1 Score: 0.7790
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.2253  [0/3200]
Loss: 0.1051  [1600/3200]
Validation Loss: 0.0354, Accuracy: 78.50%, F1 Score: 0.7801
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.3036  [0/3200]
Loss: 0.2070  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.00%, F1 Score: 0.7807
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.0758  [0/3200]
Loss: 0.1466  [1600/3200]
Validation Loss: 0.0368, Accuracy: 78.62%, F1 Score: 0.7843
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.0782  [0/3200]
Loss: 0.0582  [1600/3200]
Validation Loss: 0.0374, Accuracy: 77.62%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.0287  [0/3200]
Loss: 0.0645  [1600/3200]
Validation Loss: 0.0400, Accuracy: 77.12%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.0695  [0/3200]
Loss: 0.0102  [1600/3200]
Validation Loss: 0.0469, Accuracy: 74.75%, F1 Score: 0.7371
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.0225  [0/3200]
Loss: 0.0443  [1600/3200]
Validation Loss: 0.0394, Accuracy: 79.88%, F1 Score: 0.7983
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.0329  [0/3200]
Loss: 0.0597  [1600/3200]
Validation Loss: 0.0428, Accuracy: 77.12%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.0208  [0/3200]
Loss: 0.0196  [1600/3200]
Validation Loss: 0.0440, Accuracy: 76.38%, F1 Score: 0.7581
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.0106  [0/3200]
Loss: 0.0196  [1600/3200]
Validation Loss: 0.0441, Accuracy: 78.12%, F1 Score: 0.7799
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.0106  [0/3200]
Loss: 0.1420  [1600/3200]
Validation Loss: 0.0476, Accuracy: 77.50%, F1 Score: 0.7714
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.0085  [0/3200]
Loss: 0.0076  [1600/3200]
Validation Loss: 0.0460, Accuracy: 78.25%, F1 Score: 0.7824
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.0433  [0/3200]
Loss: 0.0137  [1600/3200]
Validation Loss: 0.0478, Accuracy: 77.88%, F1 Score: 0.7764
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0046  [0/3200]
Loss: 0.0076  [1600/3200]
Validation Loss: 0.0492, Accuracy: 78.25%, F1 Score: 0.7816
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0038  [0/3200]
Loss: 0.0023  [1600/3200]
Validation Loss: 0.0611, Accuracy: 74.50%, F1 Score: 0.7396
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.0088  [0/3200]
Loss: 0.0174  [1600/3200]
Validation Loss: 0.0521, Accuracy: 76.62%, F1 Score: 0.7602
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.0047  [0/3200]
Loss: 0.0024  [1600/3200]
Validation Loss: 0.0533, Accuracy: 76.50%, F1 Score: 0.7574
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0023  [0/3200]
Loss: 0.0133  [1600/3200]
Validation Loss: 0.0531, Accuracy: 76.38%, F1 Score: 0.7589
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0234  [0/3200]
Loss: 0.0030  [1600/3200]
Validation Loss: 0.0508, Accuracy: 77.62%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0064  [0/3200]
Loss: 0.0037  [1600/3200]
Validation Loss: 0.0511, Accuracy: 77.75%, F1 Score: 0.7749
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0008  [0/3200]
Loss: 0.0031  [1600/3200]
Validation Loss: 0.0507, Accuracy: 78.12%, F1 Score: 0.7763
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0013  [0/3200]
Loss: 0.0014  [1600/3200]
Validation Loss: 0.0551, Accuracy: 77.12%, F1 Score: 0.7682
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0215  [0/3200]
Loss: 0.0149  [1600/3200]
Validation Loss: 0.0533, Accuracy: 77.50%, F1 Score: 0.7716
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0019  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0535, Accuracy: 76.62%, F1 Score: 0.7579
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0028  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0578, Accuracy: 75.25%, F1 Score: 0.7428
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0010  [0/3200]
Loss: 0.0024  [1600/3200]
Validation Loss: 0.0587, Accuracy: 76.75%, F1 Score: 0.7642
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0031  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0552, Accuracy: 77.50%, F1 Score: 0.7713
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.0051  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0582, Accuracy: 76.62%, F1 Score: 0.7621
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0011  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0575, Accuracy: 76.62%, F1 Score: 0.7638
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0005  [0/3200]
Loss: 0.0037  [1600/3200]
Validation Loss: 0.0580, Accuracy: 77.25%, F1 Score: 0.7662
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0007  [0/3200]
Loss: 0.0013  [1600/3200]
Validation Loss: 0.0554, Accuracy: 78.00%, F1 Score: 0.7788
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0003  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0575, Accuracy: 77.12%, F1 Score: 0.7659
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0006  [0/3200]
Loss: 0.0022  [1600/3200]
Validation Loss: 0.0612, Accuracy: 76.00%, F1 Score: 0.7548
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0011  [0/3200]
Loss: 0.0017  [1600/3200]
Validation Loss: 0.0590, Accuracy: 77.00%, F1 Score: 0.7653
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0002  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0564, Accuracy: 77.50%, F1 Score: 0.7715
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0013  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0561, Accuracy: 77.00%, F1 Score: 0.7671
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0007  [0/3200]
Loss: 0.0228  [1600/3200]
Validation Loss: 0.0593, Accuracy: 76.88%, F1 Score: 0.7656
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0022  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0648, Accuracy: 75.75%, F1 Score: 0.7497
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0004  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0582, Accuracy: 76.75%, F1 Score: 0.7655
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0033  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0671, Accuracy: 74.62%, F1 Score: 0.7363
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0011  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0630, Accuracy: 75.75%, F1 Score: 0.7495
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0023  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0598, Accuracy: 76.00%, F1 Score: 0.7528
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0009  [0/3200]
Loss: 0.0089  [1600/3200]
Validation Loss: 0.0546, Accuracy: 77.88%, F1 Score: 0.7784
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0004  [0/3200]
Loss: 0.0087  [1600/3200]
Validation Loss: 0.0593, Accuracy: 76.50%, F1 Score: 0.7584
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0005  [0/3200]
Loss: 0.0075  [1600/3200]
Validation Loss: 0.0611, Accuracy: 76.75%, F1 Score: 0.7576
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0006  [0/3200]
Loss: 0.0031  [1600/3200]
Validation Loss: 0.0555, Accuracy: 77.50%, F1 Score: 0.7734
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0010  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0556, Accuracy: 77.38%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0007  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0573, Accuracy: 77.62%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0009  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0604, Accuracy: 76.75%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0011  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0565, Accuracy: 78.00%, F1 Score: 0.7757
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0004  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0535, Accuracy: 78.75%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0016  [0/3200]
Loss: 0.0020  [1600/3200]
Validation Loss: 0.0604, Accuracy: 76.75%, F1 Score: 0.7633
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0024  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0542, Accuracy: 78.25%, F1 Score: 0.7799
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0014  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0600, Accuracy: 76.25%, F1 Score: 0.7573
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0010  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0575, Accuracy: 77.25%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.001 and dropout rate 0.2...
Epoch 1/60
Loss: 1.3219  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.0077  [1600/3200]
Validation Loss: 0.0444, Accuracy: 67.88%, F1 Score: 0.6509
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.7946  [0/3200]
Loss: 0.8556  [1600/3200]
Validation Loss: 0.0400, Accuracy: 74.12%, F1 Score: 0.7303
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.4763  [0/3200]
Loss: 0.2667  [1600/3200]
Validation Loss: 0.0373, Accuracy: 76.25%, F1 Score: 0.7496
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2234  [0/3200]
Loss: 0.1973  [1600/3200]
Validation Loss: 0.0357, Accuracy: 79.38%, F1 Score: 0.7890
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.3166  [0/3200]
Loss: 0.5821  [1600/3200]
Validation Loss: 0.0350, Accuracy: 77.62%, F1 Score: 0.7687
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.4569  [0/3200]
Loss: 0.1554  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.62%, F1 Score: 0.7924
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.4251  [0/3200]
Loss: 0.2781  [1600/3200]
Validation Loss: 0.0363, Accuracy: 77.62%, F1 Score: 0.7705
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.1359  [0/3200]
Loss: 0.2027  [1600/3200]
Validation Loss: 0.0366, Accuracy: 78.00%, F1 Score: 0.7773
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.2852  [0/3200]
Loss: 0.1039  [1600/3200]
Validation Loss: 0.0359, Accuracy: 79.12%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.0581  [0/3200]
Loss: 0.1108  [1600/3200]
Validation Loss: 0.0398, Accuracy: 77.00%, F1 Score: 0.7693
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.2121  [0/3200]
Loss: 0.0643  [1600/3200]
Validation Loss: 0.0415, Accuracy: 76.25%, F1 Score: 0.7548
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.0379  [0/3200]
Loss: 0.0569  [1600/3200]
Validation Loss: 0.0388, Accuracy: 79.50%, F1 Score: 0.7937
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.0880  [0/3200]
Loss: 0.0612  [1600/3200]
Validation Loss: 0.0436, Accuracy: 76.62%, F1 Score: 0.7580
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.1390  [0/3200]
Loss: 0.0376  [1600/3200]
Validation Loss: 0.0416, Accuracy: 77.25%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.0262  [0/3200]
Loss: 0.0504  [1600/3200]
Validation Loss: 0.0428, Accuracy: 77.12%, F1 Score: 0.7677
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.0523  [0/3200]
Loss: 0.2239  [1600/3200]
Validation Loss: 0.0463, Accuracy: 77.00%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.0493  [0/3200]
Loss: 0.0244  [1600/3200]
Validation Loss: 0.0461, Accuracy: 77.62%, F1 Score: 0.7710
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.0648  [0/3200]
Loss: 0.0384  [1600/3200]
Validation Loss: 0.0494, Accuracy: 76.25%, F1 Score: 0.7556
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0188  [0/3200]
Loss: 0.0441  [1600/3200]
Validation Loss: 0.0475, Accuracy: 76.62%, F1 Score: 0.7661
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0127  [0/3200]
Loss: 0.0212  [1600/3200]
Validation Loss: 0.0510, Accuracy: 76.38%, F1 Score: 0.7607
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.0175  [0/3200]
Loss: 0.0521  [1600/3200]
Validation Loss: 0.0491, Accuracy: 77.50%, F1 Score: 0.7716
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.0249  [0/3200]
Loss: 0.0084  [1600/3200]
Validation Loss: 0.0494, Accuracy: 77.50%, F1 Score: 0.7715
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0066  [0/3200]
Loss: 0.0844  [1600/3200]
Validation Loss: 0.0523, Accuracy: 76.00%, F1 Score: 0.7577
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0269  [0/3200]
Loss: 0.0143  [1600/3200]
Validation Loss: 0.0500, Accuracy: 77.50%, F1 Score: 0.7717
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0276  [0/3200]
Loss: 0.0076  [1600/3200]
Validation Loss: 0.0500, Accuracy: 77.38%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0040  [0/3200]
Loss: 0.0081  [1600/3200]
Validation Loss: 0.0493, Accuracy: 77.62%, F1 Score: 0.7747
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0051  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.0548, Accuracy: 75.88%, F1 Score: 0.7538
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0221  [0/3200]
Loss: 0.0336  [1600/3200]
Validation Loss: 0.0519, Accuracy: 77.50%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0079  [0/3200]
Loss: 0.0014  [1600/3200]
Validation Loss: 0.0523, Accuracy: 77.88%, F1 Score: 0.7736
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0047  [0/3200]
Loss: 0.0106  [1600/3200]
Validation Loss: 0.0587, Accuracy: 76.00%, F1 Score: 0.7501
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0019  [0/3200]
Loss: 0.0113  [1600/3200]
Validation Loss: 0.0534, Accuracy: 77.62%, F1 Score: 0.7736
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0031  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0524, Accuracy: 77.50%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.0122  [0/3200]
Loss: 0.0023  [1600/3200]
Validation Loss: 0.0556, Accuracy: 77.88%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0008  [0/3200]
Loss: 0.0021  [1600/3200]
Validation Loss: 0.0534, Accuracy: 79.50%, F1 Score: 0.7915
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0036  [0/3200]
Loss: 0.0054  [1600/3200]
Validation Loss: 0.0606, Accuracy: 76.12%, F1 Score: 0.7507
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0007  [0/3200]
Loss: 0.0015  [1600/3200]
Validation Loss: 0.0559, Accuracy: 77.88%, F1 Score: 0.7768
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0004  [0/3200]
Loss: 0.0034  [1600/3200]
Validation Loss: 0.0564, Accuracy: 77.75%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0026  [0/3200]
Loss: 0.0027  [1600/3200]
Validation Loss: 0.0575, Accuracy: 77.62%, F1 Score: 0.7729
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0007  [0/3200]
Loss: 0.0035  [1600/3200]
Validation Loss: 0.0574, Accuracy: 77.50%, F1 Score: 0.7713
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0005  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0615, Accuracy: 77.62%, F1 Score: 0.7652
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0016  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0547, Accuracy: 79.25%, F1 Score: 0.7911
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0006  [0/3200]
Loss: 0.0791  [1600/3200]
Validation Loss: 0.0640, Accuracy: 75.75%, F1 Score: 0.7494
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0004  [0/3200]
Loss: 0.0127  [1600/3200]
Validation Loss: 0.0626, Accuracy: 77.12%, F1 Score: 0.7624
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0010  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0556, Accuracy: 78.50%, F1 Score: 0.7815
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0030  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0605, Accuracy: 77.50%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0010  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0557, Accuracy: 77.62%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0018  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0586, Accuracy: 76.88%, F1 Score: 0.7655
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0020  [0/3200]
Loss: 0.0055  [1600/3200]
Validation Loss: 0.0578, Accuracy: 77.38%, F1 Score: 0.7702
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0006  [0/3200]
Loss: 0.0003  [1600/3200]
Validation Loss: 0.0628, Accuracy: 76.75%, F1 Score: 0.7628
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0003  [0/3200]
Loss: 0.0077  [1600/3200]
Validation Loss: 0.0680, Accuracy: 75.88%, F1 Score: 0.7468
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0010  [0/3200]
Loss: 0.0033  [1600/3200]
Validation Loss: 0.0618, Accuracy: 77.12%, F1 Score: 0.7662
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0024  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0615, Accuracy: 77.50%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0006  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0648, Accuracy: 75.88%, F1 Score: 0.7549
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0005  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0618, Accuracy: 76.88%, F1 Score: 0.7622
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0015  [0/3200]
Loss: 0.0006  [1600/3200]
Validation Loss: 0.0629, Accuracy: 77.38%, F1 Score: 0.7659
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0003  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0610, Accuracy: 77.50%, F1 Score: 0.7733
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0015  [0/3200]
Loss: 0.0025  [1600/3200]
Validation Loss: 0.0607, Accuracy: 78.00%, F1 Score: 0.7769
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0027  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0602, Accuracy: 77.62%, F1 Score: 0.7719
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0015  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0625, Accuracy: 76.62%, F1 Score: 0.7611
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0004  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0610, Accuracy: 76.88%, F1 Score: 0.7649
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.001 and dropout rate 0.5...
Epoch 1/60
Loss: 1.4068  [0/3200]

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.0236  [1600/3200]
Validation Loss: 0.0480, Accuracy: 67.00%, F1 Score: 0.6497
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0662  [0/3200]
Loss: 0.9476  [1600/3200]
Validation Loss: 0.0451, Accuracy: 69.25%, F1 Score: 0.6772
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.8262  [0/3200]
Loss: 0.5156  [1600/3200]
Validation Loss: 0.0392, Accuracy: 75.12%, F1 Score: 0.7276
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.2991  [0/3200]
Loss: 0.2900  [1600/3200]
Validation Loss: 0.0357, Accuracy: 77.62%, F1 Score: 0.7723
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.7764  [0/3200]
Loss: 0.6466  [1600/3200]
Validation Loss: 0.0350, Accuracy: 76.38%, F1 Score: 0.7606
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.6978  [0/3200]
Loss: 0.3831  [1600/3200]
Validation Loss: 0.0372, Accuracy: 75.38%, F1 Score: 0.7578
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.6366  [0/3200]
Loss: 0.4142  [1600/3200]
Validation Loss: 0.0372, Accuracy: 75.25%, F1 Score: 0.7420
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3726  [0/3200]
Loss: 0.3985  [1600/3200]
Validation Loss: 0.0358, Accuracy: 78.88%, F1 Score: 0.7805
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.4483  [0/3200]
Loss: 0.2709  [1600/3200]
Validation Loss: 0.0343, Accuracy: 79.50%, F1 Score: 0.7901
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2386  [0/3200]
Loss: 0.2165  [1600/3200]
Validation Loss: 0.0348, Accuracy: 78.38%, F1 Score: 0.7827
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.4264  [0/3200]
Loss: 0.1825  [1600/3200]
Validation Loss: 0.0349, Accuracy: 79.38%, F1 Score: 0.7875
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2433  [0/3200]
Loss: 0.2361  [1600/3200]
Validation Loss: 0.0377, Accuracy: 78.50%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3297  [0/3200]
Loss: 0.2519  [1600/3200]
Validation Loss: 0.0375, Accuracy: 78.00%, F1 Score: 0.7702
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.3212  [0/3200]
Loss: 0.2088  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.25%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.2767  [0/3200]
Loss: 0.3777  [1600/3200]
Validation Loss: 0.0365, Accuracy: 78.88%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.2504  [0/3200]
Loss: 0.2806  [1600/3200]
Validation Loss: 0.0388, Accuracy: 77.75%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.1587  [0/3200]
Loss: 0.1916  [1600/3200]
Validation Loss: 0.0377, Accuracy: 79.25%, F1 Score: 0.7918
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.2037  [0/3200]
Loss: 0.1092  [1600/3200]
Validation Loss: 0.0396, Accuracy: 78.38%, F1 Score: 0.7779
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.0799  [0/3200]
Loss: 0.2437  [1600/3200]
Validation Loss: 0.0384, Accuracy: 77.75%, F1 Score: 0.7747
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.1137  [0/3200]
Loss: 0.0520  [1600/3200]
Validation Loss: 0.0409, Accuracy: 78.12%, F1 Score: 0.7800
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1312  [0/3200]
Loss: 0.4096  [1600/3200]
Validation Loss: 0.0440, Accuracy: 77.62%, F1 Score: 0.7662
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.1700  [0/3200]
Loss: 0.0452  [1600/3200]
Validation Loss: 0.0414, Accuracy: 78.12%, F1 Score: 0.7796
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0455  [0/3200]
Loss: 0.3180  [1600/3200]
Validation Loss: 0.0479, Accuracy: 75.50%, F1 Score: 0.7535
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.1593  [0/3200]
Loss: 0.1994  [1600/3200]
Validation Loss: 0.0433, Accuracy: 77.88%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.1223  [0/3200]
Loss: 0.0791  [1600/3200]
Validation Loss: 0.0430, Accuracy: 78.50%, F1 Score: 0.7813
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0187  [0/3200]
Loss: 0.0437  [1600/3200]
Validation Loss: 0.0440, Accuracy: 78.25%, F1 Score: 0.7783
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0317  [0/3200]
Loss: 0.0415  [1600/3200]
Validation Loss: 0.0445, Accuracy: 78.12%, F1 Score: 0.7797
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0403  [0/3200]
Loss: 0.1193  [1600/3200]
Validation Loss: 0.0450, Accuracy: 78.00%, F1 Score: 0.7768
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0201  [0/3200]
Loss: 0.0341  [1600/3200]
Validation Loss: 0.0460, Accuracy: 78.88%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0249  [0/3200]
Loss: 0.0634  [1600/3200]
Validation Loss: 0.0542, Accuracy: 76.88%, F1 Score: 0.7585
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0127  [0/3200]
Loss: 0.0411  [1600/3200]
Validation Loss: 0.0520, Accuracy: 77.62%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0104  [0/3200]
Loss: 0.0098  [1600/3200]
Validation Loss: 0.0472, Accuracy: 78.75%, F1 Score: 0.7872
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.1072  [0/3200]
Loss: 0.0148  [1600/3200]
Validation Loss: 0.0491, Accuracy: 78.75%, F1 Score: 0.7847
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0059  [0/3200]
Loss: 0.0194  [1600/3200]
Validation Loss: 0.0489, Accuracy: 78.25%, F1 Score: 0.7816
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0194  [0/3200]
Loss: 0.0644  [1600/3200]
Validation Loss: 0.0508, Accuracy: 78.25%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0080  [0/3200]
Loss: 0.0023  [1600/3200]
Validation Loss: 0.0501, Accuracy: 78.00%, F1 Score: 0.7762
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0526  [0/3200]
Loss: 0.0147  [1600/3200]
Validation Loss: 0.0517, Accuracy: 77.88%, F1 Score: 0.7764
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0187  [0/3200]
Loss: 0.0572  [1600/3200]
Validation Loss: 0.0546, Accuracy: 78.00%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0031  [0/3200]
Loss: 0.0298  [1600/3200]
Validation Loss: 0.0597, Accuracy: 76.25%, F1 Score: 0.7578
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0025  [0/3200]
Loss: 0.0048  [1600/3200]
Validation Loss: 0.0623, Accuracy: 76.25%, F1 Score: 0.7518
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0131  [0/3200]
Loss: 0.0025  [1600/3200]
Validation Loss: 0.0550, Accuracy: 76.75%, F1 Score: 0.7620
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0046  [0/3200]
Loss: 0.0565  [1600/3200]
Validation Loss: 0.0542, Accuracy: 77.50%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0158  [0/3200]
Loss: 0.0131  [1600/3200]
Validation Loss: 0.0604, Accuracy: 76.25%, F1 Score: 0.7540
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0017  [0/3200]
Loss: 0.0074  [1600/3200]
Validation Loss: 0.0526, Accuracy: 79.38%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0044  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0571, Accuracy: 78.38%, F1 Score: 0.7805
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0004  [0/3200]
Loss: 0.0093  [1600/3200]
Validation Loss: 0.0547, Accuracy: 79.38%, F1 Score: 0.7933
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0165  [0/3200]
Loss: 0.0036  [1600/3200]
Validation Loss: 0.0560, Accuracy: 78.62%, F1 Score: 0.7841
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0354  [0/3200]
Loss: 0.0043  [1600/3200]
Validation Loss: 0.0568, Accuracy: 78.75%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0227  [0/3200]
Loss: 0.0017  [1600/3200]
Validation Loss: 0.0587, Accuracy: 77.38%, F1 Score: 0.7706
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0054  [0/3200]
Loss: 0.0049  [1600/3200]
Validation Loss: 0.0613, Accuracy: 78.25%, F1 Score: 0.7767
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0020  [0/3200]
Loss: 0.0098  [1600/3200]
Validation Loss: 0.0643, Accuracy: 76.62%, F1 Score: 0.7535
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0056  [0/3200]
Loss: 0.0019  [1600/3200]
Validation Loss: 0.0573, Accuracy: 78.62%, F1 Score: 0.7822
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0016  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0631, Accuracy: 77.25%, F1 Score: 0.7695
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0055  [0/3200]
Loss: 0.0769  [1600/3200]
Validation Loss: 0.0577, Accuracy: 78.62%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0180  [0/3200]
Loss: 0.0009  [1600/3200]
Validation Loss: 0.0625, Accuracy: 77.25%, F1 Score: 0.7625
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0070  [0/3200]
Loss: 0.0026  [1600/3200]
Validation Loss: 0.0600, Accuracy: 77.62%, F1 Score: 0.7743
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0024  [0/3200]
Loss: 0.0117  [1600/3200]
Validation Loss: 0.0595, Accuracy: 78.62%, F1 Score: 0.7829
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0012  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0578, Accuracy: 79.38%, F1 Score: 0.7913
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0019  [0/3200]
Loss: 0.0060  [1600/3200]
Validation Loss: 0.0616, Accuracy: 77.62%, F1 Score: 0.7698
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0004  [0/3200]
Loss: 0.0004  [1600/3200]
Validation Loss: 0.0603, Accuracy: 78.25%, F1 Score: 0.7782
Adjusting learning rate of group 0 to 8.1316e-05.

<ipython-input-39-9d9b5d469e1b>:105: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

results_df

Εφαρμογή Regularization στο βέλτιστο μέχρι τώρα μοντέλο.

# Define the list of learning rates and corresponding scheduler names
lr = 2e-3
scheduler_name = 'OneCycleLR'
weight_decays = [0.0, 1e-4, 1e-3]
dropout_rates = [0.0, 0.2, 0.5]
epochs = 60

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=['Weight Decay', 'Dropout Rate', 'Best Accuracy', 'Best F1 Score'])

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Initialize the model with Mish activation and no BatchNorm2d layers
class CNN_pp2(nn.Module):
    def __init__(self, output_dim, activation, dropout_rate):
        super(CNN_pp2, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)

        # Fully connected (dense) layers
        self.fc1 = nn.Linear(4096, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 32)
        self.fc4 = nn.Linear(32, output_dim)

        # Activation function
        self.activation = activation

        # Dropout layer
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, x):
        # Convolutional layers
        x = self.activation(self.conv1(x))
        x = self.maxpool1(x)
        x = self.activation(self.conv2(x))
        x = self.maxpool2(x)
        x = self.activation(self.conv3(x))
        x = self.maxpool3(x)
        x = self.activation(self.conv4(x))

        # Flatten the tensor for the fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = self.activation(self.fc1(x))
        x = self.dropout(x)
        x = self.activation(self.fc2(x))
        x = self.dropout(x)
        x = self.activation(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)

        return x

# Iterate over weight decays and dropout rates
for weight_decay in weight_decays:
    for dropout_rate in dropout_rates:
        # Set the random seeds for reproducibility within the loop
        torch.manual_seed(SEED)
        random.seed(SEED)
        np.random.seed(SEED)
        torch.cuda.manual_seed_all(SEED)

        # Initialize the model
        model = CNN_pp2(output_dim=4, activation=nn.Mish(), dropout_rate=dropout_rate).to(device)
        model.apply(weights_init)  # Initialize the weights

        # Create the optimizer with Adamax and weight decay
        optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=weight_decay)

        # Create the scheduler based on the scheduler name
        if scheduler_name == 'CosineAnnealingLR':
            scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr, verbose=True)
        elif scheduler_name == 'OneCycleLR':
            scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader), verbose=True)
        elif scheduler_name == 'LambdaLR':
            scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch, verbose=True)
        elif scheduler_name == 'PolynomialLR':
            scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9, verbose=True)
        elif scheduler_name == 'MultiplicativeLR':
            scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.9, verbose=True)

        # Train the model
        print(f"Training with weight decay {weight_decay} and dropout rate {dropout_rate}...")
        best_accuracy = 0.0
        best_f1 = 0.0
        for epoch in range(epochs):
            print(f"Epoch {epoch+1}/{epochs}")
            train_loop(train_loader, model, loss_fn, optimizer)
            test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
            print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

            # Update the best accuracy and F1 score
            if accuracy > best_accuracy:
                best_accuracy = accuracy
            if f1 > best_f1:
                best_f1 = f1

            scheduler.step()  # Update the learning rate

        # Add the results to the DataFrame
        results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,
                                        'Best Accuracy': best_accuracy, 'Best F1 Score': best_f1}, ignore_index=True)


Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0 and dropout rate 0.0...
Epoch 1/60
Loss: 1.8130  [0/3200]
Loss: 1.0082  [1600/3200]
Validation Loss: 0.0518, Accuracy: 64.12%, F1 Score: 0.6167
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.8653  [0/3200]
Loss: 0.8663  [1600/3200]
Validation Loss: 0.0503, Accuracy: 64.62%, F1 Score: 0.6125
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.5948  [0/3200]
Loss: 0.5598  [1600/3200]
Validation Loss: 0.0416, Accuracy: 72.38%, F1 Score: 0.7242
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.5666  [0/3200]
Loss: 0.5504  [1600/3200]
Validation Loss: 0.0410, Accuracy: 75.25%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.5307  [0/3200]
Loss: 0.6684  [1600/3200]
Validation Loss: 0.0378, Accuracy: 76.00%, F1 Score: 0.7492
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.0975  [0/3200]
Loss: 0.4646  [1600/3200]
Validation Loss: 0.0378, Accuracy: 77.00%, F1 Score: 0.7683
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5269  [0/3200]
Loss: 0.5878  [1600/3200]
Validation Loss: 0.0364, Accuracy: 79.75%, F1 Score: 0.7944
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3505  [0/3200]
Loss: 0.5043  [1600/3200]
Validation Loss: 0.0362, Accuracy: 77.88%, F1 Score: 0.7816
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.4426  [0/3200]
Loss: 0.4681  [1600/3200]
Validation Loss: 0.0340, Accuracy: 80.75%, F1 Score: 0.8049
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.3952  [0/3200]
Loss: 0.4945  [1600/3200]
Validation Loss: 0.0414, Accuracy: 75.75%, F1 Score: 0.7619
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.5319  [0/3200]
Loss: 0.1808  [1600/3200]
Validation Loss: 0.0371, Accuracy: 78.25%, F1 Score: 0.7773
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.3071  [0/3200]
Loss: 0.1682  [1600/3200]
Validation Loss: 0.0384, Accuracy: 77.75%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3599  [0/3200]
Loss: 0.3353  [1600/3200]
Validation Loss: 0.0429, Accuracy: 74.88%, F1 Score: 0.7411
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.5915  [0/3200]
Loss: 0.2790  [1600/3200]
Validation Loss: 0.0481, Accuracy: 70.12%, F1 Score: 0.7041
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.5867  [0/3200]
Loss: 0.4311  [1600/3200]
Validation Loss: 0.0342, Accuracy: 80.00%, F1 Score: 0.8003
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.3286  [0/3200]
Loss: 0.4651  [1600/3200]
Validation Loss: 0.0345, Accuracy: 80.88%, F1 Score: 0.8062
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.2257  [0/3200]
Loss: 0.2394  [1600/3200]
Validation Loss: 0.0350, Accuracy: 80.62%, F1 Score: 0.8026
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.2997  [0/3200]
Loss: 0.1942  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.88%, F1 Score: 0.7999
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.2544  [0/3200]
Loss: 0.3014  [1600/3200]
Validation Loss: 0.0374, Accuracy: 80.75%, F1 Score: 0.8072
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.3391  [0/3200]
Loss: 0.0816  [1600/3200]
Validation Loss: 0.0353, Accuracy: 80.88%, F1 Score: 0.8080
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1816  [0/3200]
Loss: 0.4445  [1600/3200]
Validation Loss: 0.0420, Accuracy: 75.88%, F1 Score: 0.7416
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.3777  [0/3200]
Loss: 0.4136  [1600/3200]
Validation Loss: 0.0412, Accuracy: 78.00%, F1 Score: 0.7810
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.3346  [0/3200]
Loss: 0.0474  [1600/3200]
Validation Loss: 0.0397, Accuracy: 79.38%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.1614  [0/3200]
Loss: 0.4020  [1600/3200]
Validation Loss: 0.0398, Accuracy: 77.75%, F1 Score: 0.7743
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0967  [0/3200]
Loss: 0.2835  [1600/3200]
Validation Loss: 0.0405, Accuracy: 76.88%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.1596  [0/3200]
Loss: 0.0518  [1600/3200]
Validation Loss: 0.0471, Accuracy: 76.00%, F1 Score: 0.7525
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.1848  [0/3200]
Loss: 0.2479  [1600/3200]
Validation Loss: 0.0466, Accuracy: 75.25%, F1 Score: 0.7485
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.1496  [0/3200]
Loss: 0.1010  [1600/3200]
Validation Loss: 0.0454, Accuracy: 75.75%, F1 Score: 0.7541
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.1436  [0/3200]
Loss: 0.0924  [1600/3200]
Validation Loss: 0.0575, Accuracy: 74.50%, F1 Score: 0.7271
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0935  [0/3200]
Loss: 0.0777  [1600/3200]
Validation Loss: 0.0439, Accuracy: 78.00%, F1 Score: 0.7784
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0857  [0/3200]
Loss: 0.1500  [1600/3200]
Validation Loss: 0.0549, Accuracy: 74.12%, F1 Score: 0.7282
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0549  [0/3200]
Loss: 0.0247  [1600/3200]
Validation Loss: 0.0556, Accuracy: 73.62%, F1 Score: 0.7313
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.2466  [0/3200]
Loss: 0.0172  [1600/3200]
Validation Loss: 0.0520, Accuracy: 75.50%, F1 Score: 0.7480
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0358  [0/3200]
Loss: 0.1309  [1600/3200]
Validation Loss: 0.0462, Accuracy: 79.75%, F1 Score: 0.7976
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0632  [0/3200]
Loss: 0.0166  [1600/3200]
Validation Loss: 0.0552, Accuracy: 75.88%, F1 Score: 0.7490
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0631  [0/3200]
Loss: 0.0138  [1600/3200]
Validation Loss: 0.0515, Accuracy: 77.62%, F1 Score: 0.7703
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0553  [0/3200]
Loss: 0.0613  [1600/3200]
Validation Loss: 0.0538, Accuracy: 77.00%, F1 Score: 0.7612
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0045  [0/3200]
Loss: 0.0156  [1600/3200]
Validation Loss: 0.0510, Accuracy: 79.62%, F1 Score: 0.7932
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0198  [0/3200]
Loss: 0.0162  [1600/3200]
Validation Loss: 0.0525, Accuracy: 79.12%, F1 Score: 0.7903
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0079  [0/3200]
Loss: 0.0038  [1600/3200]
Validation Loss: 0.0609, Accuracy: 76.38%, F1 Score: 0.7559
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0418  [0/3200]
Loss: 0.0281  [1600/3200]
Validation Loss: 0.0549, Accuracy: 79.62%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0109  [0/3200]
Loss: 0.0350  [1600/3200]
Validation Loss: 0.0577, Accuracy: 77.38%, F1 Score: 0.7713
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0231  [0/3200]
Loss: 0.0162  [1600/3200]
Validation Loss: 0.0603, Accuracy: 77.62%, F1 Score: 0.7701
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0020  [0/3200]
Loss: 0.0055  [1600/3200]
Validation Loss: 0.0570, Accuracy: 78.62%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0098  [0/3200]
Loss: 0.0038  [1600/3200]
Validation Loss: 0.0707, Accuracy: 75.12%, F1 Score: 0.7394
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0100  [0/3200]
Loss: 0.0068  [1600/3200]
Validation Loss: 0.0579, Accuracy: 78.62%, F1 Score: 0.7842
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0101  [0/3200]
Loss: 0.0185  [1600/3200]
Validation Loss: 0.0623, Accuracy: 78.12%, F1 Score: 0.7786
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0039  [0/3200]
Loss: 0.0034  [1600/3200]
Validation Loss: 0.0636, Accuracy: 77.50%, F1 Score: 0.7726
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0122  [0/3200]
Loss: 0.0027  [1600/3200]
Validation Loss: 0.0618, Accuracy: 78.50%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0045  [0/3200]
Loss: 0.0036  [1600/3200]
Validation Loss: 0.0636, Accuracy: 77.88%, F1 Score: 0.7776
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0056  [0/3200]
Loss: 0.0048  [1600/3200]
Validation Loss: 0.0639, Accuracy: 78.25%, F1 Score: 0.7807
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0054  [0/3200]
Loss: 0.0034  [1600/3200]
Validation Loss: 0.0666, Accuracy: 77.88%, F1 Score: 0.7746
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0037  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0763, Accuracy: 75.62%, F1 Score: 0.7509
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0025  [0/3200]
Loss: 0.0038  [1600/3200]
Validation Loss: 0.0658, Accuracy: 78.62%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0024  [0/3200]
Loss: 0.0046  [1600/3200]
Validation Loss: 0.0746, Accuracy: 76.38%, F1 Score: 0.7576
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0020  [0/3200]
Loss: 0.0059  [1600/3200]
Validation Loss: 0.0738, Accuracy: 76.38%, F1 Score: 0.7592
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0155  [0/3200]
Loss: 0.0024  [1600/3200]
Validation Loss: 0.0697, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0025  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0703, Accuracy: 78.62%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0011  [0/3200]
Loss: 0.0024  [1600/3200]
Validation Loss: 0.0749, Accuracy: 76.62%, F1 Score: 0.7640
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0029  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0722, Accuracy: 78.38%, F1 Score: 0.7794
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0 and dropout rate 0.2...
Epoch 1/60
Loss: 2.1009  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.2447  [1600/3200]
Validation Loss: 0.0694, Accuracy: 56.88%, F1 Score: 0.5045
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.4183  [0/3200]
Loss: 1.0985  [1600/3200]
Validation Loss: 0.0514, Accuracy: 69.75%, F1 Score: 0.6683
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.9445  [0/3200]
Loss: 0.7218  [1600/3200]
Validation Loss: 0.0448, Accuracy: 73.25%, F1 Score: 0.7172
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.6412  [0/3200]
Loss: 0.6236  [1600/3200]
Validation Loss: 0.0417, Accuracy: 73.38%, F1 Score: 0.7272
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.5705  [0/3200]
Loss: 0.7258  [1600/3200]
Validation Loss: 0.0393, Accuracy: 73.38%, F1 Score: 0.7103
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.1585  [0/3200]
Loss: 0.4476  [1600/3200]
Validation Loss: 0.0353, Accuracy: 77.88%, F1 Score: 0.7765
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5833  [0/3200]
Loss: 0.6635  [1600/3200]
Validation Loss: 0.0381, Accuracy: 73.25%, F1 Score: 0.7080
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.4468  [0/3200]
Loss: 0.5426  [1600/3200]
Validation Loss: 0.0349, Accuracy: 78.50%, F1 Score: 0.7813
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.6936  [0/3200]
Loss: 0.4917  [1600/3200]
Validation Loss: 0.0349, Accuracy: 79.50%, F1 Score: 0.7901
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.5005  [0/3200]
Loss: 0.5434  [1600/3200]
Validation Loss: 0.0382, Accuracy: 75.38%, F1 Score: 0.7511
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.7219  [0/3200]
Loss: 0.2532  [1600/3200]
Validation Loss: 0.0338, Accuracy: 78.12%, F1 Score: 0.7796
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.4617  [0/3200]
Loss: 0.2051  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.62%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.6114  [0/3200]
Loss: 0.4548  [1600/3200]
Validation Loss: 0.0342, Accuracy: 79.00%, F1 Score: 0.7889
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.4657  [0/3200]
Loss: 0.2871  [1600/3200]
Validation Loss: 0.0336, Accuracy: 79.75%, F1 Score: 0.7996
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.5932  [0/3200]
Loss: 0.7370  [1600/3200]
Validation Loss: 0.0329, Accuracy: 80.25%, F1 Score: 0.8026
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.5350  [0/3200]
Loss: 0.3063  [1600/3200]
Validation Loss: 0.0359, Accuracy: 77.62%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.2612  [0/3200]
Loss: 0.3148  [1600/3200]
Validation Loss: 0.0330, Accuracy: 81.00%, F1 Score: 0.8092
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.4139  [0/3200]
Loss: 0.3920  [1600/3200]
Validation Loss: 0.0337, Accuracy: 79.62%, F1 Score: 0.7933
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.3408  [0/3200]
Loss: 0.4101  [1600/3200]
Validation Loss: 0.0336, Accuracy: 81.62%, F1 Score: 0.8159
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.2669  [0/3200]
Loss: 0.1332  [1600/3200]
Validation Loss: 0.0353, Accuracy: 79.00%, F1 Score: 0.7861
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.2660  [0/3200]
Loss: 0.6638  [1600/3200]
Validation Loss: 0.0400, Accuracy: 75.62%, F1 Score: 0.7460
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.6981  [0/3200]
Loss: 0.3709  [1600/3200]
Validation Loss: 0.0391, Accuracy: 77.38%, F1 Score: 0.7749
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.5379  [0/3200]
Loss: 0.0591  [1600/3200]
Validation Loss: 0.0363, Accuracy: 79.25%, F1 Score: 0.7930
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.3722  [0/3200]
Loss: 0.4222  [1600/3200]
Validation Loss: 0.0368, Accuracy: 79.50%, F1 Score: 0.7892
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.1691  [0/3200]
Loss: 0.5078  [1600/3200]
Validation Loss: 0.0471, Accuracy: 74.50%, F1 Score: 0.7321
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.5537  [0/3200]
Loss: 0.1657  [1600/3200]
Validation Loss: 0.0351, Accuracy: 80.75%, F1 Score: 0.8071
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.2470  [0/3200]
Loss: 0.6423  [1600/3200]
Validation Loss: 0.0385, Accuracy: 79.25%, F1 Score: 0.7924
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.1028  [0/3200]
Loss: 0.4791  [1600/3200]
Validation Loss: 0.0367, Accuracy: 80.50%, F1 Score: 0.8041
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.1404  [0/3200]
Loss: 0.2091  [1600/3200]
Validation Loss: 0.0367, Accuracy: 80.75%, F1 Score: 0.8043
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.2133  [0/3200]
Loss: 0.2974  [1600/3200]
Validation Loss: 0.0387, Accuracy: 78.88%, F1 Score: 0.7833
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.4125  [0/3200]
Loss: 0.2467  [1600/3200]
Validation Loss: 0.0382, Accuracy: 79.12%, F1 Score: 0.7882
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0341  [0/3200]
Loss: 0.2244  [1600/3200]
Validation Loss: 0.0412, Accuracy: 77.50%, F1 Score: 0.7691
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.1815  [0/3200]
Loss: 0.1323  [1600/3200]
Validation Loss: 0.0416, Accuracy: 80.25%, F1 Score: 0.8010
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0728  [0/3200]
Loss: 0.2615  [1600/3200]
Validation Loss: 0.0405, Accuracy: 80.25%, F1 Score: 0.8030
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0994  [0/3200]
Loss: 0.0668  [1600/3200]
Validation Loss: 0.0402, Accuracy: 78.75%, F1 Score: 0.7883
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.1786  [0/3200]
Loss: 0.2155  [1600/3200]
Validation Loss: 0.0429, Accuracy: 79.38%, F1 Score: 0.7909
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.3339  [0/3200]
Loss: 0.1262  [1600/3200]
Validation Loss: 0.0424, Accuracy: 78.12%, F1 Score: 0.7769
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0132  [0/3200]
Loss: 0.0754  [1600/3200]
Validation Loss: 0.0436, Accuracy: 80.38%, F1 Score: 0.8029
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0545  [0/3200]
Loss: 0.0598  [1600/3200]
Validation Loss: 0.0443, Accuracy: 79.50%, F1 Score: 0.7945
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0132  [0/3200]
Loss: 0.0335  [1600/3200]
Validation Loss: 0.0449, Accuracy: 78.75%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0800  [0/3200]
Loss: 0.1193  [1600/3200]
Validation Loss: 0.0475, Accuracy: 78.25%, F1 Score: 0.7840
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0925  [0/3200]
Loss: 0.0875  [1600/3200]
Validation Loss: 0.0468, Accuracy: 78.25%, F1 Score: 0.7845
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0513  [0/3200]
Loss: 0.0921  [1600/3200]
Validation Loss: 0.0490, Accuracy: 79.62%, F1 Score: 0.7935
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0165  [0/3200]
Loss: 0.0449  [1600/3200]
Validation Loss: 0.0548, Accuracy: 77.38%, F1 Score: 0.7703
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.1944  [0/3200]
Loss: 0.0475  [1600/3200]
Validation Loss: 0.0527, Accuracy: 77.12%, F1 Score: 0.7659
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0085  [0/3200]
Loss: 0.0834  [1600/3200]
Validation Loss: 0.0527, Accuracy: 78.25%, F1 Score: 0.7799
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.1296  [0/3200]
Loss: 0.0487  [1600/3200]
Validation Loss: 0.0484, Accuracy: 80.12%, F1 Score: 0.7981
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0454  [0/3200]
Loss: 0.0201  [1600/3200]
Validation Loss: 0.0549, Accuracy: 78.12%, F1 Score: 0.7800
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.1017  [0/3200]
Loss: 0.0208  [1600/3200]
Validation Loss: 0.0669, Accuracy: 75.12%, F1 Score: 0.7407
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0346  [0/3200]
Loss: 0.0595  [1600/3200]
Validation Loss: 0.0550, Accuracy: 77.62%, F1 Score: 0.7724
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.1242  [0/3200]
Loss: 0.0097  [1600/3200]
Validation Loss: 0.0556, Accuracy: 78.50%, F1 Score: 0.7800
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0448  [0/3200]
Loss: 0.0347  [1600/3200]
Validation Loss: 0.0535, Accuracy: 78.38%, F1 Score: 0.7798
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0769  [0/3200]
Loss: 0.0092  [1600/3200]
Validation Loss: 0.0637, Accuracy: 77.75%, F1 Score: 0.7671
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0135  [0/3200]
Loss: 0.0475  [1600/3200]
Validation Loss: 0.0695, Accuracy: 76.12%, F1 Score: 0.7535
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0062  [0/3200]
Loss: 0.1149  [1600/3200]
Validation Loss: 0.0640, Accuracy: 77.00%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0372  [0/3200]
Loss: 0.0799  [1600/3200]
Validation Loss: 0.0574, Accuracy: 79.00%, F1 Score: 0.7889
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0355  [0/3200]
Loss: 0.0313  [1600/3200]
Validation Loss: 0.0661, Accuracy: 75.62%, F1 Score: 0.7559
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0764  [0/3200]
Loss: 0.0336  [1600/3200]
Validation Loss: 0.0585, Accuracy: 78.75%, F1 Score: 0.7840
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0533  [0/3200]
Loss: 0.0154  [1600/3200]
Validation Loss: 0.0567, Accuracy: 79.62%, F1 Score: 0.7961
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0087  [0/3200]
Loss: 0.0297  [1600/3200]
Validation Loss: 0.0612, Accuracy: 80.88%, F1 Score: 0.8048
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0 and dropout rate 0.5...
Epoch 1/60
Loss: 3.1001  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.4501  [1600/3200]
Validation Loss: 0.0849, Accuracy: 27.38%, F1 Score: 0.1465
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.4937  [0/3200]
Loss: 1.3190  [1600/3200]
Validation Loss: 0.0759, Accuracy: 60.00%, F1 Score: 0.5280
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 1.3159  [0/3200]
Loss: 1.2862  [1600/3200]
Validation Loss: 0.0632, Accuracy: 61.38%, F1 Score: 0.5571
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 1.1713  [0/3200]
Loss: 0.9885  [1600/3200]
Validation Loss: 0.0541, Accuracy: 68.75%, F1 Score: 0.6738
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.9072  [0/3200]
Loss: 0.8548  [1600/3200]
Validation Loss: 0.0490, Accuracy: 69.38%, F1 Score: 0.6820
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.1110  [0/3200]
Loss: 0.9465  [1600/3200]
Validation Loss: 0.0472, Accuracy: 72.50%, F1 Score: 0.7312
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.9107  [0/3200]
Loss: 0.8515  [1600/3200]
Validation Loss: 0.0436, Accuracy: 69.38%, F1 Score: 0.6694
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.6672  [0/3200]
Loss: 0.7739  [1600/3200]
Validation Loss: 0.0406, Accuracy: 73.88%, F1 Score: 0.7295
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.6741  [0/3200]
Loss: 0.8041  [1600/3200]
Validation Loss: 0.0383, Accuracy: 77.25%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.6683  [0/3200]
Loss: 0.5714  [1600/3200]
Validation Loss: 0.0388, Accuracy: 74.75%, F1 Score: 0.7432
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.5596  [0/3200]
Loss: 0.2982  [1600/3200]
Validation Loss: 0.0376, Accuracy: 74.88%, F1 Score: 0.7345
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.4534  [0/3200]
Loss: 0.5528  [1600/3200]
Validation Loss: 0.0342, Accuracy: 79.00%, F1 Score: 0.7879
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.7337  [0/3200]
Loss: 0.7373  [1600/3200]
Validation Loss: 0.0375, Accuracy: 77.38%, F1 Score: 0.7741
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.5028  [0/3200]
Loss: 0.4354  [1600/3200]
Validation Loss: 0.0349, Accuracy: 79.62%, F1 Score: 0.7956
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.4964  [0/3200]
Loss: 0.8039  [1600/3200]
Validation Loss: 0.0340, Accuracy: 78.38%, F1 Score: 0.7794
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.7943  [0/3200]
Loss: 0.4576  [1600/3200]
Validation Loss: 0.0369, Accuracy: 74.12%, F1 Score: 0.7233
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.4953  [0/3200]
Loss: 0.4355  [1600/3200]
Validation Loss: 0.0326, Accuracy: 79.25%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.6844  [0/3200]
Loss: 0.6265  [1600/3200]
Validation Loss: 0.0337, Accuracy: 79.25%, F1 Score: 0.7930
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.8702  [0/3200]
Loss: 0.5701  [1600/3200]
Validation Loss: 0.0369, Accuracy: 78.38%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.4085  [0/3200]
Loss: 0.2463  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7855
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.3683  [0/3200]
Loss: 0.7692  [1600/3200]
Validation Loss: 0.0331, Accuracy: 78.88%, F1 Score: 0.7830
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.9241  [0/3200]
Loss: 0.5881  [1600/3200]
Validation Loss: 0.0329, Accuracy: 79.62%, F1 Score: 0.7962
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.7539  [0/3200]
Loss: 0.3985  [1600/3200]
Validation Loss: 0.0338, Accuracy: 78.75%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.8507  [0/3200]
Loss: 0.6467  [1600/3200]
Validation Loss: 0.0441, Accuracy: 73.25%, F1 Score: 0.7300
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.4948  [0/3200]
Loss: 0.4961  [1600/3200]
Validation Loss: 0.0329, Accuracy: 78.75%, F1 Score: 0.7836
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.8073  [0/3200]
Loss: 0.2835  [1600/3200]
Validation Loss: 0.0327, Accuracy: 79.50%, F1 Score: 0.7959
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.5516  [0/3200]
Loss: 0.9253  [1600/3200]
Validation Loss: 0.0324, Accuracy: 79.62%, F1 Score: 0.7932
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.4256  [0/3200]
Loss: 1.1406  [1600/3200]
Validation Loss: 0.0324, Accuracy: 79.88%, F1 Score: 0.7981
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.3348  [0/3200]
Loss: 0.3005  [1600/3200]
Validation Loss: 0.0319, Accuracy: 80.62%, F1 Score: 0.8064
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.6407  [0/3200]
Loss: 0.3565  [1600/3200]
Validation Loss: 0.0377, Accuracy: 75.62%, F1 Score: 0.7359
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.9992  [0/3200]
Loss: 0.3197  [1600/3200]
Validation Loss: 0.0335, Accuracy: 79.12%, F1 Score: 0.7889
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.3579  [0/3200]
Loss: 0.3361  [1600/3200]
Validation Loss: 0.0325, Accuracy: 79.75%, F1 Score: 0.7989
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.5257  [0/3200]
Loss: 0.4561  [1600/3200]
Validation Loss: 0.0340, Accuracy: 79.50%, F1 Score: 0.7932
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.2623  [0/3200]
Loss: 0.5026  [1600/3200]
Validation Loss: 0.0333, Accuracy: 79.12%, F1 Score: 0.7896
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.4227  [0/3200]
Loss: 0.3956  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.00%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.4901  [0/3200]
Loss: 0.3023  [1600/3200]
Validation Loss: 0.0372, Accuracy: 76.75%, F1 Score: 0.7548
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.5365  [0/3200]
Loss: 0.4423  [1600/3200]
Validation Loss: 0.0328, Accuracy: 80.88%, F1 Score: 0.8087
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.2339  [0/3200]
Loss: 0.2826  [1600/3200]
Validation Loss: 0.0342, Accuracy: 80.25%, F1 Score: 0.8020
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.4517  [0/3200]
Loss: 0.3633  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.38%, F1 Score: 0.7826
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.2644  [0/3200]
Loss: 0.4193  [1600/3200]
Validation Loss: 0.0364, Accuracy: 78.00%, F1 Score: 0.7759
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.2663  [0/3200]
Loss: 0.4923  [1600/3200]
Validation Loss: 0.0330, Accuracy: 81.38%, F1 Score: 0.8131
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.3760  [0/3200]
Loss: 0.3264  [1600/3200]
Validation Loss: 0.0374, Accuracy: 76.62%, F1 Score: 0.7627
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.3908  [0/3200]
Loss: 0.1529  [1600/3200]
Validation Loss: 0.0336, Accuracy: 81.50%, F1 Score: 0.8145
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.1013  [0/3200]
Loss: 0.3071  [1600/3200]
Validation Loss: 0.0342, Accuracy: 81.50%, F1 Score: 0.8142
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.4107  [0/3200]
Loss: 0.3451  [1600/3200]
Validation Loss: 0.0394, Accuracy: 78.50%, F1 Score: 0.7734
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.1054  [0/3200]
Loss: 0.2467  [1600/3200]
Validation Loss: 0.0359, Accuracy: 80.50%, F1 Score: 0.8061
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.4461  [0/3200]
Loss: 0.2854  [1600/3200]
Validation Loss: 0.0357, Accuracy: 78.50%, F1 Score: 0.7829
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.3897  [0/3200]
Loss: 0.0605  [1600/3200]
Validation Loss: 0.0354, Accuracy: 79.88%, F1 Score: 0.7998
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.2584  [0/3200]
Loss: 0.1094  [1600/3200]
Validation Loss: 0.0379, Accuracy: 80.75%, F1 Score: 0.8044
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.3179  [0/3200]
Loss: 0.5119  [1600/3200]
Validation Loss: 0.0394, Accuracy: 77.12%, F1 Score: 0.7670
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.2943  [0/3200]
Loss: 0.2074  [1600/3200]
Validation Loss: 0.0388, Accuracy: 77.75%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.3704  [0/3200]
Loss: 0.1365  [1600/3200]
Validation Loss: 0.0365, Accuracy: 80.12%, F1 Score: 0.8017
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.3993  [0/3200]
Loss: 0.0605  [1600/3200]
Validation Loss: 0.0377, Accuracy: 80.75%, F1 Score: 0.8073
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.1111  [0/3200]
Loss: 0.2708  [1600/3200]
Validation Loss: 0.0448, Accuracy: 75.88%, F1 Score: 0.7542
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.2683  [0/3200]
Loss: 0.2973  [1600/3200]
Validation Loss: 0.0429, Accuracy: 78.50%, F1 Score: 0.7806
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.1150  [0/3200]
Loss: 0.2013  [1600/3200]
Validation Loss: 0.0412, Accuracy: 78.12%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.3225  [0/3200]
Loss: 0.2148  [1600/3200]
Validation Loss: 0.0444, Accuracy: 77.38%, F1 Score: 0.7702
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.4802  [0/3200]
Loss: 0.2000  [1600/3200]
Validation Loss: 0.0420, Accuracy: 79.88%, F1 Score: 0.7986
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.5690  [0/3200]
Loss: 0.2724  [1600/3200]
Validation Loss: 0.0470, Accuracy: 76.62%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.3987  [0/3200]
Loss: 0.6882  [1600/3200]
Validation Loss: 0.0409, Accuracy: 80.12%, F1 Score: 0.7994
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0001 and dropout rate 0.0...
Epoch 1/60
Loss: 1.8130  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.0084  [1600/3200]
Validation Loss: 0.0518, Accuracy: 64.00%, F1 Score: 0.6153
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.8651  [0/3200]
Loss: 0.8666  [1600/3200]
Validation Loss: 0.0502, Accuracy: 64.62%, F1 Score: 0.6125
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.5942  [0/3200]
Loss: 0.5592  [1600/3200]
Validation Loss: 0.0416, Accuracy: 72.38%, F1 Score: 0.7242
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.5663  [0/3200]
Loss: 0.5513  [1600/3200]
Validation Loss: 0.0410, Accuracy: 75.25%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.5310  [0/3200]
Loss: 0.6676  [1600/3200]
Validation Loss: 0.0378, Accuracy: 76.00%, F1 Score: 0.7492
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.0980  [0/3200]
Loss: 0.4638  [1600/3200]
Validation Loss: 0.0378, Accuracy: 77.00%, F1 Score: 0.7687
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5259  [0/3200]
Loss: 0.5878  [1600/3200]
Validation Loss: 0.0364, Accuracy: 79.50%, F1 Score: 0.7917
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3523  [0/3200]
Loss: 0.5048  [1600/3200]
Validation Loss: 0.0362, Accuracy: 78.00%, F1 Score: 0.7828
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.4413  [0/3200]
Loss: 0.4677  [1600/3200]
Validation Loss: 0.0340, Accuracy: 80.50%, F1 Score: 0.8026
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.3964  [0/3200]
Loss: 0.4955  [1600/3200]
Validation Loss: 0.0414, Accuracy: 75.75%, F1 Score: 0.7619
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.5309  [0/3200]
Loss: 0.1812  [1600/3200]
Validation Loss: 0.0371, Accuracy: 78.25%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.3070  [0/3200]
Loss: 0.1710  [1600/3200]
Validation Loss: 0.0383, Accuracy: 77.88%, F1 Score: 0.7711
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3592  [0/3200]
Loss: 0.3372  [1600/3200]
Validation Loss: 0.0429, Accuracy: 74.62%, F1 Score: 0.7388
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.5892  [0/3200]
Loss: 0.2790  [1600/3200]
Validation Loss: 0.0481, Accuracy: 70.25%, F1 Score: 0.7053
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.5883  [0/3200]
Loss: 0.4327  [1600/3200]
Validation Loss: 0.0342, Accuracy: 80.00%, F1 Score: 0.8002
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.3316  [0/3200]
Loss: 0.4678  [1600/3200]
Validation Loss: 0.0345, Accuracy: 80.75%, F1 Score: 0.8050
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.2286  [0/3200]
Loss: 0.2380  [1600/3200]
Validation Loss: 0.0349, Accuracy: 80.50%, F1 Score: 0.8014
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.3015  [0/3200]
Loss: 0.1926  [1600/3200]
Validation Loss: 0.0354, Accuracy: 80.00%, F1 Score: 0.8014
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.2549  [0/3200]
Loss: 0.3028  [1600/3200]
Validation Loss: 0.0374, Accuracy: 80.75%, F1 Score: 0.8072
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.3394  [0/3200]
Loss: 0.0832  [1600/3200]
Validation Loss: 0.0353, Accuracy: 81.00%, F1 Score: 0.8088
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1802  [0/3200]
Loss: 0.4461  [1600/3200]
Validation Loss: 0.0421, Accuracy: 76.12%, F1 Score: 0.7451
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.3866  [0/3200]
Loss: 0.4243  [1600/3200]
Validation Loss: 0.0408, Accuracy: 77.88%, F1 Score: 0.7801
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.3405  [0/3200]
Loss: 0.0472  [1600/3200]
Validation Loss: 0.0397, Accuracy: 79.38%, F1 Score: 0.7859
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.1635  [0/3200]
Loss: 0.4015  [1600/3200]
Validation Loss: 0.0398, Accuracy: 77.50%, F1 Score: 0.7713
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0981  [0/3200]
Loss: 0.2858  [1600/3200]
Validation Loss: 0.0403, Accuracy: 77.38%, F1 Score: 0.7717
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.1624  [0/3200]
Loss: 0.0515  [1600/3200]
Validation Loss: 0.0468, Accuracy: 76.50%, F1 Score: 0.7580
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.1798  [0/3200]
Loss: 0.2481  [1600/3200]
Validation Loss: 0.0470, Accuracy: 75.38%, F1 Score: 0.7496
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.1532  [0/3200]
Loss: 0.1029  [1600/3200]
Validation Loss: 0.0449, Accuracy: 75.88%, F1 Score: 0.7558
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.1324  [0/3200]
Loss: 0.0823  [1600/3200]
Validation Loss: 0.0545, Accuracy: 75.38%, F1 Score: 0.7393
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0897  [0/3200]
Loss: 0.0687  [1600/3200]
Validation Loss: 0.0442, Accuracy: 78.12%, F1 Score: 0.7778
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0889  [0/3200]
Loss: 0.1434  [1600/3200]
Validation Loss: 0.0540, Accuracy: 73.62%, F1 Score: 0.7254
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0574  [0/3200]
Loss: 0.0269  [1600/3200]
Validation Loss: 0.0562, Accuracy: 73.38%, F1 Score: 0.7293
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.2959  [0/3200]
Loss: 0.0182  [1600/3200]
Validation Loss: 0.0516, Accuracy: 75.62%, F1 Score: 0.7498
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0339  [0/3200]
Loss: 0.1212  [1600/3200]
Validation Loss: 0.0458, Accuracy: 79.75%, F1 Score: 0.7976
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0601  [0/3200]
Loss: 0.0168  [1600/3200]
Validation Loss: 0.0562, Accuracy: 75.62%, F1 Score: 0.7468
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0677  [0/3200]
Loss: 0.0151  [1600/3200]
Validation Loss: 0.0513, Accuracy: 77.88%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0595  [0/3200]
Loss: 0.0766  [1600/3200]
Validation Loss: 0.0486, Accuracy: 78.75%, F1 Score: 0.7861
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0052  [0/3200]
Loss: 0.0155  [1600/3200]
Validation Loss: 0.0513, Accuracy: 78.88%, F1 Score: 0.7846
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0182  [0/3200]
Loss: 0.0170  [1600/3200]
Validation Loss: 0.0508, Accuracy: 78.12%, F1 Score: 0.7810
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0069  [0/3200]
Loss: 0.0044  [1600/3200]
Validation Loss: 0.0563, Accuracy: 77.12%, F1 Score: 0.7689
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0137  [0/3200]
Loss: 0.0287  [1600/3200]
Validation Loss: 0.0546, Accuracy: 78.75%, F1 Score: 0.7814
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0189  [0/3200]
Loss: 0.0520  [1600/3200]
Validation Loss: 0.0577, Accuracy: 77.00%, F1 Score: 0.7683
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0257  [0/3200]
Loss: 0.0099  [1600/3200]
Validation Loss: 0.0664, Accuracy: 77.12%, F1 Score: 0.7641
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0036  [0/3200]
Loss: 0.0083  [1600/3200]
Validation Loss: 0.0552, Accuracy: 78.62%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0111  [0/3200]
Loss: 0.0081  [1600/3200]
Validation Loss: 0.0636, Accuracy: 76.62%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0072  [0/3200]
Loss: 0.0112  [1600/3200]
Validation Loss: 0.0590, Accuracy: 80.00%, F1 Score: 0.7987
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.1325  [0/3200]
Loss: 0.0325  [1600/3200]
Validation Loss: 0.0634, Accuracy: 77.25%, F1 Score: 0.7677
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0036  [0/3200]
Loss: 0.0041  [1600/3200]
Validation Loss: 0.0638, Accuracy: 77.00%, F1 Score: 0.7657
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0171  [0/3200]
Loss: 0.0049  [1600/3200]
Validation Loss: 0.0654, Accuracy: 77.00%, F1 Score: 0.7640
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0043  [0/3200]
Loss: 0.0036  [1600/3200]
Validation Loss: 0.0728, Accuracy: 75.62%, F1 Score: 0.7491
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0076  [0/3200]
Loss: 0.0086  [1600/3200]
Validation Loss: 0.0724, Accuracy: 75.75%, F1 Score: 0.7502
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0099  [0/3200]
Loss: 0.0022  [1600/3200]
Validation Loss: 0.0634, Accuracy: 78.88%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0087  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0687, Accuracy: 76.88%, F1 Score: 0.7638
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0034  [0/3200]
Loss: 0.0029  [1600/3200]
Validation Loss: 0.0680, Accuracy: 78.75%, F1 Score: 0.7819
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0025  [0/3200]
Loss: 0.0055  [1600/3200]
Validation Loss: 0.0725, Accuracy: 76.38%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0012  [0/3200]
Loss: 0.0040  [1600/3200]
Validation Loss: 0.0659, Accuracy: 79.38%, F1 Score: 0.7926
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0061  [0/3200]
Loss: 0.0030  [1600/3200]
Validation Loss: 0.0713, Accuracy: 78.00%, F1 Score: 0.7762
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0040  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0685, Accuracy: 78.62%, F1 Score: 0.7821
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0021  [0/3200]
Loss: 0.0025  [1600/3200]
Validation Loss: 0.0750, Accuracy: 76.00%, F1 Score: 0.7566
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0033  [0/3200]
Loss: 0.0009  [1600/3200]
Validation Loss: 0.0730, Accuracy: 78.12%, F1 Score: 0.7760
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0001 and dropout rate 0.2...
Epoch 1/60
Loss: 2.1009  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.2446  [1600/3200]
Validation Loss: 0.0694, Accuracy: 56.88%, F1 Score: 0.5045
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.4184  [0/3200]
Loss: 1.0964  [1600/3200]
Validation Loss: 0.0514, Accuracy: 69.88%, F1 Score: 0.6701
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.9449  [0/3200]
Loss: 0.7211  [1600/3200]
Validation Loss: 0.0449, Accuracy: 73.50%, F1 Score: 0.7198
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.6417  [0/3200]
Loss: 0.6233  [1600/3200]
Validation Loss: 0.0416, Accuracy: 73.25%, F1 Score: 0.7265
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.5720  [0/3200]
Loss: 0.7276  [1600/3200]
Validation Loss: 0.0393, Accuracy: 73.38%, F1 Score: 0.7109
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.1605  [0/3200]
Loss: 0.4451  [1600/3200]
Validation Loss: 0.0353, Accuracy: 77.88%, F1 Score: 0.7765
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5807  [0/3200]
Loss: 0.6649  [1600/3200]
Validation Loss: 0.0380, Accuracy: 73.38%, F1 Score: 0.7098
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.4462  [0/3200]
Loss: 0.5420  [1600/3200]
Validation Loss: 0.0349, Accuracy: 78.50%, F1 Score: 0.7813
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.6928  [0/3200]
Loss: 0.4896  [1600/3200]
Validation Loss: 0.0350, Accuracy: 79.38%, F1 Score: 0.7887
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.5037  [0/3200]
Loss: 0.5395  [1600/3200]
Validation Loss: 0.0382, Accuracy: 75.25%, F1 Score: 0.7496
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.7221  [0/3200]
Loss: 0.2515  [1600/3200]
Validation Loss: 0.0338, Accuracy: 78.12%, F1 Score: 0.7796
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.4617  [0/3200]
Loss: 0.2066  [1600/3200]
Validation Loss: 0.0349, Accuracy: 78.88%, F1 Score: 0.7890
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.6092  [0/3200]
Loss: 0.4586  [1600/3200]
Validation Loss: 0.0341, Accuracy: 79.12%, F1 Score: 0.7905
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.4624  [0/3200]
Loss: 0.2890  [1600/3200]
Validation Loss: 0.0335, Accuracy: 79.62%, F1 Score: 0.7983
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.5961  [0/3200]
Loss: 0.7391  [1600/3200]
Validation Loss: 0.0329, Accuracy: 80.38%, F1 Score: 0.8038
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.5379  [0/3200]
Loss: 0.3100  [1600/3200]
Validation Loss: 0.0359, Accuracy: 77.75%, F1 Score: 0.7689
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.2618  [0/3200]
Loss: 0.3161  [1600/3200]
Validation Loss: 0.0330, Accuracy: 81.25%, F1 Score: 0.8119
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.4116  [0/3200]
Loss: 0.3976  [1600/3200]
Validation Loss: 0.0338, Accuracy: 79.75%, F1 Score: 0.7947
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.3453  [0/3200]
Loss: 0.4144  [1600/3200]
Validation Loss: 0.0335, Accuracy: 81.50%, F1 Score: 0.8146
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.2707  [0/3200]
Loss: 0.1349  [1600/3200]
Validation Loss: 0.0353, Accuracy: 78.88%, F1 Score: 0.7846
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.2689  [0/3200]
Loss: 0.6722  [1600/3200]
Validation Loss: 0.0400, Accuracy: 75.88%, F1 Score: 0.7483
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.7039  [0/3200]
Loss: 0.3757  [1600/3200]
Validation Loss: 0.0392, Accuracy: 77.00%, F1 Score: 0.7712
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.5297  [0/3200]
Loss: 0.0606  [1600/3200]
Validation Loss: 0.0362, Accuracy: 79.00%, F1 Score: 0.7903
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.3648  [0/3200]
Loss: 0.4227  [1600/3200]
Validation Loss: 0.0368, Accuracy: 79.25%, F1 Score: 0.7869
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.1699  [0/3200]
Loss: 0.5054  [1600/3200]
Validation Loss: 0.0471, Accuracy: 74.12%, F1 Score: 0.7280
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.5559  [0/3200]
Loss: 0.1712  [1600/3200]
Validation Loss: 0.0352, Accuracy: 80.75%, F1 Score: 0.8072
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.2415  [0/3200]
Loss: 0.6414  [1600/3200]
Validation Loss: 0.0384, Accuracy: 79.50%, F1 Score: 0.7949
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.1026  [0/3200]
Loss: 0.4847  [1600/3200]
Validation Loss: 0.0366, Accuracy: 80.50%, F1 Score: 0.8041
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.1390  [0/3200]
Loss: 0.2179  [1600/3200]
Validation Loss: 0.0367, Accuracy: 81.00%, F1 Score: 0.8064
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.2126  [0/3200]
Loss: 0.3072  [1600/3200]
Validation Loss: 0.0387, Accuracy: 78.75%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.4072  [0/3200]
Loss: 0.2451  [1600/3200]
Validation Loss: 0.0380, Accuracy: 79.62%, F1 Score: 0.7935
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0343  [0/3200]
Loss: 0.2237  [1600/3200]
Validation Loss: 0.0412, Accuracy: 77.00%, F1 Score: 0.7635
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.1859  [0/3200]
Loss: 0.1324  [1600/3200]
Validation Loss: 0.0416, Accuracy: 80.38%, F1 Score: 0.8019
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0765  [0/3200]
Loss: 0.2599  [1600/3200]
Validation Loss: 0.0405, Accuracy: 80.00%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.1032  [0/3200]
Loss: 0.0722  [1600/3200]
Validation Loss: 0.0405, Accuracy: 79.12%, F1 Score: 0.7918
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.1737  [0/3200]
Loss: 0.2292  [1600/3200]
Validation Loss: 0.0431, Accuracy: 79.12%, F1 Score: 0.7887
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.3318  [0/3200]
Loss: 0.1337  [1600/3200]
Validation Loss: 0.0427, Accuracy: 77.88%, F1 Score: 0.7739
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0138  [0/3200]
Loss: 0.0753  [1600/3200]
Validation Loss: 0.0433, Accuracy: 80.25%, F1 Score: 0.8020
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0578  [0/3200]
Loss: 0.0589  [1600/3200]
Validation Loss: 0.0442, Accuracy: 80.00%, F1 Score: 0.7996
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0143  [0/3200]
Loss: 0.0396  [1600/3200]
Validation Loss: 0.0450, Accuracy: 78.88%, F1 Score: 0.7889
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0746  [0/3200]
Loss: 0.1227  [1600/3200]
Validation Loss: 0.0481, Accuracy: 78.50%, F1 Score: 0.7865
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0949  [0/3200]
Loss: 0.0896  [1600/3200]
Validation Loss: 0.0466, Accuracy: 78.62%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0538  [0/3200]
Loss: 0.1019  [1600/3200]
Validation Loss: 0.0492, Accuracy: 78.62%, F1 Score: 0.7829
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0180  [0/3200]
Loss: 0.0454  [1600/3200]
Validation Loss: 0.0548, Accuracy: 76.50%, F1 Score: 0.7604
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.2043  [0/3200]
Loss: 0.0447  [1600/3200]
Validation Loss: 0.0525, Accuracy: 77.00%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0085  [0/3200]
Loss: 0.0886  [1600/3200]
Validation Loss: 0.0520, Accuracy: 78.12%, F1 Score: 0.7791
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.1284  [0/3200]
Loss: 0.0510  [1600/3200]
Validation Loss: 0.0484, Accuracy: 80.25%, F1 Score: 0.7997
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0485  [0/3200]
Loss: 0.0201  [1600/3200]
Validation Loss: 0.0552, Accuracy: 77.38%, F1 Score: 0.7719
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.1013  [0/3200]
Loss: 0.0230  [1600/3200]
Validation Loss: 0.0665, Accuracy: 75.38%, F1 Score: 0.7440
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0324  [0/3200]
Loss: 0.0713  [1600/3200]
Validation Loss: 0.0548, Accuracy: 77.50%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.1178  [0/3200]
Loss: 0.0078  [1600/3200]
Validation Loss: 0.0553, Accuracy: 79.00%, F1 Score: 0.7848
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0372  [0/3200]
Loss: 0.0314  [1600/3200]
Validation Loss: 0.0533, Accuracy: 77.88%, F1 Score: 0.7742
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0819  [0/3200]
Loss: 0.0093  [1600/3200]
Validation Loss: 0.0637, Accuracy: 77.50%, F1 Score: 0.7640
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0138  [0/3200]
Loss: 0.0433  [1600/3200]
Validation Loss: 0.0682, Accuracy: 76.00%, F1 Score: 0.7524
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0059  [0/3200]
Loss: 0.1071  [1600/3200]
Validation Loss: 0.0631, Accuracy: 77.00%, F1 Score: 0.7652
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0338  [0/3200]
Loss: 0.0749  [1600/3200]
Validation Loss: 0.0575, Accuracy: 79.00%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0325  [0/3200]
Loss: 0.0295  [1600/3200]
Validation Loss: 0.0701, Accuracy: 74.38%, F1 Score: 0.7433
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.1059  [0/3200]
Loss: 0.0237  [1600/3200]
Validation Loss: 0.0602, Accuracy: 78.12%, F1 Score: 0.7767
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0603  [0/3200]
Loss: 0.0148  [1600/3200]
Validation Loss: 0.0595, Accuracy: 79.62%, F1 Score: 0.7959
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0069  [0/3200]
Loss: 0.0500  [1600/3200]
Validation Loss: 0.0612, Accuracy: 81.12%, F1 Score: 0.8072
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.0001 and dropout rate 0.5...
Epoch 1/60
Loss: 3.1001  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.4502  [1600/3200]
Validation Loss: 0.0849, Accuracy: 27.38%, F1 Score: 0.1465
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.4943  [0/3200]
Loss: 1.3190  [1600/3200]
Validation Loss: 0.0759, Accuracy: 60.12%, F1 Score: 0.5290
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 1.3167  [0/3200]
Loss: 1.2846  [1600/3200]
Validation Loss: 0.0632, Accuracy: 61.38%, F1 Score: 0.5572
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 1.1722  [0/3200]
Loss: 0.9881  [1600/3200]
Validation Loss: 0.0541, Accuracy: 68.62%, F1 Score: 0.6728
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.9089  [0/3200]
Loss: 0.8568  [1600/3200]
Validation Loss: 0.0489, Accuracy: 69.38%, F1 Score: 0.6826
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.1100  [0/3200]
Loss: 0.9451  [1600/3200]
Validation Loss: 0.0471, Accuracy: 72.12%, F1 Score: 0.7280
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.9133  [0/3200]
Loss: 0.8519  [1600/3200]
Validation Loss: 0.0436, Accuracy: 69.25%, F1 Score: 0.6683
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.6621  [0/3200]
Loss: 0.7749  [1600/3200]
Validation Loss: 0.0407, Accuracy: 73.75%, F1 Score: 0.7284
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.6707  [0/3200]
Loss: 0.8086  [1600/3200]
Validation Loss: 0.0383, Accuracy: 77.50%, F1 Score: 0.7698
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.6723  [0/3200]
Loss: 0.5716  [1600/3200]
Validation Loss: 0.0388, Accuracy: 74.50%, F1 Score: 0.7404
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.5582  [0/3200]
Loss: 0.3000  [1600/3200]
Validation Loss: 0.0376, Accuracy: 75.12%, F1 Score: 0.7380
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.4562  [0/3200]
Loss: 0.5493  [1600/3200]
Validation Loss: 0.0342, Accuracy: 78.75%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.7380  [0/3200]
Loss: 0.7388  [1600/3200]
Validation Loss: 0.0375, Accuracy: 77.25%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.5037  [0/3200]
Loss: 0.4362  [1600/3200]
Validation Loss: 0.0348, Accuracy: 79.88%, F1 Score: 0.7981
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.4969  [0/3200]
Loss: 0.7998  [1600/3200]
Validation Loss: 0.0340, Accuracy: 78.62%, F1 Score: 0.7820
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.7913  [0/3200]
Loss: 0.4561  [1600/3200]
Validation Loss: 0.0369, Accuracy: 74.25%, F1 Score: 0.7251
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.4912  [0/3200]
Loss: 0.4335  [1600/3200]
Validation Loss: 0.0326, Accuracy: 79.00%, F1 Score: 0.7877
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.6861  [0/3200]
Loss: 0.6278  [1600/3200]
Validation Loss: 0.0338, Accuracy: 78.88%, F1 Score: 0.7891
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.8685  [0/3200]
Loss: 0.5654  [1600/3200]
Validation Loss: 0.0369, Accuracy: 78.12%, F1 Score: 0.7828
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.4098  [0/3200]
Loss: 0.2467  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7854
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.3677  [0/3200]
Loss: 0.7689  [1600/3200]
Validation Loss: 0.0331, Accuracy: 78.88%, F1 Score: 0.7831
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.9231  [0/3200]
Loss: 0.5895  [1600/3200]
Validation Loss: 0.0329, Accuracy: 79.50%, F1 Score: 0.7951
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.7559  [0/3200]
Loss: 0.4022  [1600/3200]
Validation Loss: 0.0338, Accuracy: 79.00%, F1 Score: 0.7913
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.8477  [0/3200]
Loss: 0.6516  [1600/3200]
Validation Loss: 0.0442, Accuracy: 73.25%, F1 Score: 0.7300
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.4886  [0/3200]
Loss: 0.4963  [1600/3200]
Validation Loss: 0.0329, Accuracy: 78.62%, F1 Score: 0.7824
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.8049  [0/3200]
Loss: 0.2844  [1600/3200]
Validation Loss: 0.0328, Accuracy: 79.50%, F1 Score: 0.7959
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.5519  [0/3200]
Loss: 0.9275  [1600/3200]
Validation Loss: 0.0324, Accuracy: 79.62%, F1 Score: 0.7934
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.4209  [0/3200]
Loss: 1.1313  [1600/3200]
Validation Loss: 0.0324, Accuracy: 79.88%, F1 Score: 0.7982
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.3333  [0/3200]
Loss: 0.2983  [1600/3200]
Validation Loss: 0.0319, Accuracy: 80.50%, F1 Score: 0.8053
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.6407  [0/3200]
Loss: 0.3556  [1600/3200]
Validation Loss: 0.0374, Accuracy: 75.88%, F1 Score: 0.7400
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 1.0057  [0/3200]
Loss: 0.3202  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.62%, F1 Score: 0.7833
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.3578  [0/3200]
Loss: 0.3360  [1600/3200]
Validation Loss: 0.0325, Accuracy: 79.75%, F1 Score: 0.7992
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.5247  [0/3200]
Loss: 0.4601  [1600/3200]
Validation Loss: 0.0340, Accuracy: 79.38%, F1 Score: 0.7917
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.2589  [0/3200]
Loss: 0.5056  [1600/3200]
Validation Loss: 0.0332, Accuracy: 79.00%, F1 Score: 0.7886
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.4247  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0359, Accuracy: 78.88%, F1 Score: 0.7868
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.4946  [0/3200]
Loss: 0.2940  [1600/3200]
Validation Loss: 0.0371, Accuracy: 77.12%, F1 Score: 0.7591
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.5330  [0/3200]
Loss: 0.4430  [1600/3200]
Validation Loss: 0.0328, Accuracy: 80.88%, F1 Score: 0.8087
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.2372  [0/3200]
Loss: 0.2834  [1600/3200]
Validation Loss: 0.0341, Accuracy: 80.25%, F1 Score: 0.8019
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.4472  [0/3200]
Loss: 0.3593  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.38%, F1 Score: 0.7828
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.2714  [0/3200]
Loss: 0.4200  [1600/3200]
Validation Loss: 0.0362, Accuracy: 78.12%, F1 Score: 0.7773
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.2693  [0/3200]
Loss: 0.4898  [1600/3200]
Validation Loss: 0.0329, Accuracy: 81.50%, F1 Score: 0.8142
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.3726  [0/3200]
Loss: 0.3290  [1600/3200]
Validation Loss: 0.0374, Accuracy: 76.50%, F1 Score: 0.7612
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.3914  [0/3200]
Loss: 0.1554  [1600/3200]
Validation Loss: 0.0337, Accuracy: 81.25%, F1 Score: 0.8121
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.1000  [0/3200]
Loss: 0.3078  [1600/3200]
Validation Loss: 0.0342, Accuracy: 81.50%, F1 Score: 0.8142
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.4048  [0/3200]
Loss: 0.3441  [1600/3200]
Validation Loss: 0.0394, Accuracy: 78.50%, F1 Score: 0.7729
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.1055  [0/3200]
Loss: 0.2450  [1600/3200]
Validation Loss: 0.0359, Accuracy: 80.62%, F1 Score: 0.8074
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.4478  [0/3200]
Loss: 0.2826  [1600/3200]
Validation Loss: 0.0356, Accuracy: 78.62%, F1 Score: 0.7844
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.3883  [0/3200]
Loss: 0.0626  [1600/3200]
Validation Loss: 0.0354, Accuracy: 79.50%, F1 Score: 0.7961
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.2604  [0/3200]
Loss: 0.1064  [1600/3200]
Validation Loss: 0.0380, Accuracy: 80.75%, F1 Score: 0.8042
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.3305  [0/3200]
Loss: 0.5128  [1600/3200]
Validation Loss: 0.0394, Accuracy: 77.00%, F1 Score: 0.7655
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.2991  [0/3200]
Loss: 0.1996  [1600/3200]
Validation Loss: 0.0387, Accuracy: 78.50%, F1 Score: 0.7782
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.3674  [0/3200]
Loss: 0.1378  [1600/3200]
Validation Loss: 0.0364, Accuracy: 80.25%, F1 Score: 0.8028
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.3928  [0/3200]
Loss: 0.0631  [1600/3200]
Validation Loss: 0.0377, Accuracy: 80.50%, F1 Score: 0.8048
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.1116  [0/3200]
Loss: 0.2743  [1600/3200]
Validation Loss: 0.0447, Accuracy: 76.12%, F1 Score: 0.7574
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.2592  [0/3200]
Loss: 0.2979  [1600/3200]
Validation Loss: 0.0428, Accuracy: 78.50%, F1 Score: 0.7806
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.1146  [0/3200]
Loss: 0.2041  [1600/3200]
Validation Loss: 0.0412, Accuracy: 78.00%, F1 Score: 0.7738
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.3167  [0/3200]
Loss: 0.1988  [1600/3200]
Validation Loss: 0.0444, Accuracy: 77.38%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.4801  [0/3200]
Loss: 0.1907  [1600/3200]
Validation Loss: 0.0422, Accuracy: 79.88%, F1 Score: 0.7987
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.5661  [0/3200]
Loss: 0.2632  [1600/3200]
Validation Loss: 0.0469, Accuracy: 76.25%, F1 Score: 0.7628
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.3915  [0/3200]
Loss: 0.6734  [1600/3200]
Validation Loss: 0.0407, Accuracy: 80.62%, F1 Score: 0.8046
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.001 and dropout rate 0.0...
Epoch 1/60
Loss: 1.8130  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.0082  [1600/3200]
Validation Loss: 0.0518, Accuracy: 64.00%, F1 Score: 0.6154
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.8655  [0/3200]
Loss: 0.8666  [1600/3200]
Validation Loss: 0.0503, Accuracy: 64.62%, F1 Score: 0.6125
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.5957  [0/3200]
Loss: 0.5605  [1600/3200]
Validation Loss: 0.0417, Accuracy: 72.38%, F1 Score: 0.7243
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.5698  [0/3200]
Loss: 0.5515  [1600/3200]
Validation Loss: 0.0411, Accuracy: 75.38%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.5310  [0/3200]
Loss: 0.6697  [1600/3200]
Validation Loss: 0.0379, Accuracy: 76.25%, F1 Score: 0.7521
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.0998  [0/3200]
Loss: 0.4660  [1600/3200]
Validation Loss: 0.0380, Accuracy: 76.75%, F1 Score: 0.7661
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5257  [0/3200]
Loss: 0.5903  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.38%, F1 Score: 0.7904
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3559  [0/3200]
Loss: 0.5076  [1600/3200]
Validation Loss: 0.0362, Accuracy: 78.25%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.4444  [0/3200]
Loss: 0.4713  [1600/3200]
Validation Loss: 0.0340, Accuracy: 80.62%, F1 Score: 0.8039
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.3980  [0/3200]
Loss: 0.4984  [1600/3200]
Validation Loss: 0.0412, Accuracy: 75.50%, F1 Score: 0.7596
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.5373  [0/3200]
Loss: 0.1832  [1600/3200]
Validation Loss: 0.0373, Accuracy: 77.75%, F1 Score: 0.7719
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.3146  [0/3200]
Loss: 0.1747  [1600/3200]
Validation Loss: 0.0380, Accuracy: 77.88%, F1 Score: 0.7711
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3618  [0/3200]
Loss: 0.3400  [1600/3200]
Validation Loss: 0.0432, Accuracy: 74.50%, F1 Score: 0.7374
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.6025  [0/3200]
Loss: 0.2807  [1600/3200]
Validation Loss: 0.0483, Accuracy: 70.00%, F1 Score: 0.7027
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.5958  [0/3200]
Loss: 0.4416  [1600/3200]
Validation Loss: 0.0341, Accuracy: 80.12%, F1 Score: 0.8013
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.3525  [0/3200]
Loss: 0.4728  [1600/3200]
Validation Loss: 0.0341, Accuracy: 80.75%, F1 Score: 0.8054
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.2335  [0/3200]
Loss: 0.2476  [1600/3200]
Validation Loss: 0.0348, Accuracy: 80.12%, F1 Score: 0.7979
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.3126  [0/3200]
Loss: 0.1969  [1600/3200]
Validation Loss: 0.0353, Accuracy: 80.38%, F1 Score: 0.8050
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.2726  [0/3200]
Loss: 0.3184  [1600/3200]
Validation Loss: 0.0376, Accuracy: 80.12%, F1 Score: 0.8012
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.3577  [0/3200]
Loss: 0.0834  [1600/3200]
Validation Loss: 0.0351, Accuracy: 80.62%, F1 Score: 0.8051
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1952  [0/3200]
Loss: 0.4566  [1600/3200]
Validation Loss: 0.0421, Accuracy: 76.00%, F1 Score: 0.7427
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.4331  [0/3200]
Loss: 0.4399  [1600/3200]
Validation Loss: 0.0403, Accuracy: 78.50%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.3680  [0/3200]
Loss: 0.0519  [1600/3200]
Validation Loss: 0.0386, Accuracy: 79.75%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.1773  [0/3200]
Loss: 0.4049  [1600/3200]
Validation Loss: 0.0398, Accuracy: 77.38%, F1 Score: 0.7692
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.1070  [0/3200]
Loss: 0.3146  [1600/3200]
Validation Loss: 0.0406, Accuracy: 77.50%, F1 Score: 0.7716
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.2036  [0/3200]
Loss: 0.0582  [1600/3200]
Validation Loss: 0.0434, Accuracy: 77.25%, F1 Score: 0.7657
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.1824  [0/3200]
Loss: 0.2698  [1600/3200]
Validation Loss: 0.0446, Accuracy: 75.75%, F1 Score: 0.7541
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.1708  [0/3200]
Loss: 0.1191  [1600/3200]
Validation Loss: 0.0435, Accuracy: 76.38%, F1 Score: 0.7612
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.1464  [0/3200]
Loss: 0.1019  [1600/3200]
Validation Loss: 0.0557, Accuracy: 73.75%, F1 Score: 0.7198
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.1128  [0/3200]
Loss: 0.0796  [1600/3200]
Validation Loss: 0.0411, Accuracy: 79.50%, F1 Score: 0.7943
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.1262  [0/3200]
Loss: 0.1765  [1600/3200]
Validation Loss: 0.0500, Accuracy: 75.00%, F1 Score: 0.7397
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0579  [0/3200]
Loss: 0.0357  [1600/3200]
Validation Loss: 0.0543, Accuracy: 73.62%, F1 Score: 0.7303
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.2702  [0/3200]
Loss: 0.0227  [1600/3200]
Validation Loss: 0.0490, Accuracy: 76.50%, F1 Score: 0.7589
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0371  [0/3200]
Loss: 0.1686  [1600/3200]
Validation Loss: 0.0448, Accuracy: 80.00%, F1 Score: 0.8016
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0972  [0/3200]
Loss: 0.0213  [1600/3200]
Validation Loss: 0.0552, Accuracy: 75.25%, F1 Score: 0.7450
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0771  [0/3200]
Loss: 0.0162  [1600/3200]
Validation Loss: 0.0474, Accuracy: 78.88%, F1 Score: 0.7855
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0520  [0/3200]
Loss: 0.0536  [1600/3200]
Validation Loss: 0.0519, Accuracy: 77.62%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0053  [0/3200]
Loss: 0.0284  [1600/3200]
Validation Loss: 0.0494, Accuracy: 78.12%, F1 Score: 0.7784
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0334  [0/3200]
Loss: 0.0204  [1600/3200]
Validation Loss: 0.0513, Accuracy: 78.38%, F1 Score: 0.7832
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0178  [0/3200]
Loss: 0.0061  [1600/3200]
Validation Loss: 0.0590, Accuracy: 76.12%, F1 Score: 0.7519
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0575  [0/3200]
Loss: 0.0497  [1600/3200]
Validation Loss: 0.0515, Accuracy: 79.25%, F1 Score: 0.7905
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0138  [0/3200]
Loss: 0.0475  [1600/3200]
Validation Loss: 0.0616, Accuracy: 75.50%, F1 Score: 0.7475
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0474  [0/3200]
Loss: 0.0203  [1600/3200]
Validation Loss: 0.0604, Accuracy: 77.00%, F1 Score: 0.7642
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0049  [0/3200]
Loss: 0.0104  [1600/3200]
Validation Loss: 0.0543, Accuracy: 78.38%, F1 Score: 0.7824
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0126  [0/3200]
Loss: 0.0050  [1600/3200]
Validation Loss: 0.0545, Accuracy: 79.00%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0033  [0/3200]
Loss: 0.0075  [1600/3200]
Validation Loss: 0.0591, Accuracy: 78.00%, F1 Score: 0.7735
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0301  [0/3200]
Loss: 0.0408  [1600/3200]
Validation Loss: 0.0576, Accuracy: 78.62%, F1 Score: 0.7830
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0063  [0/3200]
Loss: 0.0051  [1600/3200]
Validation Loss: 0.0621, Accuracy: 77.38%, F1 Score: 0.7658
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0119  [0/3200]
Loss: 0.0100  [1600/3200]
Validation Loss: 0.0626, Accuracy: 77.38%, F1 Score: 0.7689
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0105  [0/3200]
Loss: 0.0033  [1600/3200]
Validation Loss: 0.0706, Accuracy: 75.75%, F1 Score: 0.7503
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0076  [0/3200]
Loss: 0.0122  [1600/3200]
Validation Loss: 0.0640, Accuracy: 76.88%, F1 Score: 0.7662
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0084  [0/3200]
Loss: 0.0083  [1600/3200]
Validation Loss: 0.0620, Accuracy: 78.50%, F1 Score: 0.7807
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0071  [0/3200]
Loss: 0.0016  [1600/3200]
Validation Loss: 0.0753, Accuracy: 75.25%, F1 Score: 0.7471
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0095  [0/3200]
Loss: 0.0070  [1600/3200]
Validation Loss: 0.0639, Accuracy: 77.25%, F1 Score: 0.7697
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0021  [0/3200]
Loss: 0.0118  [1600/3200]
Validation Loss: 0.0694, Accuracy: 76.88%, F1 Score: 0.7628
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0013  [0/3200]
Loss: 0.0099  [1600/3200]
Validation Loss: 0.0685, Accuracy: 77.50%, F1 Score: 0.7679
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0045  [0/3200]
Loss: 0.0037  [1600/3200]
Validation Loss: 0.0628, Accuracy: 78.12%, F1 Score: 0.7802
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0084  [0/3200]
Loss: 0.0017  [1600/3200]
Validation Loss: 0.0824, Accuracy: 74.38%, F1 Score: 0.7334
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0091  [0/3200]
Loss: 0.0064  [1600/3200]
Validation Loss: 0.0770, Accuracy: 75.12%, F1 Score: 0.7474
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0102  [0/3200]
Loss: 0.0008  [1600/3200]
Validation Loss: 0.0667, Accuracy: 77.00%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.001 and dropout rate 0.2...
Epoch 1/60
Loss: 2.1009  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.2442  [1600/3200]
Validation Loss: 0.0695, Accuracy: 56.88%, F1 Score: 0.5045
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.4208  [0/3200]
Loss: 1.1000  [1600/3200]
Validation Loss: 0.0515, Accuracy: 69.88%, F1 Score: 0.6701
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.9436  [0/3200]
Loss: 0.7217  [1600/3200]
Validation Loss: 0.0449, Accuracy: 73.62%, F1 Score: 0.7210
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.6419  [0/3200]
Loss: 0.6198  [1600/3200]
Validation Loss: 0.0418, Accuracy: 73.25%, F1 Score: 0.7260
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.5702  [0/3200]
Loss: 0.7284  [1600/3200]
Validation Loss: 0.0394, Accuracy: 73.25%, F1 Score: 0.7084
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.1605  [0/3200]
Loss: 0.4457  [1600/3200]
Validation Loss: 0.0355, Accuracy: 78.25%, F1 Score: 0.7802
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5786  [0/3200]
Loss: 0.6682  [1600/3200]
Validation Loss: 0.0381, Accuracy: 73.25%, F1 Score: 0.7080
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.4499  [0/3200]
Loss: 0.5416  [1600/3200]
Validation Loss: 0.0350, Accuracy: 77.88%, F1 Score: 0.7745
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.6993  [0/3200]
Loss: 0.4952  [1600/3200]
Validation Loss: 0.0352, Accuracy: 79.12%, F1 Score: 0.7864
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.5125  [0/3200]
Loss: 0.5367  [1600/3200]
Validation Loss: 0.0383, Accuracy: 75.38%, F1 Score: 0.7513
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.7215  [0/3200]
Loss: 0.2548  [1600/3200]
Validation Loss: 0.0339, Accuracy: 78.12%, F1 Score: 0.7799
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.4591  [0/3200]
Loss: 0.2071  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.62%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.6123  [0/3200]
Loss: 0.4654  [1600/3200]
Validation Loss: 0.0343, Accuracy: 78.62%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.4672  [0/3200]
Loss: 0.2908  [1600/3200]
Validation Loss: 0.0336, Accuracy: 79.50%, F1 Score: 0.7972
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.5981  [0/3200]
Loss: 0.7644  [1600/3200]
Validation Loss: 0.0331, Accuracy: 80.12%, F1 Score: 0.8013
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.5521  [0/3200]
Loss: 0.3142  [1600/3200]
Validation Loss: 0.0360, Accuracy: 77.12%, F1 Score: 0.7629
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.2638  [0/3200]
Loss: 0.3239  [1600/3200]
Validation Loss: 0.0330, Accuracy: 81.00%, F1 Score: 0.8091
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.4177  [0/3200]
Loss: 0.3943  [1600/3200]
Validation Loss: 0.0338, Accuracy: 79.38%, F1 Score: 0.7907
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.3538  [0/3200]
Loss: 0.4127  [1600/3200]
Validation Loss: 0.0335, Accuracy: 81.88%, F1 Score: 0.8184
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.2804  [0/3200]
Loss: 0.1338  [1600/3200]
Validation Loss: 0.0354, Accuracy: 78.62%, F1 Score: 0.7819
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.2749  [0/3200]
Loss: 0.6712  [1600/3200]
Validation Loss: 0.0400, Accuracy: 75.88%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.7172  [0/3200]
Loss: 0.3733  [1600/3200]
Validation Loss: 0.0389, Accuracy: 77.12%, F1 Score: 0.7726
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.5392  [0/3200]
Loss: 0.0644  [1600/3200]
Validation Loss: 0.0361, Accuracy: 79.12%, F1 Score: 0.7916
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.3754  [0/3200]
Loss: 0.4156  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.38%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.1751  [0/3200]
Loss: 0.5139  [1600/3200]
Validation Loss: 0.0466, Accuracy: 74.00%, F1 Score: 0.7275
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.5672  [0/3200]
Loss: 0.1865  [1600/3200]
Validation Loss: 0.0354, Accuracy: 80.50%, F1 Score: 0.8047
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.2539  [0/3200]
Loss: 0.6275  [1600/3200]
Validation Loss: 0.0384, Accuracy: 79.25%, F1 Score: 0.7921
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.1117  [0/3200]
Loss: 0.4815  [1600/3200]
Validation Loss: 0.0363, Accuracy: 80.62%, F1 Score: 0.8055
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.1489  [0/3200]
Loss: 0.2222  [1600/3200]
Validation Loss: 0.0366, Accuracy: 81.00%, F1 Score: 0.8068
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.2459  [0/3200]
Loss: 0.3369  [1600/3200]
Validation Loss: 0.0385, Accuracy: 79.00%, F1 Score: 0.7847
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.4096  [0/3200]
Loss: 0.2404  [1600/3200]
Validation Loss: 0.0372, Accuracy: 80.50%, F1 Score: 0.8030
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0372  [0/3200]
Loss: 0.2315  [1600/3200]
Validation Loss: 0.0409, Accuracy: 77.00%, F1 Score: 0.7628
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.1943  [0/3200]
Loss: 0.1269  [1600/3200]
Validation Loss: 0.0415, Accuracy: 80.25%, F1 Score: 0.8007
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0821  [0/3200]
Loss: 0.2811  [1600/3200]
Validation Loss: 0.0399, Accuracy: 79.88%, F1 Score: 0.7995
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.1179  [0/3200]
Loss: 0.0695  [1600/3200]
Validation Loss: 0.0401, Accuracy: 78.75%, F1 Score: 0.7883
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.2020  [0/3200]
Loss: 0.2547  [1600/3200]
Validation Loss: 0.0426, Accuracy: 78.75%, F1 Score: 0.7849
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.3582  [0/3200]
Loss: 0.1421  [1600/3200]
Validation Loss: 0.0423, Accuracy: 78.00%, F1 Score: 0.7758
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0164  [0/3200]
Loss: 0.0868  [1600/3200]
Validation Loss: 0.0426, Accuracy: 80.62%, F1 Score: 0.8054
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0621  [0/3200]
Loss: 0.0599  [1600/3200]
Validation Loss: 0.0434, Accuracy: 80.00%, F1 Score: 0.7994
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0143  [0/3200]
Loss: 0.0572  [1600/3200]
Validation Loss: 0.0453, Accuracy: 78.62%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0678  [0/3200]
Loss: 0.1396  [1600/3200]
Validation Loss: 0.0482, Accuracy: 78.62%, F1 Score: 0.7881
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.1100  [0/3200]
Loss: 0.0869  [1600/3200]
Validation Loss: 0.0455, Accuracy: 78.38%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0504  [0/3200]
Loss: 0.1051  [1600/3200]
Validation Loss: 0.0475, Accuracy: 79.00%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0266  [0/3200]
Loss: 0.0586  [1600/3200]
Validation Loss: 0.0525, Accuracy: 77.00%, F1 Score: 0.7664
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.2018  [0/3200]
Loss: 0.0376  [1600/3200]
Validation Loss: 0.0516, Accuracy: 78.00%, F1 Score: 0.7749
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0081  [0/3200]
Loss: 0.0741  [1600/3200]
Validation Loss: 0.0517, Accuracy: 78.12%, F1 Score: 0.7777
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.1506  [0/3200]
Loss: 0.0580  [1600/3200]
Validation Loss: 0.0465, Accuracy: 80.38%, F1 Score: 0.8007
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0522  [0/3200]
Loss: 0.0184  [1600/3200]
Validation Loss: 0.0534, Accuracy: 78.38%, F1 Score: 0.7822
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.1124  [0/3200]
Loss: 0.0351  [1600/3200]
Validation Loss: 0.0673, Accuracy: 74.88%, F1 Score: 0.7371
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0664  [0/3200]
Loss: 0.0788  [1600/3200]
Validation Loss: 0.0543, Accuracy: 77.38%, F1 Score: 0.7671
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.1168  [0/3200]
Loss: 0.0058  [1600/3200]
Validation Loss: 0.0524, Accuracy: 80.00%, F1 Score: 0.7963
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0446  [0/3200]
Loss: 0.0170  [1600/3200]
Validation Loss: 0.0516, Accuracy: 78.12%, F1 Score: 0.7781
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0789  [0/3200]
Loss: 0.0124  [1600/3200]
Validation Loss: 0.0634, Accuracy: 77.25%, F1 Score: 0.7604
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0202  [0/3200]
Loss: 0.0645  [1600/3200]
Validation Loss: 0.0648, Accuracy: 76.38%, F1 Score: 0.7579
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0111  [0/3200]
Loss: 0.1298  [1600/3200]
Validation Loss: 0.0580, Accuracy: 78.25%, F1 Score: 0.7775
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0248  [0/3200]
Loss: 0.0720  [1600/3200]
Validation Loss: 0.0551, Accuracy: 79.88%, F1 Score: 0.7979
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0834  [0/3200]
Loss: 0.0341  [1600/3200]
Validation Loss: 0.0687, Accuracy: 74.75%, F1 Score: 0.7472
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.1263  [0/3200]
Loss: 0.0230  [1600/3200]
Validation Loss: 0.0561, Accuracy: 79.25%, F1 Score: 0.7895
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0489  [0/3200]
Loss: 0.0174  [1600/3200]
Validation Loss: 0.0572, Accuracy: 79.25%, F1 Score: 0.7916
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0095  [0/3200]
Loss: 0.0465  [1600/3200]
Validation Loss: 0.0587, Accuracy: 81.38%, F1 Score: 0.8112
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with weight decay 0.001 and dropout rate 0.5...
Epoch 1/60
Loss: 3.1001  [0/3200]

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

Loss: 1.4504  [1600/3200]
Validation Loss: 0.0849, Accuracy: 27.38%, F1 Score: 0.1465
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.4939  [0/3200]
Loss: 1.3183  [1600/3200]
Validation Loss: 0.0759, Accuracy: 60.00%, F1 Score: 0.5280
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 1.3151  [0/3200]
Loss: 1.2880  [1600/3200]
Validation Loss: 0.0632, Accuracy: 61.50%, F1 Score: 0.5584
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 1.1709  [0/3200]
Loss: 0.9896  [1600/3200]
Validation Loss: 0.0542, Accuracy: 68.75%, F1 Score: 0.6743
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.9072  [0/3200]
Loss: 0.8550  [1600/3200]
Validation Loss: 0.0489, Accuracy: 69.38%, F1 Score: 0.6820
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.1107  [0/3200]
Loss: 0.9405  [1600/3200]
Validation Loss: 0.0472, Accuracy: 72.12%, F1 Score: 0.7286
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.9083  [0/3200]
Loss: 0.8487  [1600/3200]
Validation Loss: 0.0436, Accuracy: 69.50%, F1 Score: 0.6705
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.6690  [0/3200]
Loss: 0.7761  [1600/3200]
Validation Loss: 0.0405, Accuracy: 73.88%, F1 Score: 0.7306
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.6716  [0/3200]
Loss: 0.8066  [1600/3200]
Validation Loss: 0.0384, Accuracy: 77.38%, F1 Score: 0.7684
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.6711  [0/3200]
Loss: 0.5730  [1600/3200]
Validation Loss: 0.0389, Accuracy: 74.75%, F1 Score: 0.7428
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.5549  [0/3200]
Loss: 0.3013  [1600/3200]
Validation Loss: 0.0376, Accuracy: 75.00%, F1 Score: 0.7361
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.4537  [0/3200]
Loss: 0.5395  [1600/3200]
Validation Loss: 0.0343, Accuracy: 78.75%, F1 Score: 0.7854
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.7460  [0/3200]
Loss: 0.7304  [1600/3200]
Validation Loss: 0.0376, Accuracy: 77.25%, F1 Score: 0.7729
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.5040  [0/3200]
Loss: 0.4323  [1600/3200]
Validation Loss: 0.0349, Accuracy: 79.88%, F1 Score: 0.7977
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.4920  [0/3200]
Loss: 0.8025  [1600/3200]
Validation Loss: 0.0341, Accuracy: 78.88%, F1 Score: 0.7849
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.7957  [0/3200]
Loss: 0.4555  [1600/3200]
Validation Loss: 0.0370, Accuracy: 74.25%, F1 Score: 0.7251
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.4854  [0/3200]
Loss: 0.4391  [1600/3200]
Validation Loss: 0.0327, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.6859  [0/3200]
Loss: 0.6219  [1600/3200]
Validation Loss: 0.0337, Accuracy: 79.38%, F1 Score: 0.7941
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.8664  [0/3200]
Loss: 0.5680  [1600/3200]
Validation Loss: 0.0369, Accuracy: 77.88%, F1 Score: 0.7801
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.4081  [0/3200]
Loss: 0.2529  [1600/3200]
Validation Loss: 0.0336, Accuracy: 78.75%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.3643  [0/3200]
Loss: 0.7715  [1600/3200]
Validation Loss: 0.0332, Accuracy: 78.88%, F1 Score: 0.7830
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.9348  [0/3200]
Loss: 0.5801  [1600/3200]
Validation Loss: 0.0330, Accuracy: 79.50%, F1 Score: 0.7949
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.7590  [0/3200]
Loss: 0.4111  [1600/3200]
Validation Loss: 0.0340, Accuracy: 78.88%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.8481  [0/3200]
Loss: 0.6503  [1600/3200]
Validation Loss: 0.0443, Accuracy: 73.12%, F1 Score: 0.7286
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.4934  [0/3200]
Loss: 0.4956  [1600/3200]
Validation Loss: 0.0330, Accuracy: 78.88%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.8020  [0/3200]
Loss: 0.2887  [1600/3200]
Validation Loss: 0.0329, Accuracy: 79.25%, F1 Score: 0.7937
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.5526  [0/3200]
Loss: 0.9668  [1600/3200]
Validation Loss: 0.0325, Accuracy: 79.75%, F1 Score: 0.7950
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.4083  [0/3200]
Loss: 1.1312  [1600/3200]
Validation Loss: 0.0326, Accuracy: 79.62%, F1 Score: 0.7958
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.3253  [0/3200]
Loss: 0.3139  [1600/3200]
Validation Loss: 0.0321, Accuracy: 80.50%, F1 Score: 0.8055
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.6521  [0/3200]
Loss: 0.3754  [1600/3200]
Validation Loss: 0.0377, Accuracy: 75.75%, F1 Score: 0.7364
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.9842  [0/3200]
Loss: 0.3136  [1600/3200]
Validation Loss: 0.0336, Accuracy: 79.12%, F1 Score: 0.7885
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.3526  [0/3200]
Loss: 0.3343  [1600/3200]
Validation Loss: 0.0327, Accuracy: 79.62%, F1 Score: 0.7976
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.5455  [0/3200]
Loss: 0.4579  [1600/3200]
Validation Loss: 0.0342, Accuracy: 78.88%, F1 Score: 0.7864
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.2692  [0/3200]
Loss: 0.5081  [1600/3200]
Validation Loss: 0.0333, Accuracy: 79.12%, F1 Score: 0.7897
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.4317  [0/3200]
Loss: 0.4004  [1600/3200]
Validation Loss: 0.0358, Accuracy: 78.38%, F1 Score: 0.7818
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.4941  [0/3200]
Loss: 0.2996  [1600/3200]
Validation Loss: 0.0370, Accuracy: 76.75%, F1 Score: 0.7542
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.5261  [0/3200]
Loss: 0.4256  [1600/3200]
Validation Loss: 0.0330, Accuracy: 81.00%, F1 Score: 0.8099
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.2321  [0/3200]
Loss: 0.2842  [1600/3200]
Validation Loss: 0.0344, Accuracy: 80.12%, F1 Score: 0.8004
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.4489  [0/3200]
Loss: 0.3652  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.75%, F1 Score: 0.7865
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.2618  [0/3200]
Loss: 0.4436  [1600/3200]
Validation Loss: 0.0366, Accuracy: 77.88%, F1 Score: 0.7742
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.2728  [0/3200]
Loss: 0.4991  [1600/3200]
Validation Loss: 0.0331, Accuracy: 80.50%, F1 Score: 0.8043
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.3725  [0/3200]
Loss: 0.3336  [1600/3200]
Validation Loss: 0.0375, Accuracy: 76.50%, F1 Score: 0.7609
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.3921  [0/3200]
Loss: 0.1663  [1600/3200]
Validation Loss: 0.0340, Accuracy: 80.88%, F1 Score: 0.8078
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.1057  [0/3200]
Loss: 0.3154  [1600/3200]
Validation Loss: 0.0345, Accuracy: 80.88%, F1 Score: 0.8081
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.3976  [0/3200]
Loss: 0.3482  [1600/3200]
Validation Loss: 0.0395, Accuracy: 78.38%, F1 Score: 0.7721
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.1079  [0/3200]
Loss: 0.2638  [1600/3200]
Validation Loss: 0.0364, Accuracy: 80.50%, F1 Score: 0.8060
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.4101  [0/3200]
Loss: 0.3028  [1600/3200]
Validation Loss: 0.0359, Accuracy: 78.50%, F1 Score: 0.7830
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.3830  [0/3200]
Loss: 0.0681  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.38%, F1 Score: 0.7952
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.2730  [0/3200]
Loss: 0.1154  [1600/3200]
Validation Loss: 0.0381, Accuracy: 80.50%, F1 Score: 0.8018
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.3251  [0/3200]
Loss: 0.5117  [1600/3200]
Validation Loss: 0.0396, Accuracy: 77.50%, F1 Score: 0.7710
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.3051  [0/3200]
Loss: 0.2276  [1600/3200]
Validation Loss: 0.0383, Accuracy: 78.75%, F1 Score: 0.7822
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.3387  [0/3200]
Loss: 0.1433  [1600/3200]
Validation Loss: 0.0367, Accuracy: 80.38%, F1 Score: 0.8043
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.4019  [0/3200]
Loss: 0.0634  [1600/3200]
Validation Loss: 0.0384, Accuracy: 79.00%, F1 Score: 0.7896
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.1134  [0/3200]
Loss: 0.2907  [1600/3200]
Validation Loss: 0.0443, Accuracy: 76.25%, F1 Score: 0.7589
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.2875  [0/3200]
Loss: 0.2825  [1600/3200]
Validation Loss: 0.0436, Accuracy: 79.25%, F1 Score: 0.7878
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.1150  [0/3200]
Loss: 0.1863  [1600/3200]
Validation Loss: 0.0405, Accuracy: 79.25%, F1 Score: 0.7885
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.3229  [0/3200]
Loss: 0.2699  [1600/3200]
Validation Loss: 0.0444, Accuracy: 77.25%, F1 Score: 0.7690
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.4928  [0/3200]
Loss: 0.2083  [1600/3200]
Validation Loss: 0.0423, Accuracy: 80.50%, F1 Score: 0.8039
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.5831  [0/3200]
Loss: 0.2697  [1600/3200]
Validation Loss: 0.0470, Accuracy: 76.62%, F1 Score: 0.7675
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.3724  [0/3200]
Loss: 0.5915  [1600/3200]
Validation Loss: 0.0414, Accuracy: 79.00%, F1 Score: 0.7887
Adjusting learning rate of group 0 to 8.1316e-05.

<ipython-input-40-e1ed024e5909>:114: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Weight Decay': weight_decay, 'Dropout Rate': dropout_rate,

results_df


Άρα ο συνδιασμός Βatch Νormalization και Regularization για Weight Decay = 0.000 και Dropout Rate=0.5 με Accuracy=81.625% | F1=81.33%, πλησιάζουν την απόδοση του CNN χωρίς Βatch Νormalization και Regularization με Accuracy=81.75% | F1=81.8109%.

Το βέλτιστο μοντέλο δε βελτιώνει την απόδοσή του αλλά πλησιάζει στις ίδιες περίπου τιμές με dropout rate=0.2 & weight decay= 0 με Accuracy=81.750% | F1=81.40% έναντι του απλού βέλτιστου (CNN_pp) όπου Accuracy=81.75% | F1=81.8109%.

Σε αυτήν την διερεύνηση έχουμε λάβει υπόψιν τις καλύτερες αποδόσεις απο όλο το εύρος των epochs και για αυτό δεν κρίθηκε απαραίτητο να παρατηρούμε τις υπόλοιπες μετρικές. Ο λόγος που επιλέξαμε αυτήν την προσέγγιση είναι επειδή υπάρχει drop out στα παρακάτω ερωτήματα και θεωρητικά απ τη στιγμή που εφαρμόζουμε Reproducibility πρέπει να λαμβάνουμε υπόψιν το βέλτιστο μοντέλο απο όλες τις μεθόδους βελτιστοποίησης.

Παρα ταύτα θα συνεχίσουμε χρησιμοποιώντας και τα δύο μοντέλα με και χωρις Βatch Νormalization, Regularizatio και dropout rate.

Βήμα 7: Training efficiency

Batch size

Μία μεταβλητή εκπαίδευσης που μπορεί να επηρεάσει την τελική απόδοση του μοντέλου, αλλά και τον χρόνο εκπαίδευσης είναι το batch size. Δοκιμάστε να θέσετε ως batch size τις 7 πρώτες δυνάμεις του 2 και να τυπώσετε τόσο την απόδοση όσο και τον χρόνο εκτέλεσης. Σχολιάστε τα αποτελέσματά σας.

Early stopping

Κατά την διάρκεια των πειραμάτων της εργασίας αυτής παρατηρούμε ότι αρκετές φορές, το καλύτερο στιγμιότυπο του μοντέλου πετυχαίνεται αρκετά πριν τον αριθμό εποχών που έχουμε ορίσει. Επομένως η συνέχιση της εκπαίδευσης μέχρι να φτάσουμε τον τελικό αριθμό εποχών κάνει την διαδικασία και περισσότερο χρονοβόρα αλλά προκαλεί και κατασπατάληση πόρων, κάτι το οποίο είναι μη επιθυμητό ειδικά σε σερβερς που πολλοί χρήστες επιθυμούν να έχουν πρόσβαση στις GPU. Για τον λόγο αυτό ο αριθμός εποχών που θέτουμε θα πρέπει να θεωρείται ως “ο μέγιστος αριθμός εποχών”. Επομένως, ενσωματώστε στην διαδικασία εκπαίδευσης και το early stopping, δηλαδή μία συνθήκη τερματισμού της εκπαίδευσης σε περίπτωση που η επίδοση του μοντέλου στο validation set για ένα συνεχόμενο (patience) αριθμό εποχών (π.χ. 7) δεν βελτιώνεται. Παρατηρείτε διαφορά στον χρόνο εκτέλεσης για διαφορετικές τιμές του patience;

import time

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define the list of batch sizes
batch_sizes = [2, 2**2, 2**3, 2**4, 2**5, 2**6, 2**7]

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=['Batch Size', 'Best Accuracy', 'Best F1 Score', 'Time (s)'])

# Iterate over batch sizes
for batch_size in batch_sizes:
    # Set the random seeds for reproducibility within the loop
    torch.manual_seed(SEED)
    random.seed(SEED)
    np.random.seed(SEED)
    torch.cuda.manual_seed_all(SEED)

    # Define the data and loaders with the current batch size
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

    # Initialize the model
    model = CNN_pp_re(output_dim=4, dropout_rate=0.5).to(device)
    model.apply(weights_init)  # Initialize the weights

    optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0)

    # Create the scheduler based on the scheduler name# Create the scheduler based on the scheduler name
    if scheduler_name == 'OneCycleLR':
      scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader), verbose=True)


    # Train the model
    print(f"Training with batch size {batch_size}...")
    best_accuracy = 0.0
    best_f1 = 0.0
    start_time = time.time()
    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        train_loop(train_loader, model, loss_fn, optimizer)
        test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
        print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

        # Update the best accuracy and F1 score
        if accuracy > best_accuracy:
            best_accuracy = accuracy
        if f1 > best_f1:
            best_f1 = f1

        scheduler.step()  # Update the learning rate

    end_time = time.time()
    elapsed_time = end_time - start_time

    # Add the results to the DataFrame
    results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,
                                    'Best F1 Score': best_f1, 'Time (s)': elapsed_time}, ignore_index=True)

Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 2...
Epoch 1/60
Loss: 1.2100  [0/3200]
Loss: 1.2606  [200/3200]
Loss: 1.2864  [400/3200]
Loss: 1.2835  [600/3200]
Loss: 1.0463  [800/3200]
Loss: 0.4774  [1000/3200]
Loss: 1.1214  [1200/3200]
Loss: 0.6245  [1400/3200]
Loss: 1.8993  [1600/3200]
Loss: 1.6174  [1800/3200]
Loss: 0.0628  [2000/3200]
Loss: 0.8326  [2200/3200]
Loss: 0.4281  [2400/3200]
Loss: 0.6367  [2600/3200]
Loss: 1.3671  [2800/3200]
Loss: 0.4251  [3000/3200]
Validation Loss: 0.3901, Accuracy: 66.00%, F1 Score: 0.6189
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.7211  [0/3200]
Loss: 0.7803  [200/3200]
Loss: 0.5722  [400/3200]
Loss: 0.4915  [600/3200]
Loss: 0.6504  [800/3200]
Loss: 1.0369  [1000/3200]
Loss: 1.5362  [1200/3200]
Loss: 0.8514  [1400/3200]
Loss: 0.4828  [1600/3200]
Loss: 1.3576  [1800/3200]
Loss: 0.5404  [2000/3200]
Loss: 0.4745  [2200/3200]
Loss: 0.5370  [2400/3200]
Loss: 0.5483  [2600/3200]
Loss: 0.1712  [2800/3200]
Loss: 3.0682  [3000/3200]
Validation Loss: 0.3412, Accuracy: 71.88%, F1 Score: 0.7089
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 3/60
Loss: 0.3850  [0/3200]
Loss: 0.1272  [200/3200]
Loss: 1.1251  [400/3200]
Loss: 1.1458  [600/3200]
Loss: 0.1366  [800/3200]
Loss: 1.2008  [1000/3200]
Loss: 0.4890  [1200/3200]
Loss: 0.8387  [1400/3200]
Loss: 0.2216  [1600/3200]
Loss: 0.4279  [1800/3200]
Loss: 0.0933  [2000/3200]
Loss: 0.2331  [2200/3200]
Loss: 0.7282  [2400/3200]
Loss: 2.0017  [2600/3200]
Loss: 0.1551  [2800/3200]
Loss: 0.1520  [3000/3200]
Validation Loss: 0.3269, Accuracy: 73.62%, F1 Score: 0.7335
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 4/60
Loss: 0.9634  [0/3200]
Loss: 0.5139  [200/3200]
Loss: 0.6523  [400/3200]
Loss: 0.4571  [600/3200]
Loss: 0.1220  [800/3200]
Loss: 0.9102  [1000/3200]
Loss: 0.0349  [1200/3200]
Loss: 0.0302  [1400/3200]
Loss: 0.1779  [1600/3200]
Loss: 2.2793  [1800/3200]
Loss: 1.2496  [2000/3200]
Loss: 0.2128  [2200/3200]
Loss: 0.0393  [2400/3200]
Loss: 0.3363  [2600/3200]
Loss: 0.3864  [2800/3200]
Loss: 0.1710  [3000/3200]
Validation Loss: 0.2952, Accuracy: 77.88%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 5/60
Loss: 1.7911  [0/3200]
Loss: 0.1461  [200/3200]
Loss: 0.3847  [400/3200]
Loss: 0.2123  [600/3200]
Loss: 0.9943  [800/3200]
Loss: 0.1872  [1000/3200]
Loss: 0.4282  [1200/3200]
Loss: 0.1069  [1400/3200]
Loss: 1.2023  [1600/3200]
Loss: 0.2660  [1800/3200]
Loss: 1.2397  [2000/3200]
Loss: 0.0086  [2200/3200]
Loss: 0.4174  [2400/3200]
Loss: 0.4971  [2600/3200]
Loss: 0.0968  [2800/3200]
Loss: 1.5640  [3000/3200]
Validation Loss: 0.2898, Accuracy: 78.88%, F1 Score: 0.7842
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 6/60
Loss: 0.0579  [0/3200]
Loss: 0.9974  [200/3200]
Loss: 0.3244  [400/3200]
Loss: 0.5378  [600/3200]
Loss: 0.4169  [800/3200]
Loss: 0.0216  [1000/3200]
Loss: 0.6962  [1200/3200]
Loss: 0.4992  [1400/3200]
Loss: 0.1866  [1600/3200]
Loss: 0.4217  [1800/3200]
Loss: 0.2203  [2000/3200]
Loss: 0.8205  [2200/3200]
Loss: 0.0575  [2400/3200]
Loss: 0.0814  [2600/3200]
Loss: 0.1064  [2800/3200]
Loss: 0.7314  [3000/3200]
Validation Loss: 0.3273, Accuracy: 76.25%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 7/60
Loss: 0.7909  [0/3200]
Loss: 0.5242  [200/3200]
Loss: 0.2249  [400/3200]
Loss: 0.1564  [600/3200]
Loss: 0.9958  [800/3200]
Loss: 0.0324  [1000/3200]
Loss: 1.9750  [1200/3200]
Loss: 1.0493  [1400/3200]
Loss: 0.3585  [1600/3200]
Loss: 0.3337  [1800/3200]
Loss: 0.6536  [2000/3200]
Loss: 0.9522  [2200/3200]
Loss: 0.1878  [2400/3200]
Loss: 0.4998  [2600/3200]
Loss: 0.9132  [2800/3200]
Loss: 3.2317  [3000/3200]
Validation Loss: 0.2964, Accuracy: 77.12%, F1 Score: 0.7628
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 8/60
Loss: 0.2366  [0/3200]
Loss: 0.0543  [200/3200]
Loss: 0.1506  [400/3200]
Loss: 0.2674  [600/3200]
Loss: 0.2030  [800/3200]
Loss: 0.4837  [1000/3200]
Loss: 0.7837  [1200/3200]
Loss: 0.3080  [1400/3200]
Loss: 0.0457  [1600/3200]
Loss: 0.1568  [1800/3200]
Loss: 0.0242  [2000/3200]
Loss: 0.1433  [2200/3200]
Loss: 0.1216  [2400/3200]
Loss: 0.9543  [2600/3200]
Loss: 0.0212  [2800/3200]
Loss: 0.0244  [3000/3200]
Validation Loss: 0.3249, Accuracy: 76.38%, F1 Score: 0.7581
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 9/60
Loss: 1.0463  [0/3200]
Loss: 0.8497  [200/3200]
Loss: 0.9849  [400/3200]
Loss: 0.0051  [600/3200]
Loss: 0.1755  [800/3200]
Loss: 0.0960  [1000/3200]
Loss: 0.6329  [1200/3200]
Loss: 1.4823  [1400/3200]
Loss: 0.8183  [1600/3200]
Loss: 0.6242  [1800/3200]
Loss: 0.6843  [2000/3200]
Loss: 0.2842  [2200/3200]
Loss: 0.0777  [2400/3200]
Loss: 0.6404  [2600/3200]
Loss: 0.1146  [2800/3200]
Loss: 0.9850  [3000/3200]
Validation Loss: 0.3094, Accuracy: 76.50%, F1 Score: 0.7547
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 10/60
Loss: 0.2161  [0/3200]
Loss: 0.1957  [200/3200]
Loss: 0.0076  [400/3200]
Loss: 0.0462  [600/3200]
Loss: 0.3096  [800/3200]
Loss: 0.4729  [1000/3200]
Loss: 0.0072  [1200/3200]
Loss: 0.3974  [1400/3200]
Loss: 0.2264  [1600/3200]
Loss: 0.0885  [1800/3200]
Loss: 0.1464  [2000/3200]
Loss: 0.6702  [2200/3200]
Loss: 0.2954  [2400/3200]
Loss: 0.1454  [2600/3200]
Loss: 3.0377  [2800/3200]
Loss: 0.6284  [3000/3200]
Validation Loss: 0.3174, Accuracy: 77.25%, F1 Score: 0.7735
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 11/60
Loss: 0.3479  [0/3200]
Loss: 0.4533  [200/3200]
Loss: 0.9538  [400/3200]
Loss: 0.1240  [600/3200]
Loss: 0.0030  [800/3200]
Loss: 0.0101  [1000/3200]
Loss: 0.0178  [1200/3200]
Loss: 0.1352  [1400/3200]
Loss: 0.0853  [1600/3200]
Loss: 0.8594  [1800/3200]
Loss: 0.0463  [2000/3200]
Loss: 0.4357  [2200/3200]
Loss: 0.0581  [2400/3200]
Loss: 0.0458  [2600/3200]
Loss: 0.0124  [2800/3200]
Loss: 0.0157  [3000/3200]
Validation Loss: 0.3189, Accuracy: 76.88%, F1 Score: 0.7691
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 12/60
Loss: 0.0003  [0/3200]
Loss: 0.0791  [200/3200]
Loss: 0.0005  [400/3200]
Loss: 0.0122  [600/3200]
Loss: 0.0151  [800/3200]
Loss: 0.0012  [1000/3200]
Loss: 0.0964  [1200/3200]
Loss: 0.0563  [1400/3200]
Loss: 0.1996  [1600/3200]
Loss: 0.5960  [1800/3200]
Loss: 0.1369  [2000/3200]
Loss: 0.1055  [2200/3200]
Loss: 0.4678  [2400/3200]
Loss: 1.2627  [2600/3200]
Loss: 0.4161  [2800/3200]
Loss: 0.7365  [3000/3200]
Validation Loss: 0.3074, Accuracy: 78.25%, F1 Score: 0.7831
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 13/60
Loss: 0.0069  [0/3200]
Loss: 0.0010  [200/3200]
Loss: 0.0611  [400/3200]
Loss: 0.1628  [600/3200]
Loss: 0.0583  [800/3200]
Loss: 0.1004  [1000/3200]
Loss: 0.4026  [1200/3200]
Loss: 0.0297  [1400/3200]
Loss: 0.0056  [1600/3200]
Loss: 0.0004  [1800/3200]
Loss: 0.1180  [2000/3200]
Loss: 0.0010  [2200/3200]
Loss: 0.0084  [2400/3200]
Loss: 1.3725  [2600/3200]
Loss: 0.1535  [2800/3200]
Loss: 0.0761  [3000/3200]
Validation Loss: 0.3450, Accuracy: 76.50%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 14/60
Loss: 0.0032  [0/3200]
Loss: 0.0362  [200/3200]
Loss: 0.0184  [400/3200]
Loss: 0.1104  [600/3200]
Loss: 0.0847  [800/3200]
Loss: 0.0269  [1000/3200]
Loss: 0.0866  [1200/3200]
Loss: 0.1697  [1400/3200]
Loss: 0.0169  [1600/3200]
Loss: 0.0029  [1800/3200]
Loss: 0.0394  [2000/3200]
Loss: 0.0114  [2200/3200]
Loss: 0.5500  [2400/3200]
Loss: 0.0014  [2600/3200]
Loss: 0.2342  [2800/3200]
Loss: 0.0030  [3000/3200]
Validation Loss: 0.3326, Accuracy: 78.00%, F1 Score: 0.7805
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 15/60
Loss: 0.0085  [0/3200]
Loss: 0.0319  [200/3200]
Loss: 0.0238  [400/3200]
Loss: 0.5709  [600/3200]
Loss: 0.6172  [800/3200]
Loss: 0.0078  [1000/3200]
Loss: 0.0643  [1200/3200]
Loss: 0.0513  [1400/3200]
Loss: 0.0700  [1600/3200]
Loss: 0.0626  [1800/3200]
Loss: 0.1313  [2000/3200]
Loss: 0.0034  [2200/3200]
Loss: 0.3659  [2400/3200]
Loss: 0.0330  [2600/3200]
Loss: 0.4691  [2800/3200]
Loss: 0.0600  [3000/3200]
Validation Loss: 0.3704, Accuracy: 77.62%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 16/60
Loss: 0.0144  [0/3200]
Loss: 0.0220  [200/3200]
Loss: 0.3450  [400/3200]
Loss: 0.0251  [600/3200]
Loss: 0.0010  [800/3200]
Loss: 0.0137  [1000/3200]
Loss: 0.0159  [1200/3200]
Loss: 0.0005  [1400/3200]
Loss: 0.0635  [1600/3200]
Loss: 0.0130  [1800/3200]
Loss: 0.0025  [2000/3200]
Loss: 0.0280  [2200/3200]
Loss: 0.0157  [2400/3200]
Loss: 0.0784  [2600/3200]
Loss: 0.0348  [2800/3200]
Loss: 0.4250  [3000/3200]
Validation Loss: 0.3516, Accuracy: 77.50%, F1 Score: 0.7719
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 17/60
Loss: 0.0034  [0/3200]
Loss: 1.4056  [200/3200]
Loss: 0.8442  [400/3200]
Loss: 0.0041  [600/3200]
Loss: 0.0059  [800/3200]
Loss: 0.0184  [1000/3200]
Loss: 0.0035  [1200/3200]
Loss: 0.0115  [1400/3200]
Loss: 0.0047  [1600/3200]
Loss: 0.0175  [1800/3200]
Loss: 0.0061  [2000/3200]
Loss: 0.0535  [2200/3200]
Loss: 0.0017  [2400/3200]
Loss: 0.6467  [2600/3200]
Loss: 0.0136  [2800/3200]
Loss: 0.0038  [3000/3200]
Validation Loss: 0.3912, Accuracy: 76.88%, F1 Score: 0.7646
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 18/60
Loss: 0.0689  [0/3200]
Loss: 0.0181  [200/3200]
Loss: 0.2792  [400/3200]
Loss: 0.0097  [600/3200]
Loss: 0.0137  [800/3200]
Loss: 0.1968  [1000/3200]
Loss: 0.0385  [1200/3200]
Loss: 0.0066  [1400/3200]
Loss: 0.0333  [1600/3200]
Loss: 0.0013  [1800/3200]
Loss: 0.1030  [2000/3200]
Loss: 0.0016  [2200/3200]
Loss: 0.5179  [2400/3200]
Loss: 0.0151  [2600/3200]
Loss: 0.0145  [2800/3200]
Loss: 0.0167  [3000/3200]
Validation Loss: 0.3819, Accuracy: 77.50%, F1 Score: 0.7736
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 19/60
Loss: 0.0893  [0/3200]
Loss: 0.4530  [200/3200]
Loss: 0.0263  [400/3200]
Loss: 0.0646  [600/3200]
Loss: 0.0039  [800/3200]
Loss: 0.0002  [1000/3200]
Loss: 0.0050  [1200/3200]
Loss: 0.0286  [1400/3200]
Loss: 0.2847  [1600/3200]
Loss: 0.0006  [1800/3200]
Loss: 0.0004  [2000/3200]
Loss: 0.0017  [2200/3200]
Loss: 0.0006  [2400/3200]
Loss: 0.0013  [2600/3200]
Loss: 0.0049  [2800/3200]
Loss: 0.0011  [3000/3200]
Validation Loss: 0.4276, Accuracy: 75.88%, F1 Score: 0.7540
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 20/60
Loss: 0.1456  [0/3200]
Loss: 0.2411  [200/3200]
Loss: 0.0018  [400/3200]
Loss: 0.0068  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0009  [1000/3200]
Loss: 0.4213  [1200/3200]
Loss: 0.0251  [1400/3200]
Loss: 0.0012  [1600/3200]
Loss: 0.0060  [1800/3200]
Loss: 0.0016  [2000/3200]
Loss: 0.0128  [2200/3200]
Loss: 0.0036  [2400/3200]
Loss: 0.1017  [2600/3200]
Loss: 0.0014  [2800/3200]
Loss: 0.0448  [3000/3200]
Validation Loss: 0.4633, Accuracy: 76.12%, F1 Score: 0.7540
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 21/60
Loss: 0.0326  [0/3200]
Loss: 0.0179  [200/3200]
Loss: 0.0174  [400/3200]
Loss: 0.3333  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.1720  [1000/3200]
Loss: 0.0016  [1200/3200]
Loss: 0.0028  [1400/3200]
Loss: 0.0209  [1600/3200]
Loss: 0.0016  [1800/3200]
Loss: 1.6027  [2000/3200]
Loss: 0.0044  [2200/3200]
Loss: 0.1140  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0019  [2800/3200]
Loss: 0.0041  [3000/3200]
Validation Loss: 0.4820, Accuracy: 76.38%, F1 Score: 0.7563
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 22/60
Loss: 0.0002  [0/3200]
Loss: 0.0017  [200/3200]
Loss: 0.0041  [400/3200]
Loss: 0.0013  [600/3200]
Loss: 0.0008  [800/3200]
Loss: 0.3321  [1000/3200]
Loss: 0.0021  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0019  [1600/3200]
Loss: 0.0008  [1800/3200]
Loss: 0.3781  [2000/3200]
Loss: 0.0023  [2200/3200]
Loss: 0.0055  [2400/3200]
Loss: 0.0002  [2600/3200]
Loss: 0.0278  [2800/3200]
Loss: 0.0004  [3000/3200]
Validation Loss: 0.4021, Accuracy: 78.25%, F1 Score: 0.7797
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 23/60
Loss: 0.0103  [0/3200]
Loss: 0.1366  [200/3200]
Loss: 0.0394  [400/3200]
Loss: 0.0006  [600/3200]
Loss: 0.2462  [800/3200]
Loss: 0.0007  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0044  [1400/3200]
Loss: 1.2410  [1600/3200]
Loss: 0.0003  [1800/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0026  [2200/3200]
Loss: 0.1039  [2400/3200]
Loss: 0.0029  [2600/3200]
Loss: 0.0394  [2800/3200]
Loss: 0.0021  [3000/3200]
Validation Loss: 0.4508, Accuracy: 78.25%, F1 Score: 0.7778
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 24/60
Loss: 0.0143  [0/3200]
Loss: 0.0992  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0015  [600/3200]
Loss: 0.0079  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0198  [1200/3200]
Loss: 0.0011  [1400/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0094  [2000/3200]
Loss: 0.0748  [2200/3200]
Loss: 0.0062  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0027  [2800/3200]
Loss: 0.0002  [3000/3200]
Validation Loss: 0.4407, Accuracy: 77.12%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 25/60
Loss: 0.0013  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0587  [400/3200]
Loss: 0.0002  [600/3200]
Loss: 0.0009  [800/3200]
Loss: 0.0005  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0090  [1400/3200]
Loss: 0.0005  [1600/3200]
Loss: 0.0085  [1800/3200]
Loss: 0.0012  [2000/3200]
Loss: 0.0022  [2200/3200]
Loss: 0.0006  [2400/3200]
Loss: 0.0022  [2600/3200]
Loss: 0.0007  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.5010, Accuracy: 75.50%, F1 Score: 0.7526
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 26/60
Loss: 0.0278  [0/3200]
Loss: 0.0166  [200/3200]
Loss: 0.0339  [400/3200]
Loss: 0.0017  [600/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0005  [1000/3200]
Loss: 0.0350  [1200/3200]
Loss: 0.0019  [1400/3200]
Loss: 0.0556  [1600/3200]
Loss: 0.0186  [1800/3200]
Loss: 0.0007  [2000/3200]
Loss: 0.0650  [2200/3200]
Loss: 0.5101  [2400/3200]
Loss: 0.0010  [2600/3200]
Loss: 0.9023  [2800/3200]
Loss: 0.0039  [3000/3200]
Validation Loss: 0.4741, Accuracy: 78.75%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 27/60
Loss: 0.0053  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0097  [600/3200]
Loss: 0.0151  [800/3200]
Loss: 0.0011  [1000/3200]
Loss: 0.0004  [1200/3200]
Loss: 0.0031  [1400/3200]
Loss: 0.0039  [1600/3200]
Loss: 0.0002  [1800/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0017  [2200/3200]
Loss: 0.0033  [2400/3200]
Loss: 0.0002  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0627  [3000/3200]
Validation Loss: 0.4718, Accuracy: 77.75%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 28/60
Loss: 0.0000  [0/3200]
Loss: 0.0103  [200/3200]
Loss: 0.0247  [400/3200]
Loss: 0.0005  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0002  [1000/3200]
Loss: 0.0052  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0104  [1600/3200]
Loss: 0.0013  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0008  [2200/3200]
Loss: 0.1067  [2400/3200]
Loss: 0.0121  [2600/3200]
Loss: 0.0013  [2800/3200]
Loss: 0.0005  [3000/3200]
Validation Loss: 0.5196, Accuracy: 77.75%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 29/60
Loss: 0.1130  [0/3200]
Loss: 0.0013  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0011  [800/3200]
Loss: 0.0039  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0014  [1800/3200]
Loss: 0.0105  [2000/3200]
Loss: 0.0105  [2200/3200]
Loss: 0.0024  [2400/3200]
Loss: 0.0132  [2600/3200]
Loss: 0.0033  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.4990, Accuracy: 78.50%, F1 Score: 0.7810
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 30/60
Loss: 0.0008  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0042  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0116  [800/3200]
Loss: 0.0058  [1000/3200]
Loss: 0.0003  [1200/3200]
Loss: 0.0002  [1400/3200]
Loss: 0.0030  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0059  [2000/3200]
Loss: 0.0106  [2200/3200]
Loss: 0.0003  [2400/3200]
Loss: 0.0120  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.5091, Accuracy: 78.50%, F1 Score: 0.7821
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 31/60
Loss: 0.0004  [0/3200]
Loss: 0.0008  [200/3200]
Loss: 0.0090  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0018  [1200/3200]
Loss: 0.0004  [1400/3200]
Loss: 0.0012  [1600/3200]
Loss: 0.0002  [1800/3200]
Loss: 0.0026  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.5393, Accuracy: 77.50%, F1 Score: 0.7706
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 32/60
Loss: 0.0000  [0/3200]
Loss: 0.1728  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0004  [1400/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0037  [1800/3200]
Loss: 0.1139  [2000/3200]
Loss: 0.0002  [2200/3200]
Loss: 0.0016  [2400/3200]
Loss: 0.0005  [2600/3200]
Loss: 0.0478  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.5655, Accuracy: 77.25%, F1 Score: 0.7691
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 33/60
Loss: 0.0211  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0049  [600/3200]
Loss: 0.0006  [800/3200]
Loss: 0.0006  [1000/3200]
Loss: 0.0165  [1200/3200]
Loss: 0.0062  [1400/3200]
Loss: 0.0005  [1600/3200]
Loss: 0.0020  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0012  [2200/3200]
Loss: 0.0003  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0129  [2800/3200]
Loss: 0.0002  [3000/3200]
Validation Loss: 0.5358, Accuracy: 78.12%, F1 Score: 0.7783
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 34/60
Loss: 0.0118  [0/3200]
Loss: 0.0213  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0017  [600/3200]
Loss: 0.0044  [800/3200]
Loss: 0.0019  [1000/3200]
Loss: 0.0014  [1200/3200]
Loss: 0.0084  [1400/3200]
Loss: 0.0034  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0479  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0003  [2400/3200]
Loss: 0.0005  [2600/3200]
Loss: 0.0007  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.5388, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 35/60
Loss: 0.0522  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0005  [600/3200]
Loss: 0.0012  [800/3200]
Loss: 0.0093  [1000/3200]
Loss: 0.0016  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0072  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0002  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.5719, Accuracy: 77.12%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 36/60
Loss: 0.0000  [0/3200]
Loss: 0.0735  [200/3200]
Loss: 0.0032  [400/3200]
Loss: 0.0028  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0029  [1000/3200]
Loss: 0.0127  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0006  [1800/3200]
Loss: 0.0281  [2000/3200]
Loss: 0.0008  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0003  [2800/3200]
Loss: 0.0002  [3000/3200]
Validation Loss: 0.5573, Accuracy: 77.50%, F1 Score: 0.7738
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 37/60
Loss: 0.0030  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0015  [400/3200]
Loss: 0.0046  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0013  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0002  [1400/3200]
Loss: 0.0276  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0003  [2400/3200]
Loss: 0.0597  [2600/3200]
Loss: 0.0006  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6514, Accuracy: 75.62%, F1 Score: 0.7458
Adjusting learning rate of group 0 to 8.0008e-05.
Epoch 38/60
Loss: 0.0004  [0/3200]
Loss: 0.0011  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.1712  [1200/3200]
Loss: 0.0002  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0005  [1800/3200]
Loss: 0.0129  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0024  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6234, Accuracy: 77.88%, F1 Score: 0.7725
Adjusting learning rate of group 0 to 8.0008e-05.
Epoch 39/60
Loss: 0.0005  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0018  [600/3200]
Loss: 0.0018  [800/3200]
Loss: 0.0005  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0016  [1400/3200]
Loss: 0.2776  [1600/3200]
Loss: 0.0003  [1800/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0040  [2200/3200]
Loss: 0.0005  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0002  [3000/3200]
Validation Loss: 0.5497, Accuracy: 76.62%, F1 Score: 0.7649
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 40/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0011  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0006  [1600/3200]
Loss: 0.0966  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0002  [2200/3200]
Loss: 0.0004  [2400/3200]
Loss: 0.0005  [2600/3200]
Loss: 0.0070  [2800/3200]
Loss: 0.0096  [3000/3200]
Validation Loss: 0.5745, Accuracy: 77.12%, F1 Score: 0.7690
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 41/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0009  [400/3200]
Loss: 0.0056  [600/3200]
Loss: 0.0404  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0017  [1200/3200]
Loss: 0.0002  [1400/3200]
Loss: 0.0451  [1600/3200]
Loss: 0.0018  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0014  [2200/3200]
Loss: 0.0034  [2400/3200]
Loss: 0.0009  [2600/3200]
Loss: 0.0006  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.5857, Accuracy: 77.62%, F1 Score: 0.7741
Adjusting learning rate of group 0 to 8.0010e-05.
Epoch 42/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0600  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0011  [1200/3200]
Loss: 0.0007  [1400/3200]
Loss: 0.0449  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0002  [2800/3200]
Loss: 0.0036  [3000/3200]
Validation Loss: 0.5880, Accuracy: 78.38%, F1 Score: 0.7813
Adjusting learning rate of group 0 to 8.0010e-05.
Epoch 43/60
Loss: 0.0001  [0/3200]
Loss: 0.0024  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0069  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0013  [1200/3200]
Loss: 0.0046  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0059  [1800/3200]
Loss: 0.0010  [2000/3200]
Loss: 0.0089  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0072  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6125, Accuracy: 78.00%, F1 Score: 0.7777
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 44/60
Loss: 0.0000  [0/3200]
Loss: 0.0003  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0042  [600/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.5880, Accuracy: 78.88%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 45/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0072  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0003  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0007  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6399, Accuracy: 77.25%, F1 Score: 0.7689
Adjusting learning rate of group 0 to 8.0012e-05.
Epoch 46/60
Loss: 0.0000  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0119  [400/3200]
Loss: 0.0012  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0040  [1600/3200]
Loss: 0.0026  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0002  [2800/3200]
Loss: 0.0041  [3000/3200]
Validation Loss: 0.6097, Accuracy: 78.00%, F1 Score: 0.7798
Adjusting learning rate of group 0 to 8.0012e-05.
Epoch 47/60
Loss: 0.0001  [0/3200]
Loss: 0.0062  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0014  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0006  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0016  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0013  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0078  [3000/3200]
Validation Loss: 0.6408, Accuracy: 77.62%, F1 Score: 0.7742
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 48/60
Loss: 0.0096  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0087  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0017  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.4577  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0011  [2800/3200]
Loss: 0.0051  [3000/3200]
Validation Loss: 0.6643, Accuracy: 77.38%, F1 Score: 0.7695
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 49/60
Loss: 0.0007  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0006  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0013  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6878, Accuracy: 76.50%, F1 Score: 0.7605
Adjusting learning rate of group 0 to 8.0014e-05.
Epoch 50/60
Loss: 0.0002  [0/3200]
Loss: 0.0026  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0025  [600/3200]
Loss: 0.0003  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0011  [1400/3200]
Loss: 0.0007  [1600/3200]
Loss: 0.0024  [1800/3200]
Loss: 0.0023  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0005  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7589, Accuracy: 75.62%, F1 Score: 0.7515
Adjusting learning rate of group 0 to 8.0014e-05.
Epoch 51/60
Loss: 0.0012  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0003  [800/3200]
Loss: 0.0002  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0008  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6642, Accuracy: 76.38%, F1 Score: 0.7612
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 52/60
Loss: 0.0333  [0/3200]
Loss: 0.0015  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0002  [1400/3200]
Loss: 0.0031  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0004  [2000/3200]
Loss: 0.0002  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0004  [3000/3200]
Validation Loss: 0.6920, Accuracy: 77.88%, F1 Score: 0.7774
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 53/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0045  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6692, Accuracy: 78.12%, F1 Score: 0.7781
Adjusting learning rate of group 0 to 8.0016e-05.
Epoch 54/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0004  [1000/3200]
Loss: 0.0007  [1200/3200]
Loss: 0.0215  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0004  [2800/3200]
Loss: 0.0006  [3000/3200]
Validation Loss: 0.7668, Accuracy: 77.25%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 8.0017e-05.
Epoch 55/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0284  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0018  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0055  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0008  [2400/3200]
Loss: 0.0002  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6971, Accuracy: 77.62%, F1 Score: 0.7761
Adjusting learning rate of group 0 to 8.0017e-05.
Epoch 56/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0017  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0004  [1400/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0018  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0008  [2800/3200]
Loss: 0.0006  [3000/3200]
Validation Loss: 0.7145, Accuracy: 77.75%, F1 Score: 0.7723
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 57/60
Loss: 0.0091  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0004  [1800/3200]
Loss: 0.0007  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0003  [3000/3200]
Validation Loss: 0.7201, Accuracy: 77.25%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 8.0019e-05.
Epoch 58/60
Loss: 0.0014  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0088  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0009  [1000/3200]
Loss: 0.0158  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0007  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6564, Accuracy: 79.12%, F1 Score: 0.7906
Adjusting learning rate of group 0 to 8.0019e-05.
Epoch 59/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0046  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0003  [3000/3200]
Validation Loss: 0.7341, Accuracy: 77.50%, F1 Score: 0.7751
Adjusting learning rate of group 0 to 8.0020e-05.
Epoch 60/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0006  [1000/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0026  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7343, Accuracy: 77.75%, F1 Score: 0.7750
Adjusting learning rate of group 0 to 8.0021e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 4...
Epoch 1/60
Loss: 0.8971  [0/3200]

<ipython-input-41-785969b892fd>:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Loss: 1.3020  [400/3200]
Loss: 0.8612  [800/3200]
Loss: 0.9355  [1200/3200]
Loss: 1.2483  [1600/3200]
Loss: 0.4128  [2000/3200]
Loss: 0.7323  [2400/3200]
Loss: 0.9692  [2800/3200]
Validation Loss: 0.1916, Accuracy: 67.62%, F1 Score: 0.6342
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.4196  [0/3200]
Loss: 0.2411  [400/3200]
Loss: 0.5588  [800/3200]
Loss: 0.8708  [1200/3200]
Loss: 0.7554  [1600/3200]
Loss: 0.8995  [2000/3200]
Loss: 1.1128  [2400/3200]
Loss: 0.1581  [2800/3200]
Validation Loss: 0.1602, Accuracy: 74.62%, F1 Score: 0.7300
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 3/60
Loss: 0.5738  [0/3200]
Loss: 0.7786  [400/3200]
Loss: 0.2582  [800/3200]
Loss: 0.3073  [1200/3200]
Loss: 0.4523  [1600/3200]
Loss: 0.5529  [2000/3200]
Loss: 0.5223  [2400/3200]
Loss: 0.0903  [2800/3200]
Validation Loss: 0.1452, Accuracy: 76.75%, F1 Score: 0.7585
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 4/60
Loss: 0.4511  [0/3200]
Loss: 0.6088  [400/3200]
Loss: 0.8176  [800/3200]
Loss: 0.2048  [1200/3200]
Loss: 0.1553  [1600/3200]
Loss: 0.4623  [2000/3200]
Loss: 0.0604  [2400/3200]
Loss: 0.5920  [2800/3200]
Validation Loss: 0.1428, Accuracy: 76.88%, F1 Score: 0.7659
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 5/60
Loss: 0.8058  [0/3200]
Loss: 0.2190  [400/3200]
Loss: 0.5856  [800/3200]
Loss: 0.4492  [1200/3200]
Loss: 0.4673  [1600/3200]
Loss: 0.3479  [2000/3200]
Loss: 0.4489  [2400/3200]
Loss: 0.5943  [2800/3200]
Validation Loss: 0.1444, Accuracy: 78.75%, F1 Score: 0.7816
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 6/60
Loss: 0.1427  [0/3200]
Loss: 1.0198  [400/3200]
Loss: 0.6601  [800/3200]
Loss: 0.2020  [1200/3200]
Loss: 0.3656  [1600/3200]
Loss: 0.4890  [2000/3200]
Loss: 0.2832  [2400/3200]
Loss: 0.4997  [2800/3200]
Validation Loss: 0.1463, Accuracy: 77.12%, F1 Score: 0.7656
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 7/60
Loss: 1.0665  [0/3200]
Loss: 0.8635  [400/3200]
Loss: 0.7211  [800/3200]
Loss: 1.3342  [1200/3200]
Loss: 0.8206  [1600/3200]
Loss: 0.3397  [2000/3200]
Loss: 0.1447  [2400/3200]
Loss: 0.3533  [2800/3200]
Validation Loss: 0.1419, Accuracy: 77.88%, F1 Score: 0.7665
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 8/60
Loss: 0.2255  [0/3200]
Loss: 0.2892  [400/3200]
Loss: 0.2356  [800/3200]
Loss: 0.0938  [1200/3200]
Loss: 0.1720  [1600/3200]
Loss: 0.1381  [2000/3200]
Loss: 0.0753  [2400/3200]
Loss: 0.1438  [2800/3200]
Validation Loss: 0.1472, Accuracy: 78.12%, F1 Score: 0.7788
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 9/60
Loss: 0.1760  [0/3200]
Loss: 0.4706  [400/3200]
Loss: 0.4940  [800/3200]
Loss: 0.8744  [1200/3200]
Loss: 0.4053  [1600/3200]
Loss: 0.1017  [2000/3200]
Loss: 0.1537  [2400/3200]
Loss: 0.1480  [2800/3200]
Validation Loss: 0.1482, Accuracy: 77.88%, F1 Score: 0.7678
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 10/60
Loss: 0.4921  [0/3200]
Loss: 0.0498  [400/3200]
Loss: 0.2421  [800/3200]
Loss: 0.2087  [1200/3200]
Loss: 0.0502  [1600/3200]
Loss: 0.4956  [2000/3200]
Loss: 0.3186  [2400/3200]
Loss: 1.9014  [2800/3200]
Validation Loss: 0.1578, Accuracy: 77.38%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 11/60
Loss: 0.1042  [0/3200]
Loss: 0.2202  [400/3200]
Loss: 0.0500  [800/3200]
Loss: 0.2872  [1200/3200]
Loss: 0.1282  [1600/3200]
Loss: 0.0887  [2000/3200]
Loss: 0.2870  [2400/3200]
Loss: 0.1240  [2800/3200]
Validation Loss: 0.1478, Accuracy: 78.25%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 12/60
Loss: 0.2060  [0/3200]
Loss: 0.0085  [400/3200]
Loss: 0.0921  [800/3200]
Loss: 0.0285  [1200/3200]
Loss: 0.1124  [1600/3200]
Loss: 1.3213  [2000/3200]
Loss: 0.2364  [2400/3200]
Loss: 0.3073  [2800/3200]
Validation Loss: 0.1436, Accuracy: 80.62%, F1 Score: 0.8043
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 13/60
Loss: 0.0831  [0/3200]
Loss: 0.9009  [400/3200]
Loss: 0.4095  [800/3200]
Loss: 0.2830  [1200/3200]
Loss: 0.1508  [1600/3200]
Loss: 0.7639  [2000/3200]
Loss: 0.5166  [2400/3200]
Loss: 0.0857  [2800/3200]
Validation Loss: 0.1566, Accuracy: 78.38%, F1 Score: 0.7736
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 14/60
Loss: 0.0160  [0/3200]
Loss: 0.6356  [400/3200]
Loss: 0.0914  [800/3200]
Loss: 0.2011  [1200/3200]
Loss: 0.0536  [1600/3200]
Loss: 0.3966  [2000/3200]
Loss: 0.3823  [2400/3200]
Loss: 0.0661  [2800/3200]
Validation Loss: 0.1619, Accuracy: 77.00%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 15/60
Loss: 0.2048  [0/3200]
Loss: 0.0919  [400/3200]
Loss: 0.2730  [800/3200]
Loss: 0.0509  [1200/3200]
Loss: 0.2101  [1600/3200]
Loss: 0.0400  [2000/3200]
Loss: 0.1651  [2400/3200]
Loss: 0.8163  [2800/3200]
Validation Loss: 0.1755, Accuracy: 77.25%, F1 Score: 0.7663
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 16/60
Loss: 0.2145  [0/3200]
Loss: 0.2159  [400/3200]
Loss: 0.1125  [800/3200]
Loss: 0.0099  [1200/3200]
Loss: 0.2046  [1600/3200]
Loss: 0.0131  [2000/3200]
Loss: 0.0989  [2400/3200]
Loss: 0.5328  [2800/3200]
Validation Loss: 0.1698, Accuracy: 77.50%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 17/60
Loss: 0.2768  [0/3200]
Loss: 0.3139  [400/3200]
Loss: 0.0074  [800/3200]
Loss: 0.0445  [1200/3200]
Loss: 0.0082  [1600/3200]
Loss: 0.0043  [2000/3200]
Loss: 0.0141  [2400/3200]
Loss: 0.1704  [2800/3200]
Validation Loss: 0.1815, Accuracy: 77.50%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 18/60
Loss: 0.0693  [0/3200]
Loss: 0.0854  [400/3200]
Loss: 0.0133  [800/3200]
Loss: 0.2331  [1200/3200]
Loss: 0.0042  [1600/3200]
Loss: 0.0758  [2000/3200]
Loss: 0.0460  [2400/3200]
Loss: 0.0240  [2800/3200]
Validation Loss: 0.1740, Accuracy: 77.75%, F1 Score: 0.7750
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 19/60
Loss: 0.1249  [0/3200]
Loss: 0.0765  [400/3200]
Loss: 0.0064  [800/3200]
Loss: 0.0034  [1200/3200]
Loss: 0.0485  [1600/3200]
Loss: 0.0747  [2000/3200]
Loss: 0.0012  [2400/3200]
Loss: 0.0283  [2800/3200]
Validation Loss: 0.2028, Accuracy: 75.62%, F1 Score: 0.7517
Adjusting learning rate of group 0 to 8.0008e-05.
Epoch 20/60
Loss: 0.0232  [0/3200]
Loss: 0.0728  [400/3200]
Loss: 0.0283  [800/3200]
Loss: 0.0513  [1200/3200]
Loss: 0.0019  [1600/3200]
Loss: 0.0558  [2000/3200]
Loss: 0.0042  [2400/3200]
Loss: 0.0318  [2800/3200]
Validation Loss: 0.2003, Accuracy: 77.62%, F1 Score: 0.7727
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 21/60
Loss: 0.0282  [0/3200]
Loss: 0.2977  [400/3200]
Loss: 0.0081  [800/3200]
Loss: 0.0276  [1200/3200]
Loss: 0.0973  [1600/3200]
Loss: 0.1194  [2000/3200]
Loss: 0.0025  [2400/3200]
Loss: 0.0504  [2800/3200]
Validation Loss: 0.1846, Accuracy: 77.25%, F1 Score: 0.7675
Adjusting learning rate of group 0 to 8.0010e-05.
Epoch 22/60
Loss: 0.0564  [0/3200]
Loss: 0.0473  [400/3200]
Loss: 0.0108  [800/3200]
Loss: 0.0054  [1200/3200]
Loss: 0.4767  [1600/3200]
Loss: 0.5634  [2000/3200]
Loss: 0.0490  [2400/3200]
Loss: 0.2158  [2800/3200]
Validation Loss: 0.1918, Accuracy: 78.50%, F1 Score: 0.7816
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 23/60
Loss: 0.0096  [0/3200]
Loss: 0.0013  [400/3200]
Loss: 0.0014  [800/3200]
Loss: 0.0019  [1200/3200]
Loss: 0.0087  [1600/3200]
Loss: 0.0054  [2000/3200]
Loss: 0.0063  [2400/3200]
Loss: 0.0019  [2800/3200]
Validation Loss: 0.1897, Accuracy: 79.50%, F1 Score: 0.7901
Adjusting learning rate of group 0 to 8.0012e-05.
Epoch 24/60
Loss: 0.0133  [0/3200]
Loss: 0.0014  [400/3200]
Loss: 0.0126  [800/3200]
Loss: 0.0015  [1200/3200]
Loss: 0.2611  [1600/3200]
Loss: 0.2673  [2000/3200]
Loss: 0.0221  [2400/3200]
Loss: 0.0083  [2800/3200]
Validation Loss: 0.2200, Accuracy: 76.00%, F1 Score: 0.7499
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 25/60
Loss: 0.0007  [0/3200]
Loss: 0.1151  [400/3200]
Loss: 0.0580  [800/3200]
Loss: 0.0130  [1200/3200]
Loss: 0.0928  [1600/3200]
Loss: 0.0483  [2000/3200]
Loss: 0.0019  [2400/3200]
Loss: 0.0285  [2800/3200]
Validation Loss: 0.2168, Accuracy: 76.75%, F1 Score: 0.7623
Adjusting learning rate of group 0 to 8.0014e-05.
Epoch 26/60
Loss: 0.0199  [0/3200]
Loss: 0.0051  [400/3200]
Loss: 0.0368  [800/3200]
Loss: 0.0020  [1200/3200]
Loss: 0.0310  [1600/3200]
Loss: 0.0033  [2000/3200]
Loss: 0.0069  [2400/3200]
Loss: 0.0218  [2800/3200]
Validation Loss: 0.2024, Accuracy: 79.12%, F1 Score: 0.7897
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 27/60
Loss: 0.0020  [0/3200]
Loss: 0.0132  [400/3200]
Loss: 0.0500  [800/3200]
Loss: 0.0167  [1200/3200]
Loss: 0.0103  [1600/3200]
Loss: 0.0200  [2000/3200]
Loss: 0.0004  [2400/3200]
Loss: 0.0050  [2800/3200]
Validation Loss: 0.2213, Accuracy: 78.25%, F1 Score: 0.7761
Adjusting learning rate of group 0 to 8.0017e-05.
Epoch 28/60
Loss: 0.0100  [0/3200]
Loss: 0.0418  [400/3200]
Loss: 0.0135  [800/3200]
Loss: 0.0497  [1200/3200]
Loss: 0.0045  [1600/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0103  [2400/3200]
Loss: 0.0135  [2800/3200]
Validation Loss: 0.2214, Accuracy: 78.75%, F1 Score: 0.7822
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 29/60
Loss: 0.0256  [0/3200]
Loss: 0.0018  [400/3200]
Loss: 0.0086  [800/3200]
Loss: 0.0012  [1200/3200]
Loss: 0.0005  [1600/3200]
Loss: 0.0034  [2000/3200]
Loss: 0.0030  [2400/3200]
Loss: 0.0007  [2800/3200]
Validation Loss: 0.2422, Accuracy: 76.00%, F1 Score: 0.7460
Adjusting learning rate of group 0 to 8.0019e-05.
Epoch 30/60
Loss: 0.0051  [0/3200]
Loss: 0.0060  [400/3200]
Loss: 0.0066  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.2704  [1600/3200]
Loss: 0.0106  [2000/3200]
Loss: 0.0049  [2400/3200]
Loss: 0.0000  [2800/3200]
Validation Loss: 0.2381, Accuracy: 77.25%, F1 Score: 0.7631
Adjusting learning rate of group 0 to 8.0021e-05.
Epoch 31/60
Loss: 0.0006  [0/3200]
Loss: 0.0015  [400/3200]
Loss: 0.0138  [800/3200]
Loss: 0.0217  [1200/3200]
Loss: 0.0031  [1600/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.1631  [2400/3200]
Loss: 0.0020  [2800/3200]
Validation Loss: 0.2315, Accuracy: 77.12%, F1 Score: 0.7669
Adjusting learning rate of group 0 to 8.0022e-05.
Epoch 32/60
Loss: 0.0034  [0/3200]
Loss: 0.0005  [400/3200]
Loss: 0.0005  [800/3200]
Loss: 0.0014  [1200/3200]
Loss: 0.0020  [1600/3200]
Loss: 0.0008  [2000/3200]
Loss: 0.0023  [2400/3200]
Loss: 0.0012  [2800/3200]
Validation Loss: 0.2326, Accuracy: 78.00%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 33/60
Loss: 0.0011  [0/3200]
Loss: 0.0760  [400/3200]
Loss: 0.3297  [800/3200]
Loss: 0.0108  [1200/3200]
Loss: 0.0138  [1600/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0013  [2400/3200]
Loss: 0.2581  [2800/3200]
Validation Loss: 0.2499, Accuracy: 75.88%, F1 Score: 0.7485
Adjusting learning rate of group 0 to 8.0025e-05.
Epoch 34/60
Loss: 0.0022  [0/3200]
Loss: 0.0003  [400/3200]
Loss: 0.0038  [800/3200]
Loss: 0.0005  [1200/3200]
Loss: 0.0006  [1600/3200]
Loss: 0.0110  [2000/3200]
Loss: 0.0168  [2400/3200]
Loss: 0.0023  [2800/3200]
Validation Loss: 0.2214, Accuracy: 78.38%, F1 Score: 0.7819
Adjusting learning rate of group 0 to 8.0026e-05.
Epoch 35/60
Loss: 0.0005  [0/3200]
Loss: 0.0013  [400/3200]
Loss: 0.0028  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0223  [2000/3200]
Loss: 0.0120  [2400/3200]
Loss: 0.0030  [2800/3200]
Validation Loss: 0.2817, Accuracy: 76.50%, F1 Score: 0.7534
Adjusting learning rate of group 0 to 8.0028e-05.
Epoch 36/60
Loss: 0.0011  [0/3200]
Loss: 0.0143  [400/3200]
Loss: 0.0165  [800/3200]
Loss: 0.0015  [1200/3200]
Loss: 0.0010  [1600/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0027  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.2738, Accuracy: 77.00%, F1 Score: 0.7659
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 37/60
Loss: 0.0012  [0/3200]
Loss: 0.0013  [400/3200]
Loss: 0.0014  [800/3200]
Loss: 0.0079  [1200/3200]
Loss: 0.0102  [1600/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0033  [2400/3200]
Loss: 0.0007  [2800/3200]
Validation Loss: 0.2643, Accuracy: 77.00%, F1 Score: 0.7613
Adjusting learning rate of group 0 to 8.0031e-05.
Epoch 38/60
Loss: 0.0021  [0/3200]
Loss: 0.0003  [400/3200]
Loss: 0.0009  [800/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0072  [1600/3200]
Loss: 0.0012  [2000/3200]
Loss: 0.0036  [2400/3200]
Loss: 0.0472  [2800/3200]
Validation Loss: 0.2635, Accuracy: 77.25%, F1 Score: 0.7669
Adjusting learning rate of group 0 to 8.0033e-05.
Epoch 39/60
Loss: 0.0002  [0/3200]
Loss: 0.0003  [400/3200]
Loss: 0.0153  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0112  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0044  [2400/3200]
Loss: 0.0022  [2800/3200]
Validation Loss: 0.2490, Accuracy: 77.25%, F1 Score: 0.7676
Adjusting learning rate of group 0 to 8.0035e-05.
Epoch 40/60
Loss: 0.0000  [0/3200]
Loss: 0.0031  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0007  [1200/3200]
Loss: 0.0079  [1600/3200]
Loss: 0.0061  [2000/3200]
Loss: 0.0017  [2400/3200]
Loss: 0.0039  [2800/3200]
Validation Loss: 0.2415, Accuracy: 77.62%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 41/60
Loss: 0.0025  [0/3200]
Loss: 0.0003  [400/3200]
Loss: 0.0006  [800/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0014  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0010  [2400/3200]
Loss: 0.3004  [2800/3200]
Validation Loss: 0.2654, Accuracy: 77.75%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0038e-05.
Epoch 42/60
Loss: 0.0009  [0/3200]
Loss: 0.0252  [400/3200]
Loss: 0.0191  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0099  [1600/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0007  [2400/3200]
Loss: 0.0027  [2800/3200]
Validation Loss: 0.2560, Accuracy: 77.88%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0040e-05.
Epoch 43/60
Loss: 0.1538  [0/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0004  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0012  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0003  [2800/3200]
Validation Loss: 0.2991, Accuracy: 77.12%, F1 Score: 0.7601
Adjusting learning rate of group 0 to 8.0042e-05.
Epoch 44/60
Loss: 0.0013  [0/3200]
Loss: 0.0042  [400/3200]
Loss: 0.0089  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0076  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0011  [2400/3200]
Loss: 0.0009  [2800/3200]
Validation Loss: 0.2794, Accuracy: 76.88%, F1 Score: 0.7631
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 45/60
Loss: 0.0017  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0090  [800/3200]
Loss: 0.2241  [1200/3200]
Loss: 0.0212  [1600/3200]
Loss: 0.0047  [2000/3200]
Loss: 0.0040  [2400/3200]
Loss: 0.0002  [2800/3200]
Validation Loss: 0.2688, Accuracy: 77.38%, F1 Score: 0.7677
Adjusting learning rate of group 0 to 8.0046e-05.
Epoch 46/60
Loss: 0.0018  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0013  [800/3200]
Loss: 0.0061  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0111  [2400/3200]
Loss: 0.0006  [2800/3200]
Validation Loss: 0.2845, Accuracy: 77.25%, F1 Score: 0.7678
Adjusting learning rate of group 0 to 8.0048e-05.
Epoch 47/60
Loss: 0.0033  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0080  [800/3200]
Loss: 0.0005  [1200/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0003  [2800/3200]
Validation Loss: 0.3071, Accuracy: 76.88%, F1 Score: 0.7584
Adjusting learning rate of group 0 to 8.0050e-05.
Epoch 48/60
Loss: 0.0011  [0/3200]
Loss: 0.0003  [400/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0007  [1200/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0010  [2000/3200]
Loss: 0.0199  [2400/3200]
Loss: 0.1265  [2800/3200]
Validation Loss: 0.2699, Accuracy: 78.50%, F1 Score: 0.7817
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 49/60
Loss: 0.0028  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.1304  [1200/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0158  [2000/3200]
Loss: 0.0006  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.2727, Accuracy: 78.50%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.0055e-05.
Epoch 50/60
Loss: 0.0032  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0023  [800/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0046  [1600/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0002  [2400/3200]
Loss: 0.0003  [2800/3200]
Validation Loss: 0.2769, Accuracy: 78.00%, F1 Score: 0.7763
Adjusting learning rate of group 0 to 8.0057e-05.
Epoch 51/60
Loss: 0.0006  [0/3200]
Loss: 0.0103  [400/3200]
Loss: 0.0050  [800/3200]
Loss: 0.0019  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0010  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.2761, Accuracy: 77.25%, F1 Score: 0.7690
Adjusting learning rate of group 0 to 8.0059e-05.
Epoch 52/60
Loss: 0.0006  [0/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0012  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0002  [2400/3200]
Loss: 0.0002  [2800/3200]
Validation Loss: 0.3360, Accuracy: 76.00%, F1 Score: 0.7467
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 53/60
Loss: 0.0004  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0044  [800/3200]
Loss: 0.0006  [1200/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0113  [2000/3200]
Loss: 0.0012  [2400/3200]
Loss: 0.0401  [2800/3200]
Validation Loss: 0.2932, Accuracy: 78.12%, F1 Score: 0.7784
Adjusting learning rate of group 0 to 8.0064e-05.
Epoch 54/60
Loss: 0.0005  [0/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0036  [800/3200]
Loss: 0.0009  [1200/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0027  [2000/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0307  [2800/3200]
Validation Loss: 0.3062, Accuracy: 77.75%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.0067e-05.
Epoch 55/60
Loss: 0.0001  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0008  [2400/3200]
Loss: 0.0028  [2800/3200]
Validation Loss: 0.3031, Accuracy: 78.38%, F1 Score: 0.7805
Adjusting learning rate of group 0 to 8.0069e-05.
Epoch 56/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0005  [800/3200]
Loss: 0.0015  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0008  [2000/3200]
Loss: 0.0004  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.3034, Accuracy: 77.38%, F1 Score: 0.7657
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 57/60
Loss: 0.0008  [0/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0143  [2000/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0002  [2800/3200]
Validation Loss: 0.3067, Accuracy: 77.38%, F1 Score: 0.7653
Adjusting learning rate of group 0 to 8.0074e-05.
Epoch 58/60
Loss: 0.0014  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0004  [1200/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0002  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.2804, Accuracy: 79.38%, F1 Score: 0.7914
Adjusting learning rate of group 0 to 8.0077e-05.
Epoch 59/60
Loss: 0.0010  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0044  [1200/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0004  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0051  [2800/3200]
Validation Loss: 0.3098, Accuracy: 77.50%, F1 Score: 0.7744
Adjusting learning rate of group 0 to 8.0080e-05.
Epoch 60/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0107  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0047  [2000/3200]
Loss: 0.0013  [2400/3200]
Loss: 0.0002  [2800/3200]
Validation Loss: 0.2959, Accuracy: 78.25%, F1 Score: 0.7773
Adjusting learning rate of group 0 to 8.0082e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 8...
Epoch 1/60
Loss: 1.2532  [0/3200]

<ipython-input-41-785969b892fd>:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Loss: 0.7934  [800/3200]
Loss: 1.5167  [1600/3200]
Loss: 0.9581  [2400/3200]
Validation Loss: 0.0981, Accuracy: 68.38%, F1 Score: 0.6438
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.2579  [0/3200]
Loss: 0.8616  [800/3200]
Loss: 0.7107  [1600/3200]
Loss: 0.7732  [2400/3200]
Validation Loss: 0.0846, Accuracy: 71.38%, F1 Score: 0.6960
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 3/60
Loss: 0.6311  [0/3200]
Loss: 1.0619  [800/3200]
Loss: 0.3830  [1600/3200]
Loss: 0.8937  [2400/3200]
Validation Loss: 0.0753, Accuracy: 75.38%, F1 Score: 0.7435
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 4/60
Loss: 0.3717  [0/3200]
Loss: 0.6578  [800/3200]
Loss: 0.3763  [1600/3200]
Loss: 0.4318  [2400/3200]
Validation Loss: 0.0725, Accuracy: 78.12%, F1 Score: 0.7731
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 5/60
Loss: 0.5261  [0/3200]
Loss: 0.6433  [800/3200]
Loss: 0.3865  [1600/3200]
Loss: 0.2093  [2400/3200]
Validation Loss: 0.0724, Accuracy: 78.50%, F1 Score: 0.7776
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 6/60
Loss: 0.3936  [0/3200]
Loss: 0.5126  [800/3200]
Loss: 0.4144  [1600/3200]
Loss: 0.7489  [2400/3200]
Validation Loss: 0.0720, Accuracy: 77.38%, F1 Score: 0.7756
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 7/60
Loss: 0.5496  [0/3200]
Loss: 0.2348  [800/3200]
Loss: 0.5676  [1600/3200]
Loss: 0.3946  [2400/3200]
Validation Loss: 0.0747, Accuracy: 76.75%, F1 Score: 0.7579
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 8/60
Loss: 0.2581  [0/3200]
Loss: 0.2821  [800/3200]
Loss: 0.5243  [1600/3200]
Loss: 0.5165  [2400/3200]
Validation Loss: 0.0719, Accuracy: 79.00%, F1 Score: 0.7868
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 9/60
Loss: 0.3811  [0/3200]
Loss: 0.0678  [800/3200]
Loss: 0.1451  [1600/3200]
Loss: 0.1413  [2400/3200]
Validation Loss: 0.0695, Accuracy: 78.75%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 10/60
Loss: 0.6265  [0/3200]
Loss: 1.3464  [800/3200]
Loss: 0.3605  [1600/3200]
Loss: 0.0840  [2400/3200]
Validation Loss: 0.0731, Accuracy: 78.75%, F1 Score: 0.7855
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 11/60
Loss: 0.3250  [0/3200]
Loss: 0.3971  [800/3200]
Loss: 0.1171  [1600/3200]
Loss: 0.2629  [2400/3200]
Validation Loss: 0.0767, Accuracy: 77.62%, F1 Score: 0.7680
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 12/60
Loss: 0.3273  [0/3200]
Loss: 0.1058  [800/3200]
Loss: 0.1732  [1600/3200]
Loss: 0.1105  [2400/3200]
Validation Loss: 0.0682, Accuracy: 80.25%, F1 Score: 0.8014
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 13/60
Loss: 0.0787  [0/3200]
Loss: 0.3393  [800/3200]
Loss: 0.1529  [1600/3200]
Loss: 0.0833  [2400/3200]
Validation Loss: 0.0756, Accuracy: 76.75%, F1 Score: 0.7594
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 14/60
Loss: 0.1394  [0/3200]
Loss: 0.5143  [800/3200]
Loss: 0.4229  [1600/3200]
Loss: 0.6526  [2400/3200]
Validation Loss: 0.0725, Accuracy: 78.50%, F1 Score: 0.7811
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 15/60
Loss: 0.0566  [0/3200]
Loss: 0.4678  [800/3200]
Loss: 0.1348  [1600/3200]
Loss: 0.3002  [2400/3200]
Validation Loss: 0.0761, Accuracy: 78.25%, F1 Score: 0.7787
Adjusting learning rate of group 0 to 8.0021e-05.
Epoch 16/60
Loss: 0.1048  [0/3200]
Loss: 0.0266  [800/3200]
Loss: 0.3619  [1600/3200]
Loss: 0.0872  [2400/3200]
Validation Loss: 0.0841, Accuracy: 79.12%, F1 Score: 0.7834
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 17/60
Loss: 0.0419  [0/3200]
Loss: 0.1824  [800/3200]
Loss: 0.0581  [1600/3200]
Loss: 0.2306  [2400/3200]
Validation Loss: 0.0790, Accuracy: 78.75%, F1 Score: 0.7831
Adjusting learning rate of group 0 to 8.0026e-05.
Epoch 18/60
Loss: 0.3801  [0/3200]
Loss: 0.0282  [800/3200]
Loss: 0.0255  [1600/3200]
Loss: 0.0201  [2400/3200]
Validation Loss: 0.0888, Accuracy: 76.88%, F1 Score: 0.7596
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 19/60
Loss: 0.1226  [0/3200]
Loss: 0.0266  [800/3200]
Loss: 0.0596  [1600/3200]
Loss: 0.0082  [2400/3200]
Validation Loss: 0.0814, Accuracy: 78.38%, F1 Score: 0.7797
Adjusting learning rate of group 0 to 8.0033e-05.
Epoch 20/60
Loss: 0.0816  [0/3200]
Loss: 0.0781  [800/3200]
Loss: 0.0800  [1600/3200]
Loss: 0.2396  [2400/3200]
Validation Loss: 0.0904, Accuracy: 77.75%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 21/60
Loss: 0.0650  [0/3200]
Loss: 0.0874  [800/3200]
Loss: 0.0678  [1600/3200]
Loss: 0.0221  [2400/3200]
Validation Loss: 0.0896, Accuracy: 77.50%, F1 Score: 0.7718
Adjusting learning rate of group 0 to 8.0040e-05.
Epoch 22/60
Loss: 0.0535  [0/3200]
Loss: 0.0914  [800/3200]
Loss: 0.2721  [1600/3200]
Loss: 0.1529  [2400/3200]
Validation Loss: 0.0895, Accuracy: 78.25%, F1 Score: 0.7786
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 23/60
Loss: 0.0898  [0/3200]
Loss: 0.0082  [800/3200]
Loss: 0.0105  [1600/3200]
Loss: 0.0241  [2400/3200]
Validation Loss: 0.0919, Accuracy: 78.00%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0048e-05.
Epoch 24/60
Loss: 0.1816  [0/3200]
Loss: 0.3213  [800/3200]
Loss: 0.3800  [1600/3200]
Loss: 0.0094  [2400/3200]
Validation Loss: 0.0944, Accuracy: 78.62%, F1 Score: 0.7834
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 25/60
Loss: 0.0043  [0/3200]
Loss: 0.0692  [800/3200]
Loss: 0.0304  [1600/3200]
Loss: 0.0224  [2400/3200]
Validation Loss: 0.0885, Accuracy: 79.12%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 8.0057e-05.
Epoch 26/60
Loss: 0.0446  [0/3200]
Loss: 0.0984  [800/3200]
Loss: 0.1934  [1600/3200]
Loss: 0.0157  [2400/3200]
Validation Loss: 0.0947, Accuracy: 78.50%, F1 Score: 0.7783
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 27/60
Loss: 0.0670  [0/3200]
Loss: 0.0171  [800/3200]
Loss: 0.0099  [1600/3200]
Loss: 0.0150  [2400/3200]
Validation Loss: 0.1107, Accuracy: 76.12%, F1 Score: 0.7514
Adjusting learning rate of group 0 to 8.0067e-05.
Epoch 28/60
Loss: 0.0188  [0/3200]
Loss: 0.0022  [800/3200]
Loss: 0.0665  [1600/3200]
Loss: 0.0264  [2400/3200]
Validation Loss: 0.1083, Accuracy: 77.38%, F1 Score: 0.7667
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 29/60
Loss: 0.0739  [0/3200]
Loss: 0.0090  [800/3200]
Loss: 0.0056  [1600/3200]
Loss: 0.1037  [2400/3200]
Validation Loss: 0.1022, Accuracy: 78.62%, F1 Score: 0.7795
Adjusting learning rate of group 0 to 8.0077e-05.
Epoch 30/60
Loss: 0.0032  [0/3200]
Loss: 0.0063  [800/3200]
Loss: 0.0027  [1600/3200]
Loss: 0.0096  [2400/3200]
Validation Loss: 0.1207, Accuracy: 76.00%, F1 Score: 0.7476
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 31/60
Loss: 0.0047  [0/3200]
Loss: 0.0429  [800/3200]
Loss: 0.0158  [1600/3200]
Loss: 0.0176  [2400/3200]
Validation Loss: 0.1095, Accuracy: 77.75%, F1 Score: 0.7736
Adjusting learning rate of group 0 to 8.0088e-05.
Epoch 32/60
Loss: 0.0039  [0/3200]
Loss: 0.0004  [800/3200]
Loss: 0.0082  [1600/3200]
Loss: 0.0020  [2400/3200]
Validation Loss: 0.1152, Accuracy: 76.38%, F1 Score: 0.7616
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 33/60
Loss: 0.0219  [0/3200]
Loss: 0.0057  [800/3200]
Loss: 0.0309  [1600/3200]
Loss: 0.0017  [2400/3200]
Validation Loss: 0.1028, Accuracy: 78.38%, F1 Score: 0.7807
Adjusting learning rate of group 0 to 8.0100e-05.
Epoch 34/60
Loss: 0.0074  [0/3200]
Loss: 0.0009  [800/3200]
Loss: 0.0090  [1600/3200]
Loss: 0.0086  [2400/3200]
Validation Loss: 0.1063, Accuracy: 78.75%, F1 Score: 0.7861
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 35/60
Loss: 0.0009  [0/3200]
Loss: 0.0051  [800/3200]
Loss: 0.0033  [1600/3200]
Loss: 0.0045  [2400/3200]
Validation Loss: 0.1209, Accuracy: 76.38%, F1 Score: 0.7537
Adjusting learning rate of group 0 to 8.0112e-05.
Epoch 36/60
Loss: 0.0015  [0/3200]
Loss: 0.0017  [800/3200]
Loss: 0.0065  [1600/3200]
Loss: 0.0267  [2400/3200]
Validation Loss: 0.1113, Accuracy: 78.50%, F1 Score: 0.7819
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 37/60
Loss: 0.0024  [0/3200]
Loss: 0.0044  [800/3200]
Loss: 0.0011  [1600/3200]
Loss: 0.0069  [2400/3200]
Validation Loss: 0.1200, Accuracy: 77.12%, F1 Score: 0.7635
Adjusting learning rate of group 0 to 8.0125e-05.
Epoch 38/60
Loss: 0.0025  [0/3200]
Loss: 0.0785  [800/3200]
Loss: 0.0086  [1600/3200]
Loss: 0.0009  [2400/3200]
Validation Loss: 0.1313, Accuracy: 76.50%, F1 Score: 0.7562
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 39/60
Loss: 0.0014  [0/3200]
Loss: 0.0142  [800/3200]
Loss: 0.0040  [1600/3200]
Loss: 0.0021  [2400/3200]
Validation Loss: 0.1198, Accuracy: 77.50%, F1 Score: 0.7675
Adjusting learning rate of group 0 to 8.0139e-05.
Epoch 40/60
Loss: 0.0000  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0067  [1600/3200]
Loss: 0.0034  [2400/3200]
Validation Loss: 0.1279, Accuracy: 77.12%, F1 Score: 0.7641
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 41/60
Loss: 0.0027  [0/3200]
Loss: 0.0174  [800/3200]
Loss: 0.0090  [1600/3200]
Loss: 0.0064  [2400/3200]
Validation Loss: 0.1233, Accuracy: 77.50%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0154e-05.
Epoch 42/60
Loss: 0.0040  [0/3200]
Loss: 0.0057  [800/3200]
Loss: 0.0467  [1600/3200]
Loss: 0.0002  [2400/3200]
Validation Loss: 0.1209, Accuracy: 77.12%, F1 Score: 0.7658
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 43/60
Loss: 0.0285  [0/3200]
Loss: 0.0029  [800/3200]
Loss: 0.0287  [1600/3200]
Loss: 0.0045  [2400/3200]
Validation Loss: 0.1465, Accuracy: 76.00%, F1 Score: 0.7473
Adjusting learning rate of group 0 to 8.0169e-05.
Epoch 44/60
Loss: 0.0007  [0/3200]
Loss: 0.0099  [800/3200]
Loss: 0.0007  [1600/3200]
Loss: 0.0112  [2400/3200]
Validation Loss: 0.1159, Accuracy: 78.38%, F1 Score: 0.7788
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 45/60
Loss: 0.0009  [0/3200]
Loss: 0.0203  [800/3200]
Loss: 0.0005  [1600/3200]
Loss: 0.0119  [2400/3200]
Validation Loss: 0.1303, Accuracy: 76.88%, F1 Score: 0.7627
Adjusting learning rate of group 0 to 8.0185e-05.
Epoch 46/60
Loss: 0.0015  [0/3200]
Loss: 0.0008  [800/3200]
Loss: 0.0025  [1600/3200]
Loss: 0.0033  [2400/3200]
Validation Loss: 0.1255, Accuracy: 76.75%, F1 Score: 0.7613
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 47/60
Loss: 0.0065  [0/3200]
Loss: 0.0018  [800/3200]
Loss: 0.0038  [1600/3200]
Loss: 0.0034  [2400/3200]
Validation Loss: 0.1279, Accuracy: 78.12%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.0202e-05.
Epoch 48/60
Loss: 0.0002  [0/3200]
Loss: 0.0039  [800/3200]
Loss: 0.0020  [1600/3200]
Loss: 0.0006  [2400/3200]
Validation Loss: 0.1231, Accuracy: 78.50%, F1 Score: 0.7834
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 49/60
Loss: 0.0088  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0093  [1600/3200]
Loss: 0.0001  [2400/3200]
Validation Loss: 0.1379, Accuracy: 76.62%, F1 Score: 0.7601
Adjusting learning rate of group 0 to 8.0219e-05.
Epoch 50/60
Loss: 0.0145  [0/3200]
Loss: 0.0524  [800/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0014  [2400/3200]
Validation Loss: 0.1369, Accuracy: 77.50%, F1 Score: 0.7667
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 51/60
Loss: 0.0003  [0/3200]
Loss: 0.0016  [800/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0001  [2400/3200]
Validation Loss: 0.1466, Accuracy: 76.38%, F1 Score: 0.7539
Adjusting learning rate of group 0 to 8.0238e-05.
Epoch 52/60
Loss: 0.0017  [0/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0490  [2400/3200]
Validation Loss: 0.1314, Accuracy: 77.25%, F1 Score: 0.7691
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 53/60
Loss: 0.0092  [0/3200]
Loss: 0.0028  [800/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0007  [2400/3200]
Validation Loss: 0.1411, Accuracy: 76.50%, F1 Score: 0.7600
Adjusting learning rate of group 0 to 8.0257e-05.
Epoch 54/60
Loss: 0.0009  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0033  [2400/3200]
Validation Loss: 0.1375, Accuracy: 79.00%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 55/60
Loss: 0.0009  [0/3200]
Loss: 0.0069  [800/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0075  [2400/3200]
Validation Loss: 0.1385, Accuracy: 79.12%, F1 Score: 0.7861
Adjusting learning rate of group 0 to 8.0277e-05.
Epoch 56/60
Loss: 0.0005  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.1363  [2400/3200]
Validation Loss: 0.1497, Accuracy: 77.12%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 57/60
Loss: 0.0002  [0/3200]
Loss: 0.0016  [800/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0005  [2400/3200]
Validation Loss: 0.1539, Accuracy: 76.75%, F1 Score: 0.7612
Adjusting learning rate of group 0 to 8.0297e-05.
Epoch 58/60
Loss: 0.0067  [0/3200]
Loss: 0.0012  [800/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0010  [2400/3200]
Validation Loss: 0.1452, Accuracy: 77.88%, F1 Score: 0.7701
Adjusting learning rate of group 0 to 8.0307e-05.
Epoch 59/60
Loss: 0.0056  [0/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0008  [2400/3200]
Validation Loss: 0.1415, Accuracy: 78.00%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.0318e-05.
Epoch 60/60
Loss: 0.0008  [0/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0003  [2400/3200]
Validation Loss: 0.1478, Accuracy: 76.88%, F1 Score: 0.7617
Adjusting learning rate of group 0 to 8.0329e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 16...
Epoch 1/60
Loss: 1.4068  [0/3200]

<ipython-input-41-785969b892fd>:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Loss: 1.0340  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.38%, F1 Score: 0.6587
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0309  [0/3200]
Loss: 0.9789  [1600/3200]
Validation Loss: 0.0446, Accuracy: 69.38%, F1 Score: 0.6808
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7666  [0/3200]
Loss: 0.5071  [1600/3200]
Validation Loss: 0.0384, Accuracy: 76.00%, F1 Score: 0.7459
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.3333  [0/3200]
Loss: 0.2567  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8734  [0/3200]
Loss: 0.6187  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7106  [0/3200]
Loss: 0.3061  [1600/3200]
Validation Loss: 0.0371, Accuracy: 76.62%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5991  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7522
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3038  [0/3200]
Loss: 0.3593  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.12%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.5215  [0/3200]
Loss: 0.3155  [1600/3200]
Validation Loss: 0.0339, Accuracy: 80.00%, F1 Score: 0.7938
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2824  [0/3200]
Loss: 0.2885  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.3508  [0/3200]
Loss: 0.1472  [1600/3200]
Validation Loss: 0.0332, Accuracy: 80.50%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2190  [0/3200]
Loss: 0.2166  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.62%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3937  [0/3200]
Loss: 0.1673  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.12%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.2727  [0/3200]
Loss: 0.2061  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.3493  [0/3200]
Loss: 0.3084  [1600/3200]
Validation Loss: 0.0353, Accuracy: 79.12%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.2409  [0/3200]
Loss: 0.3674  [1600/3200]
Validation Loss: 0.0375, Accuracy: 78.88%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.1645  [0/3200]
Loss: 0.1782  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.75%, F1 Score: 0.7967
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.1921  [0/3200]
Loss: 0.1559  [1600/3200]
Validation Loss: 0.0377, Accuracy: 78.62%, F1 Score: 0.7821
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.1434  [0/3200]
Loss: 0.3144  [1600/3200]
Validation Loss: 0.0373, Accuracy: 79.00%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0890  [0/3200]
Loss: 0.0579  [1600/3200]
Validation Loss: 0.0399, Accuracy: 78.62%, F1 Score: 0.7871
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1354  [0/3200]
Loss: 0.3022  [1600/3200]
Validation Loss: 0.0407, Accuracy: 79.00%, F1 Score: 0.7827
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.1071  [0/3200]
Loss: 0.0615  [1600/3200]
Validation Loss: 0.0395, Accuracy: 79.12%, F1 Score: 0.7890
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.0352  [0/3200]
Loss: 0.3220  [1600/3200]
Validation Loss: 0.0435, Accuracy: 76.62%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.0740  [0/3200]
Loss: 0.2077  [1600/3200]
Validation Loss: 0.0419, Accuracy: 78.25%, F1 Score: 0.7787
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.2023  [0/3200]
Loss: 0.0523  [1600/3200]
Validation Loss: 0.0411, Accuracy: 79.50%, F1 Score: 0.7937
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.0448  [0/3200]
Loss: 0.0335  [1600/3200]
Validation Loss: 0.0423, Accuracy: 79.00%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.0219  [0/3200]
Loss: 0.0496  [1600/3200]
Validation Loss: 0.0438, Accuracy: 78.62%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.0509  [0/3200]
Loss: 0.1963  [1600/3200]
Validation Loss: 0.0468, Accuracy: 77.62%, F1 Score: 0.7695
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.0238  [0/3200]
Loss: 0.0277  [1600/3200]
Validation Loss: 0.0445, Accuracy: 78.62%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0291  [0/3200]
Loss: 0.0403  [1600/3200]
Validation Loss: 0.0486, Accuracy: 78.25%, F1 Score: 0.7753
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0102  [0/3200]
Loss: 0.0528  [1600/3200]
Validation Loss: 0.0491, Accuracy: 77.75%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0074  [0/3200]
Loss: 0.0048  [1600/3200]
Validation Loss: 0.0458, Accuracy: 78.75%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.1512  [0/3200]
Loss: 0.0292  [1600/3200]
Validation Loss: 0.0483, Accuracy: 79.00%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0056  [0/3200]
Loss: 0.0385  [1600/3200]
Validation Loss: 0.0464, Accuracy: 79.75%, F1 Score: 0.7953
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0606  [0/3200]
Loss: 0.0637  [1600/3200]
Validation Loss: 0.0503, Accuracy: 79.00%, F1 Score: 0.7851
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0063  [0/3200]
Loss: 0.0068  [1600/3200]
Validation Loss: 0.0493, Accuracy: 78.62%, F1 Score: 0.7839
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0141  [0/3200]
Loss: 0.0316  [1600/3200]
Validation Loss: 0.0492, Accuracy: 78.75%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0097  [0/3200]
Loss: 0.0173  [1600/3200]
Validation Loss: 0.0508, Accuracy: 78.12%, F1 Score: 0.7782
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0037  [0/3200]
Loss: 0.0083  [1600/3200]
Validation Loss: 0.0537, Accuracy: 77.88%, F1 Score: 0.7764
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0026  [0/3200]
Loss: 0.0046  [1600/3200]
Validation Loss: 0.0558, Accuracy: 76.25%, F1 Score: 0.7526
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0109  [0/3200]
Loss: 0.0125  [1600/3200]
Validation Loss: 0.0529, Accuracy: 78.88%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0034  [0/3200]
Loss: 0.0438  [1600/3200]
Validation Loss: 0.0521, Accuracy: 78.88%, F1 Score: 0.7877
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0218  [0/3200]
Loss: 0.0095  [1600/3200]
Validation Loss: 0.0549, Accuracy: 78.62%, F1 Score: 0.7822
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0015  [0/3200]
Loss: 0.0092  [1600/3200]
Validation Loss: 0.0542, Accuracy: 79.00%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0049  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0560, Accuracy: 78.88%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0007  [0/3200]
Loss: 0.0123  [1600/3200]
Validation Loss: 0.0530, Accuracy: 79.12%, F1 Score: 0.7894
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0265  [0/3200]
Loss: 0.0053  [1600/3200]
Validation Loss: 0.0540, Accuracy: 79.75%, F1 Score: 0.7965
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0613  [0/3200]
Loss: 0.0033  [1600/3200]
Validation Loss: 0.0572, Accuracy: 79.25%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0113  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0573, Accuracy: 78.50%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0049  [0/3200]
Loss: 0.0123  [1600/3200]
Validation Loss: 0.0616, Accuracy: 77.38%, F1 Score: 0.7676
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0010  [0/3200]
Loss: 0.0104  [1600/3200]
Validation Loss: 0.0618, Accuracy: 78.00%, F1 Score: 0.7729
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0017  [0/3200]
Loss: 0.0022  [1600/3200]
Validation Loss: 0.0587, Accuracy: 79.12%, F1 Score: 0.7863
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0010  [0/3200]
Loss: 0.0012  [1600/3200]
Validation Loss: 0.0609, Accuracy: 78.12%, F1 Score: 0.7784
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0012  [0/3200]
Loss: 0.1055  [1600/3200]
Validation Loss: 0.0580, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0078  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0573, Accuracy: 79.12%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0056  [0/3200]
Loss: 0.0032  [1600/3200]
Validation Loss: 0.0602, Accuracy: 78.62%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0039  [0/3200]
Loss: 0.0057  [1600/3200]
Validation Loss: 0.0581, Accuracy: 80.12%, F1 Score: 0.7987
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0002  [0/3200]
Loss: 0.0010  [1600/3200]
Validation Loss: 0.0627, Accuracy: 78.25%, F1 Score: 0.7762
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0008  [0/3200]
Loss: 0.0014  [1600/3200]
Validation Loss: 0.0612, Accuracy: 79.50%, F1 Score: 0.7922
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0011  [0/3200]
Loss: 0.0002  [1600/3200]
Validation Loss: 0.0644, Accuracy: 78.50%, F1 Score: 0.7800
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 32...
Epoch 1/60
Loss: 1.4743  [0/3200]

<ipython-input-41-785969b892fd>:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Validation Loss: 0.0242, Accuracy: 67.88%, F1 Score: 0.6611
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/60
Loss: 0.8727  [0/3200]
Validation Loss: 0.0224, Accuracy: 70.00%, F1 Score: 0.6900
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/60
Loss: 0.8007  [0/3200]
Validation Loss: 0.0201, Accuracy: 73.00%, F1 Score: 0.7242
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/60
Loss: 0.7027  [0/3200]
Validation Loss: 0.0188, Accuracy: 76.12%, F1 Score: 0.7615
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/60
Loss: 0.4788  [0/3200]
Validation Loss: 0.0178, Accuracy: 78.00%, F1 Score: 0.7776
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/60
Loss: 0.6257  [0/3200]
Validation Loss: 0.0176, Accuracy: 79.12%, F1 Score: 0.7899
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/60
Loss: 0.7233  [0/3200]
Validation Loss: 0.0182, Accuracy: 76.75%, F1 Score: 0.7624
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/60
Loss: 0.2242  [0/3200]
Validation Loss: 0.0172, Accuracy: 79.00%, F1 Score: 0.7829
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/60
Loss: 0.3948  [0/3200]
Validation Loss: 0.0176, Accuracy: 78.50%, F1 Score: 0.7761
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/60
Loss: 0.2966  [0/3200]
Validation Loss: 0.0172, Accuracy: 79.88%, F1 Score: 0.7965
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/60
Loss: 0.3505  [0/3200]
Validation Loss: 0.0181, Accuracy: 77.88%, F1 Score: 0.7746
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/60
Loss: 0.2348  [0/3200]
Validation Loss: 0.0177, Accuracy: 79.25%, F1 Score: 0.7863
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/60
Loss: 0.2631  [0/3200]
Validation Loss: 0.0178, Accuracy: 77.50%, F1 Score: 0.7743
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/60
Loss: 0.2062  [0/3200]
Validation Loss: 0.0191, Accuracy: 78.25%, F1 Score: 0.7765
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/60
Loss: 0.2211  [0/3200]
Validation Loss: 0.0178, Accuracy: 78.88%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/60
Loss: 0.2029  [0/3200]
Validation Loss: 0.0182, Accuracy: 80.38%, F1 Score: 0.8000
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/60
Loss: 0.1755  [0/3200]
Validation Loss: 0.0182, Accuracy: 80.00%, F1 Score: 0.7969
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/60
Loss: 0.2220  [0/3200]
Validation Loss: 0.0186, Accuracy: 79.50%, F1 Score: 0.7927
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/60
Loss: 0.0914  [0/3200]
Validation Loss: 0.0189, Accuracy: 78.88%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/60
Loss: 0.1339  [0/3200]
Validation Loss: 0.0198, Accuracy: 78.62%, F1 Score: 0.7836
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/60
Loss: 0.1952  [0/3200]
Validation Loss: 0.0187, Accuracy: 80.38%, F1 Score: 0.8039
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/60
Loss: 0.0768  [0/3200]
Validation Loss: 0.0202, Accuracy: 78.25%, F1 Score: 0.7774
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/60
Loss: 0.0919  [0/3200]
Validation Loss: 0.0207, Accuracy: 77.25%, F1 Score: 0.7690
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/60
Loss: 0.1362  [0/3200]
Validation Loss: 0.0206, Accuracy: 77.38%, F1 Score: 0.7711
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/60
Loss: 0.1864  [0/3200]
Validation Loss: 0.0210, Accuracy: 77.38%, F1 Score: 0.7702
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/60
Loss: 0.0405  [0/3200]
Validation Loss: 0.0213, Accuracy: 77.00%, F1 Score: 0.7635
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/60
Loss: 0.0314  [0/3200]
Validation Loss: 0.0213, Accuracy: 78.25%, F1 Score: 0.7795
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/60
Loss: 0.0604  [0/3200]
Validation Loss: 0.0218, Accuracy: 78.25%, F1 Score: 0.7786
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/60
Loss: 0.0708  [0/3200]
Validation Loss: 0.0222, Accuracy: 79.50%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 8.1231e-05.
Epoch 30/60
Loss: 0.0299  [0/3200]
Validation Loss: 0.0220, Accuracy: 78.88%, F1 Score: 0.7851
Adjusting learning rate of group 0 to 8.1317e-05.
Epoch 31/60
Loss: 0.0887  [0/3200]
Validation Loss: 0.0226, Accuracy: 78.62%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.1406e-05.
Epoch 32/60
Loss: 0.0232  [0/3200]
Validation Loss: 0.0236, Accuracy: 77.50%, F1 Score: 0.7720
Adjusting learning rate of group 0 to 8.1499e-05.
Epoch 33/60
Loss: 0.0897  [0/3200]
Validation Loss: 0.0244, Accuracy: 77.38%, F1 Score: 0.7681
Adjusting learning rate of group 0 to 8.1594e-05.
Epoch 34/60
Loss: 0.0408  [0/3200]
Validation Loss: 0.0235, Accuracy: 78.75%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.1692e-05.
Epoch 35/60
Loss: 0.0192  [0/3200]
Validation Loss: 0.0236, Accuracy: 78.50%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.1793e-05.
Epoch 36/60
Loss: 0.0331  [0/3200]
Validation Loss: 0.0266, Accuracy: 76.75%, F1 Score: 0.7634
Adjusting learning rate of group 0 to 8.1896e-05.
Epoch 37/60
Loss: 0.0093  [0/3200]
Validation Loss: 0.0257, Accuracy: 78.62%, F1 Score: 0.7816
Adjusting learning rate of group 0 to 8.2003e-05.
Epoch 38/60
Loss: 0.0086  [0/3200]
Validation Loss: 0.0260, Accuracy: 77.12%, F1 Score: 0.7683
Adjusting learning rate of group 0 to 8.2113e-05.
Epoch 39/60
Loss: 0.0117  [0/3200]
Validation Loss: 0.0283, Accuracy: 75.75%, F1 Score: 0.7476
Adjusting learning rate of group 0 to 8.2226e-05.
Epoch 40/60
Loss: 0.0094  [0/3200]
Validation Loss: 0.0272, Accuracy: 75.88%, F1 Score: 0.7544
Adjusting learning rate of group 0 to 8.2341e-05.
Epoch 41/60
Loss: 0.0158  [0/3200]
Validation Loss: 0.0279, Accuracy: 77.38%, F1 Score: 0.7667
Adjusting learning rate of group 0 to 8.2460e-05.
Epoch 42/60
Loss: 0.1096  [0/3200]
Validation Loss: 0.0282, Accuracy: 76.62%, F1 Score: 0.7617
Adjusting learning rate of group 0 to 8.2581e-05.
Epoch 43/60
Loss: 0.0145  [0/3200]
Validation Loss: 0.0293, Accuracy: 76.38%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.2705e-05.
Epoch 44/60
Loss: 0.0447  [0/3200]
Validation Loss: 0.0278, Accuracy: 78.25%, F1 Score: 0.7757
Adjusting learning rate of group 0 to 8.2833e-05.
Epoch 45/60
Loss: 0.0098  [0/3200]
Validation Loss: 0.0281, Accuracy: 78.00%, F1 Score: 0.7749
Adjusting learning rate of group 0 to 8.2963e-05.
Epoch 46/60
Loss: 0.0047  [0/3200]
Validation Loss: 0.0266, Accuracy: 79.12%, F1 Score: 0.7883
Adjusting learning rate of group 0 to 8.3096e-05.
Epoch 47/60
Loss: 0.0049  [0/3200]
Validation Loss: 0.0277, Accuracy: 78.38%, F1 Score: 0.7824
Adjusting learning rate of group 0 to 8.3232e-05.
Epoch 48/60
Loss: 0.0109  [0/3200]
Validation Loss: 0.0280, Accuracy: 77.62%, F1 Score: 0.7714
Adjusting learning rate of group 0 to 8.3371e-05.
Epoch 49/60
Loss: 0.0087  [0/3200]
Validation Loss: 0.0267, Accuracy: 79.00%, F1 Score: 0.7889
Adjusting learning rate of group 0 to 8.3512e-05.
Epoch 50/60
Loss: 0.0110  [0/3200]
Validation Loss: 0.0292, Accuracy: 78.25%, F1 Score: 0.7764
Adjusting learning rate of group 0 to 8.3657e-05.
Epoch 51/60
Loss: 0.0052  [0/3200]
Validation Loss: 0.0292, Accuracy: 78.00%, F1 Score: 0.7761
Adjusting learning rate of group 0 to 8.3805e-05.
Epoch 52/60
Loss: 0.0068  [0/3200]
Validation Loss: 0.0297, Accuracy: 77.62%, F1 Score: 0.7702
Adjusting learning rate of group 0 to 8.3955e-05.
Epoch 53/60
Loss: 0.0063  [0/3200]
Validation Loss: 0.0296, Accuracy: 77.62%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.4109e-05.
Epoch 54/60
Loss: 0.0048  [0/3200]
Validation Loss: 0.0285, Accuracy: 78.62%, F1 Score: 0.7840
Adjusting learning rate of group 0 to 8.4265e-05.
Epoch 55/60
Loss: 0.0148  [0/3200]
Validation Loss: 0.0305, Accuracy: 77.25%, F1 Score: 0.7685
Adjusting learning rate of group 0 to 8.4425e-05.
Epoch 56/60
Loss: 0.0061  [0/3200]
Validation Loss: 0.0288, Accuracy: 78.62%, F1 Score: 0.7840
Adjusting learning rate of group 0 to 8.4587e-05.
Epoch 57/60
Loss: 0.0115  [0/3200]
Validation Loss: 0.0321, Accuracy: 78.25%, F1 Score: 0.7759
Adjusting learning rate of group 0 to 8.4752e-05.
Epoch 58/60
Loss: 0.0043  [0/3200]
Validation Loss: 0.0317, Accuracy: 76.50%, F1 Score: 0.7584
Adjusting learning rate of group 0 to 8.4920e-05.
Epoch 59/60
Loss: 0.0170  [0/3200]
Validation Loss: 0.0290, Accuracy: 79.00%, F1 Score: 0.7885
Adjusting learning rate of group 0 to 8.5091e-05.
Epoch 60/60
Loss: 0.0011  [0/3200]
Validation Loss: 0.0305, Accuracy: 78.88%, F1 Score: 0.7833
Adjusting learning rate of group 0 to 8.5265e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 64...
Epoch 1/60
Loss: 1.5495  [0/3200]

<ipython-input-41-785969b892fd>:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Validation Loss: 0.0133, Accuracy: 64.62%, F1 Score: 0.6241
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 2/60
Loss: 0.8689  [0/3200]
Validation Loss: 0.0119, Accuracy: 69.50%, F1 Score: 0.6796
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 3/60
Loss: 0.7967  [0/3200]
Validation Loss: 0.0109, Accuracy: 72.25%, F1 Score: 0.7180
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 4/60
Loss: 0.7959  [0/3200]
Validation Loss: 0.0100, Accuracy: 74.12%, F1 Score: 0.7409
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 5/60
Loss: 0.5726  [0/3200]
Validation Loss: 0.0095, Accuracy: 77.38%, F1 Score: 0.7744
Adjusting learning rate of group 0 to 8.0147e-05.
Epoch 6/60
Loss: 0.5784  [0/3200]
Validation Loss: 0.0101, Accuracy: 74.25%, F1 Score: 0.7434
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 7/60
Loss: 0.4905  [0/3200]
Validation Loss: 0.0093, Accuracy: 77.00%, F1 Score: 0.7692
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 8/60
Loss: 0.4226  [0/3200]
Validation Loss: 0.0092, Accuracy: 77.00%, F1 Score: 0.7659
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 9/60
Loss: 0.5192  [0/3200]
Validation Loss: 0.0090, Accuracy: 79.00%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 8.0475e-05.
Epoch 10/60
Loss: 0.3491  [0/3200]
Validation Loss: 0.0089, Accuracy: 79.25%, F1 Score: 0.7886
Adjusting learning rate of group 0 to 8.0586e-05.
Epoch 11/60
Loss: 0.4536  [0/3200]
Validation Loss: 0.0091, Accuracy: 78.62%, F1 Score: 0.7827
Adjusting learning rate of group 0 to 8.0709e-05.
Epoch 12/60
Loss: 0.2942  [0/3200]
Validation Loss: 0.0089, Accuracy: 78.62%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0844e-05.
Epoch 13/60
Loss: 0.2929  [0/3200]
Validation Loss: 0.0089, Accuracy: 78.50%, F1 Score: 0.7846
Adjusting learning rate of group 0 to 8.0990e-05.
Epoch 14/60
Loss: 0.2925  [0/3200]
Validation Loss: 0.0093, Accuracy: 77.62%, F1 Score: 0.7711
Adjusting learning rate of group 0 to 8.1149e-05.
Epoch 15/60
Loss: 0.2276  [0/3200]
Validation Loss: 0.0088, Accuracy: 79.62%, F1 Score: 0.7949
Adjusting learning rate of group 0 to 8.1319e-05.
Epoch 16/60
Loss: 0.2942  [0/3200]
Validation Loss: 0.0090, Accuracy: 79.75%, F1 Score: 0.7935
Adjusting learning rate of group 0 to 8.1500e-05.
Epoch 17/60
Loss: 0.2759  [0/3200]
Validation Loss: 0.0091, Accuracy: 78.38%, F1 Score: 0.7833
Adjusting learning rate of group 0 to 8.1694e-05.
Epoch 18/60
Loss: 0.2471  [0/3200]
Validation Loss: 0.0091, Accuracy: 78.88%, F1 Score: 0.7847
Adjusting learning rate of group 0 to 8.1899e-05.
Epoch 19/60
Loss: 0.2627  [0/3200]
Validation Loss: 0.0091, Accuracy: 79.00%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.2115e-05.
Epoch 20/60
Loss: 0.2601  [0/3200]
Validation Loss: 0.0096, Accuracy: 79.25%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.2344e-05.
Epoch 21/60
Loss: 0.1779  [0/3200]
Validation Loss: 0.0093, Accuracy: 79.25%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.2584e-05.
Epoch 22/60
Loss: 0.1654  [0/3200]
Validation Loss: 0.0099, Accuracy: 78.38%, F1 Score: 0.7802
Adjusting learning rate of group 0 to 8.2836e-05.
Epoch 23/60
Loss: 0.2062  [0/3200]
Validation Loss: 0.0101, Accuracy: 77.75%, F1 Score: 0.7754
Adjusting learning rate of group 0 to 8.3099e-05.
Epoch 24/60
Loss: 0.1505  [0/3200]
Validation Loss: 0.0103, Accuracy: 78.38%, F1 Score: 0.7791
Adjusting learning rate of group 0 to 8.3374e-05.
Epoch 25/60
Loss: 0.1392  [0/3200]
Validation Loss: 0.0100, Accuracy: 78.12%, F1 Score: 0.7769
Adjusting learning rate of group 0 to 8.3661e-05.
Epoch 26/60
Loss: 0.1378  [0/3200]
Validation Loss: 0.0103, Accuracy: 78.12%, F1 Score: 0.7786
Adjusting learning rate of group 0 to 8.3960e-05.
Epoch 27/60
Loss: 0.0832  [0/3200]
Validation Loss: 0.0118, Accuracy: 75.25%, F1 Score: 0.7506
Adjusting learning rate of group 0 to 8.4270e-05.
Epoch 28/60
Loss: 0.2445  [0/3200]
Validation Loss: 0.0104, Accuracy: 77.75%, F1 Score: 0.7739
Adjusting learning rate of group 0 to 8.4592e-05.
Epoch 29/60
Loss: 0.1220  [0/3200]
Validation Loss: 0.0111, Accuracy: 77.88%, F1 Score: 0.7747
Adjusting learning rate of group 0 to 8.4925e-05.
Epoch 30/60
Loss: 0.1045  [0/3200]
Validation Loss: 0.0112, Accuracy: 78.12%, F1 Score: 0.7776
Adjusting learning rate of group 0 to 8.5271e-05.
Epoch 31/60
Loss: 0.0592  [0/3200]
Validation Loss: 0.0114, Accuracy: 77.50%, F1 Score: 0.7730
Adjusting learning rate of group 0 to 8.5628e-05.
Epoch 32/60
Loss: 0.0658  [0/3200]
Validation Loss: 0.0110, Accuracy: 78.62%, F1 Score: 0.7838
Adjusting learning rate of group 0 to 8.5996e-05.
Epoch 33/60
Loss: 0.1099  [0/3200]
Validation Loss: 0.0121, Accuracy: 76.75%, F1 Score: 0.7647
Adjusting learning rate of group 0 to 8.6376e-05.
Epoch 34/60
Loss: 0.0161  [0/3200]
Validation Loss: 0.0114, Accuracy: 76.88%, F1 Score: 0.7679
Adjusting learning rate of group 0 to 8.6768e-05.
Epoch 35/60
Loss: 0.0406  [0/3200]
Validation Loss: 0.0127, Accuracy: 77.12%, F1 Score: 0.7689
Adjusting learning rate of group 0 to 8.7172e-05.
Epoch 36/60
Loss: 0.0180  [0/3200]
Validation Loss: 0.0121, Accuracy: 77.50%, F1 Score: 0.7722
Adjusting learning rate of group 0 to 8.7587e-05.
Epoch 37/60
Loss: 0.0461  [0/3200]
Validation Loss: 0.0122, Accuracy: 77.12%, F1 Score: 0.7688
Adjusting learning rate of group 0 to 8.8013e-05.
Epoch 38/60
Loss: 0.0251  [0/3200]
Validation Loss: 0.0130, Accuracy: 77.38%, F1 Score: 0.7725
Adjusting learning rate of group 0 to 8.8452e-05.
Epoch 39/60
Loss: 0.0201  [0/3200]
Validation Loss: 0.0136, Accuracy: 76.50%, F1 Score: 0.7595
Adjusting learning rate of group 0 to 8.8902e-05.
Epoch 40/60
Loss: 0.0333  [0/3200]
Validation Loss: 0.0126, Accuracy: 77.62%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.9363e-05.
Epoch 41/60
Loss: 0.0296  [0/3200]
Validation Loss: 0.0122, Accuracy: 78.88%, F1 Score: 0.7877
Adjusting learning rate of group 0 to 8.9837e-05.
Epoch 42/60
Loss: 0.0232  [0/3200]
Validation Loss: 0.0136, Accuracy: 76.62%, F1 Score: 0.7632
Adjusting learning rate of group 0 to 9.0321e-05.
Epoch 43/60
Loss: 0.0352  [0/3200]
Validation Loss: 0.0128, Accuracy: 77.50%, F1 Score: 0.7735
Adjusting learning rate of group 0 to 9.0818e-05.
Epoch 44/60
Loss: 0.0056  [0/3200]
Validation Loss: 0.0129, Accuracy: 78.25%, F1 Score: 0.7798
Adjusting learning rate of group 0 to 9.1326e-05.
Epoch 45/60
Loss: 0.0319  [0/3200]
Validation Loss: 0.0136, Accuracy: 77.75%, F1 Score: 0.7734
Adjusting learning rate of group 0 to 9.1845e-05.
Epoch 46/60
Loss: 0.0215  [0/3200]
Validation Loss: 0.0135, Accuracy: 77.00%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 9.2377e-05.
Epoch 47/60
Loss: 0.0420  [0/3200]
Validation Loss: 0.0137, Accuracy: 77.12%, F1 Score: 0.7685
Adjusting learning rate of group 0 to 9.2919e-05.
Epoch 48/60
Loss: 0.0134  [0/3200]
Validation Loss: 0.0140, Accuracy: 77.25%, F1 Score: 0.7681
Adjusting learning rate of group 0 to 9.3474e-05.
Epoch 49/60
Loss: 0.0160  [0/3200]
Validation Loss: 0.0137, Accuracy: 78.38%, F1 Score: 0.7802
Adjusting learning rate of group 0 to 9.4040e-05.
Epoch 50/60
Loss: 0.0199  [0/3200]
Validation Loss: 0.0143, Accuracy: 78.12%, F1 Score: 0.7766
Adjusting learning rate of group 0 to 9.4617e-05.
Epoch 51/60
Loss: 0.0036  [0/3200]
Validation Loss: 0.0141, Accuracy: 77.25%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 9.5206e-05.
Epoch 52/60
Loss: 0.0098  [0/3200]
Validation Loss: 0.0147, Accuracy: 76.88%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 9.5806e-05.
Epoch 53/60
Loss: 0.0101  [0/3200]
Validation Loss: 0.0153, Accuracy: 76.75%, F1 Score: 0.7655
Adjusting learning rate of group 0 to 9.6418e-05.
Epoch 54/60
Loss: 0.0047  [0/3200]
Validation Loss: 0.0158, Accuracy: 75.25%, F1 Score: 0.7473
Adjusting learning rate of group 0 to 9.7042e-05.
Epoch 55/60
Loss: 0.0140  [0/3200]
Validation Loss: 0.0161, Accuracy: 76.50%, F1 Score: 0.7585
Adjusting learning rate of group 0 to 9.7677e-05.
Epoch 56/60
Loss: 0.0097  [0/3200]
Validation Loss: 0.0149, Accuracy: 77.38%, F1 Score: 0.7697
Adjusting learning rate of group 0 to 9.8324e-05.
Epoch 57/60
Loss: 0.0100  [0/3200]
Validation Loss: 0.0148, Accuracy: 77.25%, F1 Score: 0.7686
Adjusting learning rate of group 0 to 9.8982e-05.
Epoch 58/60
Loss: 0.0085  [0/3200]
Validation Loss: 0.0142, Accuracy: 77.75%, F1 Score: 0.7758
Adjusting learning rate of group 0 to 9.9651e-05.
Epoch 59/60
Loss: 0.0035  [0/3200]
Validation Loss: 0.0151, Accuracy: 78.00%, F1 Score: 0.7790
Adjusting learning rate of group 0 to 1.0033e-04.
Epoch 60/60
Loss: 0.0174  [0/3200]
Validation Loss: 0.0156, Accuracy: 76.88%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 1.0102e-04.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 128...
Epoch 1/60
Loss: 1.5469  [0/3200]

<ipython-input-41-785969b892fd>:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Validation Loss: 0.0084, Accuracy: 63.38%, F1 Score: 0.6219
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 2/60
Loss: 1.0463  [0/3200]
Validation Loss: 0.0068, Accuracy: 67.00%, F1 Score: 0.6557
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 3/60
Loss: 0.9515  [0/3200]
Validation Loss: 0.0067, Accuracy: 69.12%, F1 Score: 0.6842
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 4/60
Loss: 0.8041  [0/3200]
Validation Loss: 0.0058, Accuracy: 71.62%, F1 Score: 0.7091
Adjusting learning rate of group 0 to 8.0376e-05.
Epoch 5/60
Loss: 0.6654  [0/3200]
Validation Loss: 0.0055, Accuracy: 73.00%, F1 Score: 0.7262
Adjusting learning rate of group 0 to 8.0587e-05.
Epoch 6/60
Loss: 0.6131  [0/3200]
Validation Loss: 0.0058, Accuracy: 72.12%, F1 Score: 0.7189
Adjusting learning rate of group 0 to 8.0846e-05.
Epoch 7/60
Loss: 0.5431  [0/3200]
Validation Loss: 0.0053, Accuracy: 76.00%, F1 Score: 0.7593
Adjusting learning rate of group 0 to 8.1151e-05.
Epoch 8/60
Loss: 0.5500  [0/3200]
Validation Loss: 0.0051, Accuracy: 76.50%, F1 Score: 0.7567
Adjusting learning rate of group 0 to 8.1504e-05.
Epoch 9/60
Loss: 0.5010  [0/3200]
Validation Loss: 0.0052, Accuracy: 75.88%, F1 Score: 0.7570
Adjusting learning rate of group 0 to 8.1903e-05.
Epoch 10/60
Loss: 0.4341  [0/3200]
Validation Loss: 0.0053, Accuracy: 75.00%, F1 Score: 0.7460
Adjusting learning rate of group 0 to 8.2349e-05.
Epoch 11/60
Loss: 0.4843  [0/3200]
Validation Loss: 0.0050, Accuracy: 77.12%, F1 Score: 0.7601
Adjusting learning rate of group 0 to 8.2842e-05.
Epoch 12/60
Loss: 0.3444  [0/3200]
Validation Loss: 0.0052, Accuracy: 76.38%, F1 Score: 0.7532
Adjusting learning rate of group 0 to 8.3382e-05.
Epoch 13/60
Loss: 0.3806  [0/3200]
Validation Loss: 0.0047, Accuracy: 77.25%, F1 Score: 0.7672
Adjusting learning rate of group 0 to 8.3969e-05.
Epoch 14/60
Loss: 0.3318  [0/3200]
Validation Loss: 0.0049, Accuracy: 78.88%, F1 Score: 0.7832
Adjusting learning rate of group 0 to 8.4602e-05.
Epoch 15/60
Loss: 0.3221  [0/3200]
Validation Loss: 0.0049, Accuracy: 78.88%, F1 Score: 0.7849
Adjusting learning rate of group 0 to 8.5282e-05.
Epoch 16/60
Loss: 0.3237  [0/3200]
Validation Loss: 0.0049, Accuracy: 78.50%, F1 Score: 0.7789
Adjusting learning rate of group 0 to 8.6009e-05.
Epoch 17/60
Loss: 0.3237  [0/3200]
Validation Loss: 0.0051, Accuracy: 78.25%, F1 Score: 0.7777
Adjusting learning rate of group 0 to 8.6783e-05.
Epoch 18/60
Loss: 0.3028  [0/3200]
Validation Loss: 0.0047, Accuracy: 78.38%, F1 Score: 0.7782
Adjusting learning rate of group 0 to 8.7604e-05.
Epoch 19/60
Loss: 0.3899  [0/3200]
Validation Loss: 0.0046, Accuracy: 79.50%, F1 Score: 0.7903
Adjusting learning rate of group 0 to 8.8471e-05.
Epoch 20/60
Loss: 0.2791  [0/3200]
Validation Loss: 0.0051, Accuracy: 79.38%, F1 Score: 0.7919
Adjusting learning rate of group 0 to 8.9384e-05.
Epoch 21/60
Loss: 0.2882  [0/3200]
Validation Loss: 0.0049, Accuracy: 78.62%, F1 Score: 0.7844
Adjusting learning rate of group 0 to 9.0344e-05.
Epoch 22/60
Loss: 0.2435  [0/3200]
Validation Loss: 0.0051, Accuracy: 78.62%, F1 Score: 0.7847
Adjusting learning rate of group 0 to 9.1351e-05.
Epoch 23/60
Loss: 0.2536  [0/3200]
Validation Loss: 0.0050, Accuracy: 79.00%, F1 Score: 0.7897
Adjusting learning rate of group 0 to 9.2404e-05.
Epoch 24/60
Loss: 0.2066  [0/3200]
Validation Loss: 0.0052, Accuracy: 79.12%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 9.3504e-05.
Epoch 25/60
Loss: 0.2411  [0/3200]
Validation Loss: 0.0048, Accuracy: 79.12%, F1 Score: 0.7891
Adjusting learning rate of group 0 to 9.4649e-05.
Epoch 26/60
Loss: 0.2094  [0/3200]
Validation Loss: 0.0054, Accuracy: 78.50%, F1 Score: 0.7814
Adjusting learning rate of group 0 to 9.5842e-05.
Epoch 27/60
Loss: 0.1366  [0/3200]
Validation Loss: 0.0055, Accuracy: 78.38%, F1 Score: 0.7801
Adjusting learning rate of group 0 to 9.7080e-05.
Epoch 28/60
Loss: 0.1465  [0/3200]
Validation Loss: 0.0052, Accuracy: 78.25%, F1 Score: 0.7761
Adjusting learning rate of group 0 to 9.8364e-05.
Epoch 29/60
Loss: 0.1338  [0/3200]
Validation Loss: 0.0054, Accuracy: 78.50%, F1 Score: 0.7831
Adjusting learning rate of group 0 to 9.9695e-05.
Epoch 30/60
Loss: 0.1590  [0/3200]
Validation Loss: 0.0058, Accuracy: 78.88%, F1 Score: 0.7837
Adjusting learning rate of group 0 to 1.0107e-04.
Epoch 31/60
Loss: 0.1087  [0/3200]
Validation Loss: 0.0059, Accuracy: 77.12%, F1 Score: 0.7695
Adjusting learning rate of group 0 to 1.0249e-04.
Epoch 32/60
Loss: 0.0920  [0/3200]
Validation Loss: 0.0056, Accuracy: 77.62%, F1 Score: 0.7742
Adjusting learning rate of group 0 to 1.0396e-04.
Epoch 33/60
Loss: 0.1257  [0/3200]
Validation Loss: 0.0062, Accuracy: 76.75%, F1 Score: 0.7622
Adjusting learning rate of group 0 to 1.0548e-04.
Epoch 34/60
Loss: 0.0736  [0/3200]
Validation Loss: 0.0053, Accuracy: 79.12%, F1 Score: 0.7887
Adjusting learning rate of group 0 to 1.0704e-04.
Epoch 35/60
Loss: 0.0810  [0/3200]
Validation Loss: 0.0059, Accuracy: 78.38%, F1 Score: 0.7792
Adjusting learning rate of group 0 to 1.0864e-04.
Epoch 36/60
Loss: 0.0612  [0/3200]
Validation Loss: 0.0060, Accuracy: 78.38%, F1 Score: 0.7783
Adjusting learning rate of group 0 to 1.1029e-04.
Epoch 37/60
Loss: 0.0655  [0/3200]
Validation Loss: 0.0061, Accuracy: 78.75%, F1 Score: 0.7859
Adjusting learning rate of group 0 to 1.1199e-04.
Epoch 38/60
Loss: 0.0630  [0/3200]
Validation Loss: 0.0066, Accuracy: 77.50%, F1 Score: 0.7742
Adjusting learning rate of group 0 to 1.1373e-04.
Epoch 39/60
Loss: 0.0581  [0/3200]
Validation Loss: 0.0062, Accuracy: 78.38%, F1 Score: 0.7779
Adjusting learning rate of group 0 to 1.1552e-04.
Epoch 40/60
Loss: 0.0313  [0/3200]
Validation Loss: 0.0067, Accuracy: 77.25%, F1 Score: 0.7662
Adjusting learning rate of group 0 to 1.1735e-04.
Epoch 41/60
Loss: 0.0491  [0/3200]
Validation Loss: 0.0065, Accuracy: 79.00%, F1 Score: 0.7852
Adjusting learning rate of group 0 to 1.1923e-04.
Epoch 42/60
Loss: 0.0626  [0/3200]
Validation Loss: 0.0065, Accuracy: 79.62%, F1 Score: 0.7923
Adjusting learning rate of group 0 to 1.2115e-04.
Epoch 43/60
Loss: 0.0823  [0/3200]
Validation Loss: 0.0067, Accuracy: 78.38%, F1 Score: 0.7744
Adjusting learning rate of group 0 to 1.2312e-04.
Epoch 44/60
Loss: 0.0446  [0/3200]
Validation Loss: 0.0072, Accuracy: 77.88%, F1 Score: 0.7725
Adjusting learning rate of group 0 to 1.2514e-04.
Epoch 45/60
Loss: 0.0268  [0/3200]
Validation Loss: 0.0071, Accuracy: 77.12%, F1 Score: 0.7655
Adjusting learning rate of group 0 to 1.2719e-04.
Epoch 46/60
Loss: 0.0290  [0/3200]
Validation Loss: 0.0062, Accuracy: 78.88%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 1.2930e-04.
Epoch 47/60
Loss: 0.0422  [0/3200]
Validation Loss: 0.0074, Accuracy: 77.00%, F1 Score: 0.7635
Adjusting learning rate of group 0 to 1.3144e-04.
Epoch 48/60
Loss: 0.0276  [0/3200]
Validation Loss: 0.0074, Accuracy: 76.75%, F1 Score: 0.7644
Adjusting learning rate of group 0 to 1.3363e-04.
Epoch 49/60
Loss: 0.0258  [0/3200]
Validation Loss: 0.0077, Accuracy: 77.25%, F1 Score: 0.7686
Adjusting learning rate of group 0 to 1.3587e-04.
Epoch 50/60
Loss: 0.0172  [0/3200]
Validation Loss: 0.0073, Accuracy: 77.38%, F1 Score: 0.7694
Adjusting learning rate of group 0 to 1.3815e-04.
Epoch 51/60
Loss: 0.0172  [0/3200]
Validation Loss: 0.0076, Accuracy: 76.00%, F1 Score: 0.7560
Adjusting learning rate of group 0 to 1.4047e-04.
Epoch 52/60
Loss: 0.0189  [0/3200]
Validation Loss: 0.0081, Accuracy: 77.00%, F1 Score: 0.7653
Adjusting learning rate of group 0 to 1.4284e-04.
Epoch 53/60
Loss: 0.0151  [0/3200]
Validation Loss: 0.0086, Accuracy: 77.38%, F1 Score: 0.7665
Adjusting learning rate of group 0 to 1.4526e-04.
Epoch 54/60
Loss: 0.0199  [0/3200]
Validation Loss: 0.0074, Accuracy: 78.62%, F1 Score: 0.7837
Adjusting learning rate of group 0 to 1.4771e-04.
Epoch 55/60
Loss: 0.0148  [0/3200]
Validation Loss: 0.0082, Accuracy: 78.62%, F1 Score: 0.7808
Adjusting learning rate of group 0 to 1.5021e-04.
Epoch 56/60
Loss: 0.0097  [0/3200]
Validation Loss: 0.0077, Accuracy: 77.50%, F1 Score: 0.7682
Adjusting learning rate of group 0 to 1.5275e-04.
Epoch 57/60
Loss: 0.0187  [0/3200]
Validation Loss: 0.0087, Accuracy: 76.38%, F1 Score: 0.7551
Adjusting learning rate of group 0 to 1.5534e-04.
Epoch 58/60
Loss: 0.0109  [0/3200]
Validation Loss: 0.0074, Accuracy: 77.62%, F1 Score: 0.7743
Adjusting learning rate of group 0 to 1.5797e-04.
Epoch 59/60
Loss: 0.0079  [0/3200]
Validation Loss: 0.0087, Accuracy: 78.38%, F1 Score: 0.7788
Adjusting learning rate of group 0 to 1.6064e-04.
Epoch 60/60
Loss: 0.0063  [0/3200]
Validation Loss: 0.0093, Accuracy: 76.00%, F1 Score: 0.7529
Adjusting learning rate of group 0 to 1.6336e-04.

<ipython-input-41-785969b892fd>:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,


print(results_df)
   Batch Size  Best Accuracy  Best F1 Score    Time (s)
0         2.0         80.000       0.798569  532.487686
1         4.0         80.875       0.805687  269.526724
2         8.0         80.375       0.801175  139.239063
3        16.0         81.625       0.813300   71.567933
4        32.0         81.500       0.814557   37.567944
5        64.0         81.000       0.806589   20.638910
6       128.0         81.125       0.809945   11.557831

import time

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define the list of batch sizes
batch_sizes = [2, 2**2, 2**3, 2**4, 2**5, 2**6, 2**7]

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=['Batch Size', 'Best Accuracy', 'Best F1 Score', 'Time (s)'])

# Iterate over batch sizes
for batch_size in batch_sizes:
    # Set the random seeds for reproducibility within the loop
    torch.manual_seed(SEED)
    random.seed(SEED)
    np.random.seed(SEED)
    torch.cuda.manual_seed_all(SEED)

    # Define the data and loaders with the current batch size
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

    # Initialize the model
    model = CNN_pp(output_dim=4, activation=nn.Mish()).to(device)
    model.apply(weights_init)  # Initialize the weights

    optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0.0)

    # Create the scheduler based on the scheduler name
    if scheduler_name == 'OneCycleLR':
        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader), verbose=True)

    # Train the model
    print(f"Training with batch size {batch_size}...")
    best_accuracy = 0.0
    best_f1 = 0.0
    start_time = time.time()
    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        train_loop(train_loader, model, loss_fn, optimizer)
        test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
        print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

        # Update the best accuracy and F1 score
        if accuracy > best_accuracy:
            best_accuracy = accuracy
        if f1 > best_f1:
            best_f1 = f1

        scheduler.step()  # Update the learning rate

    end_time = time.time()
    elapsed_time = end_time - start_time

    # Add the results to the DataFrame
    results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,
                                    'Best F1 Score': best_f1, 'Time (s)': elapsed_time}, ignore_index=True)

Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 2...
Epoch 1/60
Loss: 2.2292  [0/3200]
Loss: 1.2886  [200/3200]
Loss: 1.7825  [400/3200]
Loss: 0.9313  [600/3200]
Loss: 1.2859  [800/3200]
Loss: 0.5999  [1000/3200]
Loss: 0.4298  [1200/3200]
Loss: 0.8027  [1400/3200]
Loss: 1.2693  [1600/3200]
Loss: 1.5294  [1800/3200]
Loss: 0.0219  [2000/3200]
Loss: 1.8608  [2200/3200]
Loss: 0.6175  [2400/3200]
Loss: 0.6150  [2600/3200]
Loss: 0.9795  [2800/3200]
Loss: 1.3573  [3000/3200]
Validation Loss: 0.3719, Accuracy: 69.25%, F1 Score: 0.6672
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.0782  [0/3200]
Loss: 1.0320  [200/3200]
Loss: 0.3152  [400/3200]
Loss: 0.4915  [600/3200]
Loss: 1.1379  [800/3200]
Loss: 0.4290  [1000/3200]
Loss: 1.0517  [1200/3200]
Loss: 0.5463  [1400/3200]
Loss: 0.2260  [1600/3200]
Loss: 0.7163  [1800/3200]
Loss: 0.0604  [2000/3200]
Loss: 0.2561  [2200/3200]
Loss: 0.2000  [2400/3200]
Loss: 0.2657  [2600/3200]
Loss: 0.1040  [2800/3200]
Loss: 2.8324  [3000/3200]
Validation Loss: 0.3137, Accuracy: 74.25%, F1 Score: 0.7344
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 3/60
Loss: 0.0978  [0/3200]
Loss: 0.0627  [200/3200]
Loss: 0.4852  [400/3200]
Loss: 0.7255  [600/3200]
Loss: 0.0802  [800/3200]
Loss: 1.1034  [1000/3200]
Loss: 0.1479  [1200/3200]
Loss: 0.9439  [1400/3200]
Loss: 0.6940  [1600/3200]
Loss: 0.3822  [1800/3200]
Loss: 0.0455  [2000/3200]
Loss: 0.0684  [2200/3200]
Loss: 0.9488  [2400/3200]
Loss: 1.2650  [2600/3200]
Loss: 0.2686  [2800/3200]
Loss: 0.0578  [3000/3200]
Validation Loss: 0.2938, Accuracy: 76.62%, F1 Score: 0.7716
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 4/60
Loss: 0.1976  [0/3200]
Loss: 1.1684  [200/3200]
Loss: 0.5151  [400/3200]
Loss: 0.4776  [600/3200]
Loss: 0.2275  [800/3200]
Loss: 0.9730  [1000/3200]
Loss: 0.2962  [1200/3200]
Loss: 0.0632  [1400/3200]
Loss: 0.2276  [1600/3200]
Loss: 0.7269  [1800/3200]
Loss: 0.7162  [2000/3200]
Loss: 0.6687  [2200/3200]
Loss: 0.0195  [2400/3200]
Loss: 0.4000  [2600/3200]
Loss: 0.1234  [2800/3200]
Loss: 0.2508  [3000/3200]
Validation Loss: 0.3149, Accuracy: 74.88%, F1 Score: 0.7425
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 5/60
Loss: 0.9104  [0/3200]
Loss: 0.0278  [200/3200]
Loss: 0.0743  [400/3200]
Loss: 0.3538  [600/3200]
Loss: 0.7755  [800/3200]
Loss: 0.5860  [1000/3200]
Loss: 0.5058  [1200/3200]
Loss: 0.2130  [1400/3200]
Loss: 1.1858  [1600/3200]
Loss: 0.0253  [1800/3200]
Loss: 1.2058  [2000/3200]
Loss: 0.0086  [2200/3200]
Loss: 0.7515  [2400/3200]
Loss: 0.1298  [2600/3200]
Loss: 0.1802  [2800/3200]
Loss: 2.1707  [3000/3200]
Validation Loss: 0.3392, Accuracy: 74.00%, F1 Score: 0.7471
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 6/60
Loss: 0.3102  [0/3200]
Loss: 1.1138  [200/3200]
Loss: 0.5715  [400/3200]
Loss: 0.2530  [600/3200]
Loss: 0.9667  [800/3200]
Loss: 0.0328  [1000/3200]
Loss: 0.2398  [1200/3200]
Loss: 1.3224  [1400/3200]
Loss: 0.3704  [1600/3200]
Loss: 0.4063  [1800/3200]
Loss: 0.0744  [2000/3200]
Loss: 1.2113  [2200/3200]
Loss: 0.0368  [2400/3200]
Loss: 0.0487  [2600/3200]
Loss: 0.3442  [2800/3200]
Loss: 0.2961  [3000/3200]
Validation Loss: 0.3115, Accuracy: 76.25%, F1 Score: 0.7531
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 7/60
Loss: 0.2397  [0/3200]
Loss: 1.0640  [200/3200]
Loss: 0.0413  [400/3200]
Loss: 0.7013  [600/3200]
Loss: 0.9645  [800/3200]
Loss: 0.0367  [1000/3200]
Loss: 1.3242  [1200/3200]
Loss: 2.1875  [1400/3200]
Loss: 0.8756  [1600/3200]
Loss: 0.8988  [1800/3200]
Loss: 1.6013  [2000/3200]
Loss: 1.7528  [2200/3200]
Loss: 0.2518  [2400/3200]
Loss: 0.4819  [2600/3200]
Loss: 1.0741  [2800/3200]
Loss: 2.1773  [3000/3200]
Validation Loss: 0.3213, Accuracy: 75.12%, F1 Score: 0.7469
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 8/60
Loss: 0.0797  [0/3200]
Loss: 0.0137  [200/3200]
Loss: 0.6677  [400/3200]
Loss: 0.1618  [600/3200]
Loss: 0.1385  [800/3200]
Loss: 0.0332  [1000/3200]
Loss: 0.5250  [1200/3200]
Loss: 0.3160  [1400/3200]
Loss: 0.0615  [1600/3200]
Loss: 0.0318  [1800/3200]
Loss: 0.0108  [2000/3200]
Loss: 0.3128  [2200/3200]
Loss: 0.3516  [2400/3200]
Loss: 1.6882  [2600/3200]
Loss: 0.0153  [2800/3200]
Loss: 0.2107  [3000/3200]
Validation Loss: 0.2751, Accuracy: 80.38%, F1 Score: 0.8005
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 9/60
Loss: 0.1819  [0/3200]
Loss: 0.2538  [200/3200]
Loss: 0.0145  [400/3200]
Loss: 0.0192  [600/3200]
Loss: 0.0887  [800/3200]
Loss: 0.1093  [1000/3200]
Loss: 0.3189  [1200/3200]
Loss: 0.2882  [1400/3200]
Loss: 0.5923  [1600/3200]
Loss: 1.3521  [1800/3200]
Loss: 0.0212  [2000/3200]
Loss: 0.7956  [2200/3200]
Loss: 0.0732  [2400/3200]
Loss: 0.0903  [2600/3200]
Loss: 0.4125  [2800/3200]
Loss: 0.3941  [3000/3200]
Validation Loss: 0.2960, Accuracy: 80.38%, F1 Score: 0.7987
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 10/60
Loss: 0.1258  [0/3200]
Loss: 0.2583  [200/3200]
Loss: 0.0446  [400/3200]
Loss: 0.0448  [600/3200]
Loss: 0.0492  [800/3200]
Loss: 0.0159  [1000/3200]
Loss: 0.0132  [1200/3200]
Loss: 0.4512  [1400/3200]
Loss: 0.0376  [1600/3200]
Loss: 0.4253  [1800/3200]
Loss: 0.0083  [2000/3200]
Loss: 0.1433  [2200/3200]
Loss: 0.5347  [2400/3200]
Loss: 0.0158  [2600/3200]
Loss: 2.0827  [2800/3200]
Loss: 0.3558  [3000/3200]
Validation Loss: 0.3281, Accuracy: 76.62%, F1 Score: 0.7631
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 11/60
Loss: 0.0711  [0/3200]
Loss: 1.9062  [200/3200]
Loss: 0.4526  [400/3200]
Loss: 0.5760  [600/3200]
Loss: 0.0036  [800/3200]
Loss: 0.0475  [1000/3200]
Loss: 0.0454  [1200/3200]
Loss: 0.2958  [1400/3200]
Loss: 0.0631  [1600/3200]
Loss: 0.4929  [1800/3200]
Loss: 0.0907  [2000/3200]
Loss: 0.3542  [2200/3200]
Loss: 0.0528  [2400/3200]
Loss: 0.0265  [2600/3200]
Loss: 0.0048  [2800/3200]
Loss: 0.0188  [3000/3200]
Validation Loss: 0.3009, Accuracy: 79.12%, F1 Score: 0.7937
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 12/60
Loss: 0.0533  [0/3200]
Loss: 0.2079  [200/3200]
Loss: 0.0030  [400/3200]
Loss: 0.1365  [600/3200]
Loss: 0.0156  [800/3200]
Loss: 0.0229  [1000/3200]
Loss: 0.0826  [1200/3200]
Loss: 0.1381  [1400/3200]
Loss: 0.4886  [1600/3200]
Loss: 0.3080  [1800/3200]
Loss: 0.5508  [2000/3200]
Loss: 0.3155  [2200/3200]
Loss: 0.3180  [2400/3200]
Loss: 0.6338  [2600/3200]
Loss: 0.0424  [2800/3200]
Loss: 0.2196  [3000/3200]
Validation Loss: 0.3144, Accuracy: 79.00%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 13/60
Loss: 0.0369  [0/3200]
Loss: 0.0055  [200/3200]
Loss: 0.6130  [400/3200]
Loss: 0.1505  [600/3200]
Loss: 0.0279  [800/3200]
Loss: 0.1725  [1000/3200]
Loss: 0.0910  [1200/3200]
Loss: 0.0858  [1400/3200]
Loss: 0.0157  [1600/3200]
Loss: 0.0042  [1800/3200]
Loss: 1.8757  [2000/3200]
Loss: 0.0021  [2200/3200]
Loss: 0.0190  [2400/3200]
Loss: 1.7535  [2600/3200]
Loss: 0.0351  [2800/3200]
Loss: 0.1495  [3000/3200]
Validation Loss: 0.3217, Accuracy: 78.50%, F1 Score: 0.7756
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 14/60
Loss: 0.0038  [0/3200]
Loss: 0.3989  [200/3200]
Loss: 0.0459  [400/3200]
Loss: 0.1441  [600/3200]
Loss: 0.2459  [800/3200]
Loss: 0.0205  [1000/3200]
Loss: 0.1360  [1200/3200]
Loss: 0.0217  [1400/3200]
Loss: 0.0041  [1600/3200]
Loss: 0.0061  [1800/3200]
Loss: 0.1379  [2000/3200]
Loss: 0.0557  [2200/3200]
Loss: 0.2516  [2400/3200]
Loss: 0.0003  [2600/3200]
Loss: 0.0343  [2800/3200]
Loss: 0.3584  [3000/3200]
Validation Loss: 0.3376, Accuracy: 77.12%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 15/60
Loss: 0.0849  [0/3200]
Loss: 0.2874  [200/3200]
Loss: 0.0335  [400/3200]
Loss: 0.7385  [600/3200]
Loss: 0.1233  [800/3200]
Loss: 0.0174  [1000/3200]
Loss: 0.0945  [1200/3200]
Loss: 0.0728  [1400/3200]
Loss: 0.1211  [1600/3200]
Loss: 0.1591  [1800/3200]
Loss: 0.0734  [2000/3200]
Loss: 0.0870  [2200/3200]
Loss: 0.0721  [2400/3200]
Loss: 0.0553  [2600/3200]
Loss: 0.1859  [2800/3200]
Loss: 0.0235  [3000/3200]
Validation Loss: 0.3477, Accuracy: 78.00%, F1 Score: 0.7819
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 16/60
Loss: 0.0121  [0/3200]
Loss: 0.4433  [200/3200]
Loss: 0.0414  [400/3200]
Loss: 0.0048  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0187  [1000/3200]
Loss: 0.1334  [1200/3200]
Loss: 0.0009  [1400/3200]
Loss: 0.0078  [1600/3200]
Loss: 0.0824  [1800/3200]
Loss: 0.0009  [2000/3200]
Loss: 0.0123  [2200/3200]
Loss: 0.0300  [2400/3200]
Loss: 0.0039  [2600/3200]
Loss: 0.0164  [2800/3200]
Loss: 0.3658  [3000/3200]
Validation Loss: 0.3401, Accuracy: 78.62%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 17/60
Loss: 0.0025  [0/3200]
Loss: 0.3135  [200/3200]
Loss: 0.0015  [400/3200]
Loss: 0.4555  [600/3200]
Loss: 0.0083  [800/3200]
Loss: 0.0042  [1000/3200]
Loss: 0.0014  [1200/3200]
Loss: 0.0143  [1400/3200]
Loss: 0.0462  [1600/3200]
Loss: 0.0222  [1800/3200]
Loss: 0.0019  [2000/3200]
Loss: 0.0211  [2200/3200]
Loss: 0.0078  [2400/3200]
Loss: 1.2533  [2600/3200]
Loss: 0.4913  [2800/3200]
Loss: 0.0075  [3000/3200]
Validation Loss: 0.3539, Accuracy: 80.62%, F1 Score: 0.8048
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 18/60
Loss: 0.0057  [0/3200]
Loss: 0.0138  [200/3200]
Loss: 0.0025  [400/3200]
Loss: 0.0595  [600/3200]
Loss: 0.0412  [800/3200]
Loss: 0.0530  [1000/3200]
Loss: 0.0827  [1200/3200]
Loss: 0.3019  [1400/3200]
Loss: 0.5634  [1600/3200]
Loss: 0.0711  [1800/3200]
Loss: 0.0078  [2000/3200]
Loss: 0.0024  [2200/3200]
Loss: 0.0665  [2400/3200]
Loss: 0.1884  [2600/3200]
Loss: 0.0914  [2800/3200]
Loss: 0.0115  [3000/3200]
Validation Loss: 0.3692, Accuracy: 78.50%, F1 Score: 0.7830
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 19/60
Loss: 0.0062  [0/3200]
Loss: 0.0854  [200/3200]
Loss: 0.0651  [400/3200]
Loss: 0.0038  [600/3200]
Loss: 0.0113  [800/3200]
Loss: 0.0010  [1000/3200]
Loss: 0.0054  [1200/3200]
Loss: 0.0049  [1400/3200]
Loss: 0.0034  [1600/3200]
Loss: 0.0005  [1800/3200]
Loss: 0.0264  [2000/3200]
Loss: 0.0003  [2200/3200]
Loss: 0.0005  [2400/3200]
Loss: 0.0111  [2600/3200]
Loss: 0.0004  [2800/3200]
Loss: 0.0735  [3000/3200]
Validation Loss: 0.4178, Accuracy: 77.50%, F1 Score: 0.7738
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 20/60
Loss: 0.0150  [0/3200]
Loss: 0.0213  [200/3200]
Loss: 0.0086  [400/3200]
Loss: 0.0095  [600/3200]
Loss: 0.0212  [800/3200]
Loss: 0.0076  [1000/3200]
Loss: 0.0399  [1200/3200]
Loss: 0.0134  [1400/3200]
Loss: 0.0005  [1600/3200]
Loss: 0.0034  [1800/3200]
Loss: 0.0438  [2000/3200]
Loss: 0.0082  [2200/3200]
Loss: 0.0039  [2400/3200]
Loss: 0.0053  [2600/3200]
Loss: 0.0370  [2800/3200]
Loss: 0.0430  [3000/3200]
Validation Loss: 0.4418, Accuracy: 75.62%, F1 Score: 0.7565
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 21/60
Loss: 0.0469  [0/3200]
Loss: 0.0271  [200/3200]
Loss: 0.0246  [400/3200]
Loss: 0.0188  [600/3200]
Loss: 0.0004  [800/3200]
Loss: 0.0636  [1000/3200]
Loss: 0.0098  [1200/3200]
Loss: 0.0006  [1400/3200]
Loss: 0.0977  [1600/3200]
Loss: 0.0075  [1800/3200]
Loss: 0.0686  [2000/3200]
Loss: 0.2058  [2200/3200]
Loss: 0.0318  [2400/3200]
Loss: 0.0129  [2600/3200]
Loss: 0.0003  [2800/3200]
Loss: 0.0106  [3000/3200]
Validation Loss: 0.4617, Accuracy: 76.50%, F1 Score: 0.7633
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 22/60
Loss: 0.0060  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0005  [400/3200]
Loss: 0.1305  [600/3200]
Loss: 0.0055  [800/3200]
Loss: 0.0014  [1000/3200]
Loss: 0.0003  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0017  [1800/3200]
Loss: 0.0837  [2000/3200]
Loss: 0.0255  [2200/3200]
Loss: 0.0373  [2400/3200]
Loss: 0.0017  [2600/3200]
Loss: 0.0013  [2800/3200]
Loss: 0.0054  [3000/3200]
Validation Loss: 0.5486, Accuracy: 74.00%, F1 Score: 0.7280
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 23/60
Loss: 0.0222  [0/3200]
Loss: 0.0683  [200/3200]
Loss: 0.0065  [400/3200]
Loss: 0.0169  [600/3200]
Loss: 0.0033  [800/3200]
Loss: 0.0028  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0077  [1400/3200]
Loss: 0.0067  [1600/3200]
Loss: 0.0059  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0002  [2200/3200]
Loss: 0.0024  [2400/3200]
Loss: 0.0217  [2600/3200]
Loss: 0.0078  [2800/3200]
Loss: 0.0026  [3000/3200]
Validation Loss: 0.4909, Accuracy: 77.25%, F1 Score: 0.7695
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 24/60
Loss: 0.0023  [0/3200]
Loss: 0.0024  [200/3200]
Loss: 0.0036  [400/3200]
Loss: 0.0005  [600/3200]
Loss: 0.0796  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0187  [1200/3200]
Loss: 0.0053  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0034  [1800/3200]
Loss: 0.1148  [2000/3200]
Loss: 0.0053  [2200/3200]
Loss: 0.0015  [2400/3200]
Loss: 0.0041  [2600/3200]
Loss: 0.0893  [2800/3200]
Loss: 0.0002  [3000/3200]
Validation Loss: 0.4669, Accuracy: 78.12%, F1 Score: 0.7788
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 25/60
Loss: 0.0004  [0/3200]
Loss: 0.0004  [200/3200]
Loss: 0.5793  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0002  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0028  [1400/3200]
Loss: 0.0103  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0708  [2000/3200]
Loss: 0.0010  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0520  [2600/3200]
Loss: 0.0042  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.5195, Accuracy: 76.62%, F1 Score: 0.7654
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 26/60
Loss: 0.0025  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0053  [400/3200]
Loss: 0.0004  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0002  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0042  [1400/3200]
Loss: 0.0029  [1600/3200]
Loss: 0.0174  [1800/3200]
Loss: 0.0027  [2000/3200]
Loss: 0.0012  [2200/3200]
Loss: 0.0014  [2400/3200]
Loss: 0.0058  [2600/3200]
Loss: 0.0002  [2800/3200]
Loss: 0.1941  [3000/3200]
Validation Loss: 0.4805, Accuracy: 79.62%, F1 Score: 0.7940
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 27/60
Loss: 0.0036  [0/3200]
Loss: 0.0003  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0006  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0011  [1000/3200]
Loss: 0.0022  [1200/3200]
Loss: 0.0039  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0079  [1800/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0008  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.5046, Accuracy: 79.50%, F1 Score: 0.7955
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 28/60
Loss: 0.0005  [0/3200]
Loss: 0.0003  [200/3200]
Loss: 0.0008  [400/3200]
Loss: 0.0009  [600/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0002  [1000/3200]
Loss: 0.0408  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0126  [1600/3200]
Loss: 0.0012  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0139  [2200/3200]
Loss: 0.0067  [2400/3200]
Loss: 0.0020  [2600/3200]
Loss: 0.0021  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.5293, Accuracy: 79.50%, F1 Score: 0.7924
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 29/60
Loss: 0.0020  [0/3200]
Loss: 0.0003  [200/3200]
Loss: 0.0022  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0008  [800/3200]
Loss: 0.0165  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0004  [1400/3200]
Loss: 0.0007  [1600/3200]
Loss: 0.0017  [1800/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0005  [2200/3200]
Loss: 0.0019  [2400/3200]
Loss: 0.0109  [2600/3200]
Loss: 0.0010  [2800/3200]
Loss: 0.0005  [3000/3200]
Validation Loss: 0.5616, Accuracy: 79.12%, F1 Score: 0.7900
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 30/60
Loss: 0.0005  [0/3200]
Loss: 0.0004  [200/3200]
Loss: 0.0018  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0018  [800/3200]
Loss: 0.0008  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0002  [2200/3200]
Loss: 0.0047  [2400/3200]
Loss: 0.0013  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0043  [3000/3200]
Validation Loss: 0.5667, Accuracy: 78.62%, F1 Score: 0.7866
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 31/60
Loss: 0.0004  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0013  [1000/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0003  [1400/3200]
Loss: 0.5019  [1600/3200]
Loss: 0.0007  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0036  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.5512, Accuracy: 79.12%, F1 Score: 0.7895
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 32/60
Loss: 0.0000  [0/3200]
Loss: 0.0016  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0002  [1000/3200]
Loss: 0.0054  [1200/3200]
Loss: 0.0015  [1400/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0156  [1800/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0008  [2200/3200]
Loss: 0.0005  [2400/3200]
Loss: 0.0005  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0012  [3000/3200]
Validation Loss: 0.5532, Accuracy: 78.62%, F1 Score: 0.7862
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 33/60
Loss: 0.0002  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0090  [400/3200]
Loss: 0.0013  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0013  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0018  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0025  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0147  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0004  [3000/3200]
Validation Loss: 0.6028, Accuracy: 77.50%, F1 Score: 0.7737
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 34/60
Loss: 0.0002  [0/3200]
Loss: 0.0024  [200/3200]
Loss: 0.0025  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0008  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0006  [1200/3200]
Loss: 0.0011  [1400/3200]
Loss: 0.0032  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0070  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0041  [2400/3200]
Loss: 0.0008  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0006  [3000/3200]
Validation Loss: 0.6234, Accuracy: 77.75%, F1 Score: 0.7790
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 35/60
Loss: 0.0026  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0137  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0017  [1800/3200]
Loss: 0.0169  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0005  [2400/3200]
Loss: 0.0006  [2600/3200]
Loss: 0.0009  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6134, Accuracy: 79.00%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 36/60
Loss: 0.0000  [0/3200]
Loss: 0.0007  [200/3200]
Loss: 0.1986  [400/3200]
Loss: 0.0039  [600/3200]
Loss: 0.0023  [800/3200]
Loss: 0.0007  [1000/3200]
Loss: 0.0022  [1200/3200]
Loss: 0.0008  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0004  [1800/3200]
Loss: 0.0007  [2000/3200]
Loss: 0.0007  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0011  [3000/3200]
Validation Loss: 0.6025, Accuracy: 78.62%, F1 Score: 0.7845
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 37/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0012  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0010  [1000/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0037  [1400/3200]
Loss: 0.0019  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0010  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7022, Accuracy: 75.50%, F1 Score: 0.7501
Adjusting learning rate of group 0 to 8.0008e-05.
Epoch 38/60
Loss: 0.0007  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0012  [400/3200]
Loss: 0.0007  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0004  [1800/3200]
Loss: 0.0005  [2000/3200]
Loss: 0.0003  [2200/3200]
Loss: 0.0003  [2400/3200]
Loss: 0.0004  [2600/3200]
Loss: 0.0008  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6454, Accuracy: 77.88%, F1 Score: 0.7769
Adjusting learning rate of group 0 to 8.0008e-05.
Epoch 39/60
Loss: 0.0003  [0/3200]
Loss: 0.0024  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0018  [600/3200]
Loss: 0.0003  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0007  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0251  [1800/3200]
Loss: 0.0041  [2000/3200]
Loss: 0.0009  [2200/3200]
Loss: 0.0053  [2400/3200]
Loss: 0.0006  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6299, Accuracy: 77.75%, F1 Score: 0.7751
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 40/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0020  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0002  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0025  [1800/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0007  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0003  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6738, Accuracy: 77.38%, F1 Score: 0.7717
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 41/60
Loss: 0.0000  [0/3200]
Loss: 0.0017  [200/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0005  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0006  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0002  [2400/3200]
Loss: 0.0012  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6475, Accuracy: 78.38%, F1 Score: 0.7815
Adjusting learning rate of group 0 to 8.0010e-05.
Epoch 42/60
Loss: 0.0014  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0005  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0004  [1200/3200]
Loss: 0.0400  [1400/3200]
Loss: 0.0119  [1600/3200]
Loss: 0.0003  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.6776, Accuracy: 77.50%, F1 Score: 0.7741
Adjusting learning rate of group 0 to 8.0010e-05.
Epoch 43/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0005  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0002  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0003  [1800/3200]
Loss: 0.0005  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0006  [2800/3200]
Loss: 0.0009  [3000/3200]
Validation Loss: 0.6714, Accuracy: 78.88%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 44/60
Loss: 0.0003  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0006  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0009  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0060  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6763, Accuracy: 78.62%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 45/60
Loss: 0.0000  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0003  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7065, Accuracy: 78.50%, F1 Score: 0.7834
Adjusting learning rate of group 0 to 8.0012e-05.
Epoch 46/60
Loss: 0.0000  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0040  [1600/3200]
Loss: 0.0093  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0007  [2400/3200]
Loss: 0.0002  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.6679, Accuracy: 79.00%, F1 Score: 0.7881
Adjusting learning rate of group 0 to 8.0012e-05.
Epoch 47/60
Loss: 0.0010  [0/3200]
Loss: 0.0005  [200/3200]
Loss: 0.0006  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0003  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0003  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0002  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0006  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0002  [3000/3200]
Validation Loss: 0.7071, Accuracy: 78.62%, F1 Score: 0.7865
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 48/60
Loss: 0.0011  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0006  [600/3200]
Loss: 0.0019  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0007  [1600/3200]
Loss: 0.0038  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0002  [3000/3200]
Validation Loss: 0.6749, Accuracy: 79.38%, F1 Score: 0.7917
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 49/60
Loss: 0.0000  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0006  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0007  [1800/3200]
Loss: 0.0004  [2000/3200]
Loss: 0.0001  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7113, Accuracy: 79.25%, F1 Score: 0.7911
Adjusting learning rate of group 0 to 8.0014e-05.
Epoch 50/60
Loss: 0.0001  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0003  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0002  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0009  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.7245, Accuracy: 78.25%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.0014e-05.
Epoch 51/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0003  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0003  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0002  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7301, Accuracy: 78.38%, F1 Score: 0.7814
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 52/60
Loss: 0.0001  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.7314, Accuracy: 78.25%, F1 Score: 0.7795
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 53/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0008  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0003  [2800/3200]
Loss: 0.0010  [3000/3200]
Validation Loss: 0.7523, Accuracy: 78.00%, F1 Score: 0.7775
Adjusting learning rate of group 0 to 8.0016e-05.
Epoch 54/60
Loss: 0.0000  [0/3200]
Loss: 0.0001  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0003  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0003  [1800/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.7652, Accuracy: 78.50%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0017e-05.
Epoch 55/60
Loss: 0.0000  [0/3200]
Loss: 0.0002  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0414  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0002  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7733, Accuracy: 79.00%, F1 Score: 0.7870
Adjusting learning rate of group 0 to 8.0017e-05.
Epoch 56/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0000  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0004  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7588, Accuracy: 78.62%, F1 Score: 0.7848
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 57/60
Loss: 0.0001  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0001  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0001  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.7700, Accuracy: 78.38%, F1 Score: 0.7814
Adjusting learning rate of group 0 to 8.0019e-05.
Epoch 58/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0183  [1600/3200]
Loss: 0.0006  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0002  [2800/3200]
Loss: 0.0001  [3000/3200]
Validation Loss: 0.7582, Accuracy: 78.00%, F1 Score: 0.7771
Adjusting learning rate of group 0 to 8.0019e-05.
Epoch 59/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [200/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0000  [600/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1400/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0003  [1800/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0001  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7726, Accuracy: 78.25%, F1 Score: 0.7804
Adjusting learning rate of group 0 to 8.0020e-05.
Epoch 60/60
Loss: 0.0000  [0/3200]
Loss: 0.0003  [200/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0001  [600/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0001  [1000/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0001  [1400/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [1800/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2200/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2600/3200]
Loss: 0.0000  [2800/3200]
Loss: 0.0000  [3000/3200]
Validation Loss: 0.7778, Accuracy: 78.38%, F1 Score: 0.7815
Adjusting learning rate of group 0 to 8.0021e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 4...
Epoch 1/60
Loss: 1.8172  [0/3200]

<ipython-input-42-fa3622509717>:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Loss: 1.4316  [400/3200]
Loss: 1.0382  [800/3200]
Loss: 1.0741  [1200/3200]
Loss: 1.3246  [1600/3200]
Loss: 0.4029  [2000/3200]
Loss: 1.3718  [2400/3200]
Loss: 0.9872  [2800/3200]
Validation Loss: 0.2140, Accuracy: 61.00%, F1 Score: 0.5296
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.4742  [0/3200]
Loss: 0.4623  [400/3200]
Loss: 0.6106  [800/3200]
Loss: 0.8271  [1200/3200]
Loss: 0.5088  [1600/3200]
Loss: 0.8617  [2000/3200]
Loss: 0.2621  [2400/3200]
Loss: 0.1364  [2800/3200]
Validation Loss: 0.1711, Accuracy: 72.50%, F1 Score: 0.7255
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 3/60
Loss: 0.4453  [0/3200]
Loss: 0.4916  [400/3200]
Loss: 0.5804  [800/3200]
Loss: 0.4507  [1200/3200]
Loss: 0.7996  [1600/3200]
Loss: 0.5177  [2000/3200]
Loss: 0.8797  [2400/3200]
Loss: 0.1776  [2800/3200]
Validation Loss: 0.1545, Accuracy: 76.75%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 4/60
Loss: 0.2752  [0/3200]
Loss: 0.4817  [400/3200]
Loss: 0.4376  [800/3200]
Loss: 0.2677  [1200/3200]
Loss: 0.3542  [1600/3200]
Loss: 0.5598  [2000/3200]
Loss: 0.1299  [2400/3200]
Loss: 0.2109  [2800/3200]
Validation Loss: 0.1393, Accuracy: 79.50%, F1 Score: 0.7932
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 5/60
Loss: 0.8350  [0/3200]
Loss: 0.0793  [400/3200]
Loss: 0.6113  [800/3200]
Loss: 0.6118  [1200/3200]
Loss: 0.5786  [1600/3200]
Loss: 0.7957  [2000/3200]
Loss: 0.6853  [2400/3200]
Loss: 0.3749  [2800/3200]
Validation Loss: 0.1832, Accuracy: 72.00%, F1 Score: 0.7235
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 6/60
Loss: 0.3120  [0/3200]
Loss: 0.7278  [400/3200]
Loss: 1.5368  [800/3200]
Loss: 0.3022  [1200/3200]
Loss: 0.1644  [1600/3200]
Loss: 0.0984  [2000/3200]
Loss: 0.4863  [2400/3200]
Loss: 0.4510  [2800/3200]
Validation Loss: 0.1370, Accuracy: 80.12%, F1 Score: 0.7989
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 7/60
Loss: 0.8451  [0/3200]
Loss: 0.3374  [400/3200]
Loss: 0.7209  [800/3200]
Loss: 1.2066  [1200/3200]
Loss: 0.4384  [1600/3200]
Loss: 0.6011  [2000/3200]
Loss: 0.2509  [2400/3200]
Loss: 0.3115  [2800/3200]
Validation Loss: 0.1370, Accuracy: 78.88%, F1 Score: 0.7872
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 8/60
Loss: 0.4076  [0/3200]
Loss: 1.0486  [400/3200]
Loss: 0.1560  [800/3200]
Loss: 0.1822  [1200/3200]
Loss: 0.5479  [1600/3200]
Loss: 0.1112  [2000/3200]
Loss: 0.6553  [2400/3200]
Loss: 0.1978  [2800/3200]
Validation Loss: 0.1400, Accuracy: 80.75%, F1 Score: 0.8076
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 9/60
Loss: 0.2700  [0/3200]
Loss: 0.9094  [400/3200]
Loss: 0.2256  [800/3200]
Loss: 0.8202  [1200/3200]
Loss: 0.2822  [1600/3200]
Loss: 0.1704  [2000/3200]
Loss: 0.1908  [2400/3200]
Loss: 0.2698  [2800/3200]
Validation Loss: 0.1344, Accuracy: 80.00%, F1 Score: 0.7950
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 10/60
Loss: 0.9889  [0/3200]
Loss: 0.2590  [400/3200]
Loss: 0.1893  [800/3200]
Loss: 0.0585  [1200/3200]
Loss: 0.0382  [1600/3200]
Loss: 0.2289  [2000/3200]
Loss: 0.3787  [2400/3200]
Loss: 1.5825  [2800/3200]
Validation Loss: 0.1492, Accuracy: 77.75%, F1 Score: 0.7811
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 11/60
Loss: 0.2052  [0/3200]
Loss: 0.3572  [400/3200]
Loss: 0.0157  [800/3200]
Loss: 0.3591  [1200/3200]
Loss: 0.0794  [1600/3200]
Loss: 0.0215  [2000/3200]
Loss: 0.2667  [2400/3200]
Loss: 0.2045  [2800/3200]
Validation Loss: 0.1705, Accuracy: 74.25%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 12/60
Loss: 0.3066  [0/3200]
Loss: 0.0075  [400/3200]
Loss: 0.3789  [800/3200]
Loss: 0.1676  [1200/3200]
Loss: 0.1307  [1600/3200]
Loss: 0.6473  [2000/3200]
Loss: 0.4945  [2400/3200]
Loss: 0.0759  [2800/3200]
Validation Loss: 0.1395, Accuracy: 79.75%, F1 Score: 0.8004
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 13/60
Loss: 0.3234  [0/3200]
Loss: 0.6422  [400/3200]
Loss: 0.4800  [800/3200]
Loss: 0.1265  [1200/3200]
Loss: 0.0365  [1600/3200]
Loss: 0.8695  [2000/3200]
Loss: 0.0908  [2400/3200]
Loss: 0.0783  [2800/3200]
Validation Loss: 0.1381, Accuracy: 81.50%, F1 Score: 0.8117
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 14/60
Loss: 0.1465  [0/3200]
Loss: 0.1500  [400/3200]
Loss: 0.3814  [800/3200]
Loss: 0.1814  [1200/3200]
Loss: 0.0823  [1600/3200]
Loss: 0.5842  [2000/3200]
Loss: 0.3180  [2400/3200]
Loss: 0.0929  [2800/3200]
Validation Loss: 0.1419, Accuracy: 81.00%, F1 Score: 0.8088
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 15/60
Loss: 0.1164  [0/3200]
Loss: 0.2501  [400/3200]
Loss: 0.6701  [800/3200]
Loss: 0.1143  [1200/3200]
Loss: 0.2848  [1600/3200]
Loss: 0.1488  [2000/3200]
Loss: 0.2331  [2400/3200]
Loss: 0.4802  [2800/3200]
Validation Loss: 0.1598, Accuracy: 76.12%, F1 Score: 0.7519
Adjusting learning rate of group 0 to 8.0005e-05.
Epoch 16/60
Loss: 0.5487  [0/3200]
Loss: 0.2355  [400/3200]
Loss: 0.0083  [800/3200]
Loss: 0.0501  [1200/3200]
Loss: 0.5645  [1600/3200]
Loss: 0.0513  [2000/3200]
Loss: 0.2611  [2400/3200]
Loss: 0.3696  [2800/3200]
Validation Loss: 0.1521, Accuracy: 78.25%, F1 Score: 0.7820
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 17/60
Loss: 0.0381  [0/3200]
Loss: 1.2607  [400/3200]
Loss: 0.0209  [800/3200]
Loss: 0.3018  [1200/3200]
Loss: 0.0765  [1600/3200]
Loss: 0.0314  [2000/3200]
Loss: 0.0499  [2400/3200]
Loss: 0.0772  [2800/3200]
Validation Loss: 0.1445, Accuracy: 80.88%, F1 Score: 0.8077
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 18/60
Loss: 0.0887  [0/3200]
Loss: 0.0291  [400/3200]
Loss: 0.0242  [800/3200]
Loss: 0.1291  [1200/3200]
Loss: 0.2738  [1600/3200]
Loss: 0.3078  [2000/3200]
Loss: 0.0897  [2400/3200]
Loss: 0.1764  [2800/3200]
Validation Loss: 0.1523, Accuracy: 78.00%, F1 Score: 0.7765
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 19/60
Loss: 0.1750  [0/3200]
Loss: 0.3230  [400/3200]
Loss: 0.0123  [800/3200]
Loss: 0.0252  [1200/3200]
Loss: 0.0161  [1600/3200]
Loss: 0.1783  [2000/3200]
Loss: 0.0043  [2400/3200]
Loss: 0.0157  [2800/3200]
Validation Loss: 0.1603, Accuracy: 79.50%, F1 Score: 0.7896
Adjusting learning rate of group 0 to 8.0008e-05.
Epoch 20/60
Loss: 0.0713  [0/3200]
Loss: 0.0360  [400/3200]
Loss: 0.1010  [800/3200]
Loss: 0.2569  [1200/3200]
Loss: 0.0233  [1600/3200]
Loss: 0.1346  [2000/3200]
Loss: 0.3523  [2400/3200]
Loss: 0.0875  [2800/3200]
Validation Loss: 0.1737, Accuracy: 77.50%, F1 Score: 0.7751
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 21/60
Loss: 0.1008  [0/3200]
Loss: 0.1396  [400/3200]
Loss: 0.0430  [800/3200]
Loss: 0.7457  [1200/3200]
Loss: 0.5951  [1600/3200]
Loss: 0.3125  [2000/3200]
Loss: 0.0548  [2400/3200]
Loss: 0.0303  [2800/3200]
Validation Loss: 0.1712, Accuracy: 78.38%, F1 Score: 0.7748
Adjusting learning rate of group 0 to 8.0010e-05.
Epoch 22/60
Loss: 0.1926  [0/3200]
Loss: 0.0099  [400/3200]
Loss: 0.0467  [800/3200]
Loss: 0.0242  [1200/3200]
Loss: 0.1681  [1600/3200]
Loss: 0.2134  [2000/3200]
Loss: 0.1857  [2400/3200]
Loss: 0.0578  [2800/3200]
Validation Loss: 0.1767, Accuracy: 78.25%, F1 Score: 0.7799
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 23/60
Loss: 0.1173  [0/3200]
Loss: 0.0213  [400/3200]
Loss: 0.0414  [800/3200]
Loss: 0.0012  [1200/3200]
Loss: 0.0888  [1600/3200]
Loss: 0.0128  [2000/3200]
Loss: 0.0689  [2400/3200]
Loss: 0.1583  [2800/3200]
Validation Loss: 0.2141, Accuracy: 76.50%, F1 Score: 0.7553
Adjusting learning rate of group 0 to 8.0012e-05.
Epoch 24/60
Loss: 0.0246  [0/3200]
Loss: 0.5413  [400/3200]
Loss: 0.3476  [800/3200]
Loss: 0.0446  [1200/3200]
Loss: 0.0178  [1600/3200]
Loss: 0.3163  [2000/3200]
Loss: 0.0052  [2400/3200]
Loss: 0.0693  [2800/3200]
Validation Loss: 0.1757, Accuracy: 78.38%, F1 Score: 0.7798
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 25/60
Loss: 0.0454  [0/3200]
Loss: 0.3114  [400/3200]
Loss: 0.0006  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.1489  [1600/3200]
Loss: 0.2558  [2000/3200]
Loss: 0.0068  [2400/3200]
Loss: 0.0161  [2800/3200]
Validation Loss: 0.1980, Accuracy: 77.75%, F1 Score: 0.7777
Adjusting learning rate of group 0 to 8.0014e-05.
Epoch 26/60
Loss: 0.0922  [0/3200]
Loss: 0.0584  [400/3200]
Loss: 0.0385  [800/3200]
Loss: 0.0029  [1200/3200]
Loss: 0.0293  [1600/3200]
Loss: 0.0118  [2000/3200]
Loss: 0.0232  [2400/3200]
Loss: 0.0110  [2800/3200]
Validation Loss: 0.1944, Accuracy: 78.75%, F1 Score: 0.7824
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 27/60
Loss: 0.0228  [0/3200]
Loss: 0.0122  [400/3200]
Loss: 0.0593  [800/3200]
Loss: 0.0140  [1200/3200]
Loss: 0.0024  [1600/3200]
Loss: 0.0443  [2000/3200]
Loss: 0.0022  [2400/3200]
Loss: 0.0642  [2800/3200]
Validation Loss: 0.1951, Accuracy: 79.88%, F1 Score: 0.7961
Adjusting learning rate of group 0 to 8.0017e-05.
Epoch 28/60
Loss: 0.0106  [0/3200]
Loss: 0.0606  [400/3200]
Loss: 0.0186  [800/3200]
Loss: 0.0237  [1200/3200]
Loss: 0.0313  [1600/3200]
Loss: 0.0013  [2000/3200]
Loss: 0.0191  [2400/3200]
Loss: 0.2877  [2800/3200]
Validation Loss: 0.1969, Accuracy: 80.25%, F1 Score: 0.7998
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 29/60
Loss: 0.0135  [0/3200]
Loss: 0.0476  [400/3200]
Loss: 0.0132  [800/3200]
Loss: 0.0003  [1200/3200]
Loss: 0.0118  [1600/3200]
Loss: 0.0455  [2000/3200]
Loss: 0.0091  [2400/3200]
Loss: 0.0041  [2800/3200]
Validation Loss: 0.2083, Accuracy: 79.88%, F1 Score: 0.7956
Adjusting learning rate of group 0 to 8.0019e-05.
Epoch 30/60
Loss: 0.0097  [0/3200]
Loss: 0.0031  [400/3200]
Loss: 0.0068  [800/3200]
Loss: 0.0028  [1200/3200]
Loss: 0.0066  [1600/3200]
Loss: 0.0749  [2000/3200]
Loss: 0.0114  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.2037, Accuracy: 80.38%, F1 Score: 0.8006
Adjusting learning rate of group 0 to 8.0021e-05.
Epoch 31/60
Loss: 0.0029  [0/3200]
Loss: 0.0239  [400/3200]
Loss: 0.0004  [800/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0076  [1600/3200]
Loss: 0.0003  [2000/3200]
Loss: 0.0023  [2400/3200]
Loss: 0.0060  [2800/3200]
Validation Loss: 0.2135, Accuracy: 79.62%, F1 Score: 0.7944
Adjusting learning rate of group 0 to 8.0022e-05.
Epoch 32/60
Loss: 0.0199  [0/3200]
Loss: 0.0077  [400/3200]
Loss: 0.0027  [800/3200]
Loss: 0.0161  [1200/3200]
Loss: 0.0013  [1600/3200]
Loss: 0.0089  [2000/3200]
Loss: 0.0015  [2400/3200]
Loss: 0.0006  [2800/3200]
Validation Loss: 0.2187, Accuracy: 80.25%, F1 Score: 0.8031
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 33/60
Loss: 0.0110  [0/3200]
Loss: 0.0272  [400/3200]
Loss: 0.0438  [800/3200]
Loss: 0.0137  [1200/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0083  [2400/3200]
Loss: 0.0054  [2800/3200]
Validation Loss: 0.2251, Accuracy: 79.50%, F1 Score: 0.7933
Adjusting learning rate of group 0 to 8.0025e-05.
Epoch 34/60
Loss: 0.0039  [0/3200]
Loss: 0.0015  [400/3200]
Loss: 0.0032  [800/3200]
Loss: 0.0067  [1200/3200]
Loss: 0.0274  [1600/3200]
Loss: 0.0081  [2000/3200]
Loss: 0.0098  [2400/3200]
Loss: 0.0021  [2800/3200]
Validation Loss: 0.2388, Accuracy: 79.62%, F1 Score: 0.7948
Adjusting learning rate of group 0 to 8.0026e-05.
Epoch 35/60
Loss: 0.0150  [0/3200]
Loss: 0.0051  [400/3200]
Loss: 0.0105  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0197  [1600/3200]
Loss: 0.0299  [2000/3200]
Loss: 0.0302  [2400/3200]
Loss: 0.0018  [2800/3200]
Validation Loss: 0.2314, Accuracy: 80.00%, F1 Score: 0.7970
Adjusting learning rate of group 0 to 8.0028e-05.
Epoch 36/60
Loss: 0.0025  [0/3200]
Loss: 0.0028  [400/3200]
Loss: 0.0015  [800/3200]
Loss: 0.0038  [1200/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0071  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0018  [2800/3200]
Validation Loss: 0.2533, Accuracy: 78.25%, F1 Score: 0.7785
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 37/60
Loss: 0.0109  [0/3200]
Loss: 0.0058  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0044  [1200/3200]
Loss: 0.0022  [1600/3200]
Loss: 0.0007  [2000/3200]
Loss: 0.0013  [2400/3200]
Loss: 0.0003  [2800/3200]
Validation Loss: 0.2542, Accuracy: 80.12%, F1 Score: 0.7990
Adjusting learning rate of group 0 to 8.0031e-05.
Epoch 38/60
Loss: 0.0009  [0/3200]
Loss: 0.0019  [400/3200]
Loss: 0.0021  [800/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0029  [1600/3200]
Loss: 0.0009  [2000/3200]
Loss: 0.0021  [2400/3200]
Loss: 0.0006  [2800/3200]
Validation Loss: 0.2600, Accuracy: 79.12%, F1 Score: 0.7906
Adjusting learning rate of group 0 to 8.0033e-05.
Epoch 39/60
Loss: 0.0203  [0/3200]
Loss: 0.0006  [400/3200]
Loss: 0.0013  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0005  [1600/3200]
Loss: 0.0064  [2000/3200]
Loss: 0.0013  [2400/3200]
Loss: 0.0047  [2800/3200]
Validation Loss: 0.2543, Accuracy: 79.88%, F1 Score: 0.7962
Adjusting learning rate of group 0 to 8.0035e-05.
Epoch 40/60
Loss: 0.0000  [0/3200]
Loss: 0.0021  [400/3200]
Loss: 0.0015  [800/3200]
Loss: 0.0010  [1200/3200]
Loss: 0.0006  [1600/3200]
Loss: 0.0027  [2000/3200]
Loss: 0.0137  [2400/3200]
Loss: 0.0030  [2800/3200]
Validation Loss: 0.2961, Accuracy: 77.88%, F1 Score: 0.7788
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 41/60
Loss: 0.0000  [0/3200]
Loss: 0.0027  [400/3200]
Loss: 0.0022  [800/3200]
Loss: 0.0008  [1200/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0005  [2400/3200]
Loss: 0.0255  [2800/3200]
Validation Loss: 0.2795, Accuracy: 78.38%, F1 Score: 0.7800
Adjusting learning rate of group 0 to 8.0038e-05.
Epoch 42/60
Loss: 0.0077  [0/3200]
Loss: 0.0014  [400/3200]
Loss: 0.0957  [800/3200]
Loss: 0.0006  [1200/3200]
Loss: 0.0057  [1600/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0002  [2400/3200]
Loss: 0.0007  [2800/3200]
Validation Loss: 0.2744, Accuracy: 79.75%, F1 Score: 0.7983
Adjusting learning rate of group 0 to 8.0040e-05.
Epoch 43/60
Loss: 0.0006  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0011  [1200/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0036  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0028  [2800/3200]
Validation Loss: 0.2776, Accuracy: 80.88%, F1 Score: 0.8071
Adjusting learning rate of group 0 to 8.0042e-05.
Epoch 44/60
Loss: 0.0003  [0/3200]
Loss: 0.0005  [400/3200]
Loss: 0.0003  [800/3200]
Loss: 0.3094  [1200/3200]
Loss: 0.1018  [1600/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0017  [2400/3200]
Loss: 0.0008  [2800/3200]
Validation Loss: 0.2744, Accuracy: 80.25%, F1 Score: 0.8000
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 45/60
Loss: 0.0014  [0/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0015  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0011  [1600/3200]
Loss: 0.0006  [2000/3200]
Loss: 0.0006  [2400/3200]
Loss: 0.0005  [2800/3200]
Validation Loss: 0.2750, Accuracy: 80.38%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.0046e-05.
Epoch 46/60
Loss: 0.0000  [0/3200]
Loss: 0.0003  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0025  [1200/3200]
Loss: 0.0008  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0009  [2400/3200]
Loss: 0.0011  [2800/3200]
Validation Loss: 0.2763, Accuracy: 80.12%, F1 Score: 0.7986
Adjusting learning rate of group 0 to 8.0048e-05.
Epoch 47/60
Loss: 0.0002  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0013  [800/3200]
Loss: 0.0005  [1200/3200]
Loss: 0.0015  [1600/3200]
Loss: 0.0004  [2000/3200]
Loss: 0.0014  [2400/3200]
Loss: 0.0002  [2800/3200]
Validation Loss: 0.2797, Accuracy: 80.38%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.0050e-05.
Epoch 48/60
Loss: 0.0014  [0/3200]
Loss: 0.0028  [400/3200]
Loss: 0.0010  [800/3200]
Loss: 0.0003  [1200/3200]
Loss: 0.0971  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0003  [2400/3200]
Loss: 0.0025  [2800/3200]
Validation Loss: 0.2939, Accuracy: 80.75%, F1 Score: 0.8060
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 49/60
Loss: 0.0013  [0/3200]
Loss: 0.0010  [400/3200]
Loss: 0.0009  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0002  [2400/3200]
Loss: 0.0012  [2800/3200]
Validation Loss: 0.2947, Accuracy: 80.50%, F1 Score: 0.8030
Adjusting learning rate of group 0 to 8.0055e-05.
Epoch 50/60
Loss: 0.0001  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0003  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.3074, Accuracy: 79.75%, F1 Score: 0.7945
Adjusting learning rate of group 0 to 8.0057e-05.
Epoch 51/60
Loss: 0.0007  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0003  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0011  [1600/3200]
Loss: 0.0009  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2800/3200]
Validation Loss: 0.2982, Accuracy: 79.75%, F1 Score: 0.7955
Adjusting learning rate of group 0 to 8.0059e-05.
Epoch 52/60
Loss: 0.0004  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0003  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0007  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0002  [2800/3200]
Validation Loss: 0.2968, Accuracy: 80.00%, F1 Score: 0.7976
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 53/60
Loss: 0.0008  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0008  [800/3200]
Loss: 0.0000  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0003  [2800/3200]
Validation Loss: 0.3082, Accuracy: 80.50%, F1 Score: 0.8031
Adjusting learning rate of group 0 to 8.0064e-05.
Epoch 54/60
Loss: 0.0001  [0/3200]
Loss: 0.0001  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0006  [1200/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0004  [2000/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0000  [2800/3200]
Validation Loss: 0.3124, Accuracy: 80.00%, F1 Score: 0.7970
Adjusting learning rate of group 0 to 8.0067e-05.
Epoch 55/60
Loss: 0.0001  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0000  [2000/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0006  [2800/3200]
Validation Loss: 0.3088, Accuracy: 79.75%, F1 Score: 0.7940
Adjusting learning rate of group 0 to 8.0069e-05.
Epoch 56/60
Loss: 0.0001  [0/3200]
Loss: 0.0005  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0009  [1200/3200]
Loss: 0.0010  [1600/3200]
Loss: 0.0004  [2000/3200]
Loss: 0.0011  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.3122, Accuracy: 80.38%, F1 Score: 0.8018
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 57/60
Loss: 0.0004  [0/3200]
Loss: 0.0002  [400/3200]
Loss: 0.0001  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0000  [2800/3200]
Validation Loss: 0.3189, Accuracy: 80.00%, F1 Score: 0.7977
Adjusting learning rate of group 0 to 8.0074e-05.
Epoch 58/60
Loss: 0.0000  [0/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0716  [800/3200]
Loss: 0.0049  [1200/3200]
Loss: 0.0007  [1600/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0005  [2400/3200]
Loss: 0.0019  [2800/3200]
Validation Loss: 0.3121, Accuracy: 79.88%, F1 Score: 0.7962
Adjusting learning rate of group 0 to 8.0077e-05.
Epoch 59/60
Loss: 0.0001  [0/3200]
Loss: 0.0004  [400/3200]
Loss: 0.0006  [800/3200]
Loss: 0.0001  [1200/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0001  [2000/3200]
Loss: 0.0001  [2400/3200]
Loss: 0.0014  [2800/3200]
Validation Loss: 0.3157, Accuracy: 79.75%, F1 Score: 0.7957
Adjusting learning rate of group 0 to 8.0080e-05.
Epoch 60/60
Loss: 0.0004  [0/3200]
Loss: 0.0000  [400/3200]
Loss: 0.0000  [800/3200]
Loss: 0.0002  [1200/3200]
Loss: 0.0001  [1600/3200]
Loss: 0.0002  [2000/3200]
Loss: 0.0000  [2400/3200]
Loss: 0.0001  [2800/3200]
Validation Loss: 0.3122, Accuracy: 79.75%, F1 Score: 0.7960
Adjusting learning rate of group 0 to 8.0082e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 8...
Epoch 1/60
Loss: 1.8980  [0/3200]

<ipython-input-42-fa3622509717>:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Loss: 0.8699  [800/3200]
Loss: 1.5543  [1600/3200]
Loss: 0.8129  [2400/3200]
Validation Loss: 0.1158, Accuracy: 62.88%, F1 Score: 0.5394
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.1836  [0/3200]
Loss: 0.7224  [800/3200]
Loss: 0.8824  [1600/3200]
Loss: 0.5047  [2400/3200]
Validation Loss: 0.0881, Accuracy: 69.88%, F1 Score: 0.7011
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 3/60
Loss: 0.5893  [0/3200]
Loss: 0.9619  [800/3200]
Loss: 0.5124  [1600/3200]
Loss: 0.4516  [2400/3200]
Validation Loss: 0.0946, Accuracy: 70.00%, F1 Score: 0.7015
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 4/60
Loss: 0.5944  [0/3200]
Loss: 0.7177  [800/3200]
Loss: 0.2184  [1600/3200]
Loss: 0.3766  [2400/3200]
Validation Loss: 0.0786, Accuracy: 77.38%, F1 Score: 0.7654
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 5/60
Loss: 0.7047  [0/3200]
Loss: 0.8518  [800/3200]
Loss: 0.6340  [1600/3200]
Loss: 0.3946  [2400/3200]
Validation Loss: 0.0703, Accuracy: 79.62%, F1 Score: 0.7979
Adjusting learning rate of group 0 to 8.0002e-05.
Epoch 6/60
Loss: 0.5635  [0/3200]
Loss: 1.1952  [800/3200]
Loss: 0.2204  [1600/3200]
Loss: 0.6484  [2400/3200]
Validation Loss: 0.0786, Accuracy: 77.50%, F1 Score: 0.7651
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 7/60
Loss: 0.4310  [0/3200]
Loss: 0.5425  [800/3200]
Loss: 0.3925  [1600/3200]
Loss: 0.2554  [2400/3200]
Validation Loss: 0.0787, Accuracy: 76.00%, F1 Score: 0.7532
Adjusting learning rate of group 0 to 8.0004e-05.
Epoch 8/60
Loss: 0.4246  [0/3200]
Loss: 0.6309  [800/3200]
Loss: 0.2831  [1600/3200]
Loss: 0.5985  [2400/3200]
Validation Loss: 0.0671, Accuracy: 81.12%, F1 Score: 0.8096
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 9/60
Loss: 0.2007  [0/3200]
Loss: 0.0657  [800/3200]
Loss: 0.2392  [1600/3200]
Loss: 0.1373  [2400/3200]
Validation Loss: 0.0731, Accuracy: 78.38%, F1 Score: 0.7777
Adjusting learning rate of group 0 to 8.0007e-05.
Epoch 10/60
Loss: 0.6387  [0/3200]
Loss: 1.4408  [800/3200]
Loss: 0.7380  [1600/3200]
Loss: 0.4479  [2400/3200]
Validation Loss: 0.0804, Accuracy: 76.00%, F1 Score: 0.7596
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 11/60
Loss: 0.2747  [0/3200]
Loss: 0.0287  [800/3200]
Loss: 0.1417  [1600/3200]
Loss: 0.4524  [2400/3200]
Validation Loss: 0.0781, Accuracy: 78.38%, F1 Score: 0.7870
Adjusting learning rate of group 0 to 8.0011e-05.
Epoch 12/60
Loss: 0.4361  [0/3200]
Loss: 0.1633  [800/3200]
Loss: 0.1671  [1600/3200]
Loss: 0.2550  [2400/3200]
Validation Loss: 0.0698, Accuracy: 80.38%, F1 Score: 0.8036
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 13/60
Loss: 0.1532  [0/3200]
Loss: 0.4543  [800/3200]
Loss: 0.4102  [1600/3200]
Loss: 0.0731  [2400/3200]
Validation Loss: 0.0724, Accuracy: 79.38%, F1 Score: 0.7857
Adjusting learning rate of group 0 to 8.0015e-05.
Epoch 14/60
Loss: 0.1912  [0/3200]
Loss: 0.3239  [800/3200]
Loss: 0.4769  [1600/3200]
Loss: 0.4945  [2400/3200]
Validation Loss: 0.0804, Accuracy: 74.88%, F1 Score: 0.7545
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 15/60
Loss: 0.2334  [0/3200]
Loss: 0.4079  [800/3200]
Loss: 0.2815  [1600/3200]
Loss: 0.0751  [2400/3200]
Validation Loss: 0.0837, Accuracy: 76.88%, F1 Score: 0.7622
Adjusting learning rate of group 0 to 8.0021e-05.
Epoch 16/60
Loss: 0.3954  [0/3200]
Loss: 0.0543  [800/3200]
Loss: 0.4484  [1600/3200]
Loss: 0.2277  [2400/3200]
Validation Loss: 0.0784, Accuracy: 77.50%, F1 Score: 0.7654
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 17/60
Loss: 0.1409  [0/3200]
Loss: 0.3165  [800/3200]
Loss: 0.0942  [1600/3200]
Loss: 0.4572  [2400/3200]
Validation Loss: 0.0824, Accuracy: 77.00%, F1 Score: 0.7687
Adjusting learning rate of group 0 to 8.0026e-05.
Epoch 18/60
Loss: 0.0745  [0/3200]
Loss: 0.0535  [800/3200]
Loss: 0.2950  [1600/3200]
Loss: 0.0776  [2400/3200]
Validation Loss: 0.0731, Accuracy: 78.75%, F1 Score: 0.7869
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 19/60
Loss: 0.0971  [0/3200]
Loss: 0.1455  [800/3200]
Loss: 0.1569  [1600/3200]
Loss: 0.0979  [2400/3200]
Validation Loss: 0.0814, Accuracy: 79.88%, F1 Score: 0.7951
Adjusting learning rate of group 0 to 8.0033e-05.
Epoch 20/60
Loss: 0.1039  [0/3200]
Loss: 0.0282  [800/3200]
Loss: 0.0054  [1600/3200]
Loss: 0.0268  [2400/3200]
Validation Loss: 0.0886, Accuracy: 76.25%, F1 Score: 0.7648
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 21/60
Loss: 0.0775  [0/3200]
Loss: 0.1207  [800/3200]
Loss: 0.0889  [1600/3200]
Loss: 0.1019  [2400/3200]
Validation Loss: 0.0788, Accuracy: 79.12%, F1 Score: 0.7863
Adjusting learning rate of group 0 to 8.0040e-05.
Epoch 22/60
Loss: 0.0894  [0/3200]
Loss: 0.4016  [800/3200]
Loss: 0.0322  [1600/3200]
Loss: 0.1729  [2400/3200]
Validation Loss: 0.0819, Accuracy: 79.62%, F1 Score: 0.7951
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 23/60
Loss: 0.0467  [0/3200]
Loss: 0.0145  [800/3200]
Loss: 0.0388  [1600/3200]
Loss: 0.0378  [2400/3200]
Validation Loss: 0.0843, Accuracy: 80.38%, F1 Score: 0.7990
Adjusting learning rate of group 0 to 8.0048e-05.
Epoch 24/60
Loss: 0.2710  [0/3200]
Loss: 0.1807  [800/3200]
Loss: 0.2329  [1600/3200]
Loss: 0.0080  [2400/3200]
Validation Loss: 0.0882, Accuracy: 76.88%, F1 Score: 0.7684
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 25/60
Loss: 0.0471  [0/3200]
Loss: 0.0926  [800/3200]
Loss: 0.0984  [1600/3200]
Loss: 0.0261  [2400/3200]
Validation Loss: 0.1018, Accuracy: 73.88%, F1 Score: 0.7345
Adjusting learning rate of group 0 to 8.0057e-05.
Epoch 26/60
Loss: 0.1023  [0/3200]
Loss: 0.0526  [800/3200]
Loss: 0.0170  [1600/3200]
Loss: 0.1369  [2400/3200]
Validation Loss: 0.0951, Accuracy: 77.38%, F1 Score: 0.7688
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 27/60
Loss: 0.0299  [0/3200]
Loss: 0.0329  [800/3200]
Loss: 0.0170  [1600/3200]
Loss: 0.0050  [2400/3200]
Validation Loss: 0.1001, Accuracy: 78.00%, F1 Score: 0.7719
Adjusting learning rate of group 0 to 8.0067e-05.
Epoch 28/60
Loss: 0.0342  [0/3200]
Loss: 0.0376  [800/3200]
Loss: 0.0369  [1600/3200]
Loss: 0.0283  [2400/3200]
Validation Loss: 0.1018, Accuracy: 77.88%, F1 Score: 0.7765
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 29/60
Loss: 0.0194  [0/3200]
Loss: 0.0019  [800/3200]
Loss: 0.0100  [1600/3200]
Loss: 0.0244  [2400/3200]
Validation Loss: 0.1049, Accuracy: 78.00%, F1 Score: 0.7750
Adjusting learning rate of group 0 to 8.0077e-05.
Epoch 30/60
Loss: 0.0386  [0/3200]
Loss: 0.0337  [800/3200]
Loss: 0.0040  [1600/3200]
Loss: 0.0081  [2400/3200]
Validation Loss: 0.0977, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 31/60
Loss: 0.0297  [0/3200]
Loss: 0.0193  [800/3200]
Loss: 0.0405  [1600/3200]
Loss: 0.0032  [2400/3200]
Validation Loss: 0.1119, Accuracy: 77.12%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0088e-05.
Epoch 32/60
Loss: 0.0198  [0/3200]
Loss: 0.0027  [800/3200]
Loss: 0.0125  [1600/3200]
Loss: 0.0059  [2400/3200]
Validation Loss: 0.1092, Accuracy: 78.38%, F1 Score: 0.7863
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 33/60
Loss: 0.0452  [0/3200]
Loss: 0.0635  [800/3200]
Loss: 0.0170  [1600/3200]
Loss: 0.0131  [2400/3200]
Validation Loss: 0.1203, Accuracy: 76.75%, F1 Score: 0.7635
Adjusting learning rate of group 0 to 8.0100e-05.
Epoch 34/60
Loss: 0.0062  [0/3200]
Loss: 0.0023  [800/3200]
Loss: 0.0331  [1600/3200]
Loss: 0.0061  [2400/3200]
Validation Loss: 0.1116, Accuracy: 79.00%, F1 Score: 0.7905
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 35/60
Loss: 0.0193  [0/3200]
Loss: 0.0105  [800/3200]
Loss: 0.0104  [1600/3200]
Loss: 0.0113  [2400/3200]
Validation Loss: 0.1306, Accuracy: 76.75%, F1 Score: 0.7654
Adjusting learning rate of group 0 to 8.0112e-05.
Epoch 36/60
Loss: 0.0005  [0/3200]
Loss: 0.0021  [800/3200]
Loss: 0.0031  [1600/3200]
Loss: 0.0002  [2400/3200]
Validation Loss: 0.1145, Accuracy: 79.00%, F1 Score: 0.7870
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 37/60
Loss: 0.0055  [0/3200]
Loss: 0.0018  [800/3200]
Loss: 0.0027  [1600/3200]
Loss: 0.0052  [2400/3200]
Validation Loss: 0.1226, Accuracy: 77.50%, F1 Score: 0.7741
Adjusting learning rate of group 0 to 8.0125e-05.
Epoch 38/60
Loss: 0.0003  [0/3200]
Loss: 0.0038  [800/3200]
Loss: 0.0073  [1600/3200]
Loss: 0.0011  [2400/3200]
Validation Loss: 0.1226, Accuracy: 78.50%, F1 Score: 0.7842
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 39/60
Loss: 0.0056  [0/3200]
Loss: 0.0039  [800/3200]
Loss: 0.0017  [1600/3200]
Loss: 0.0011  [2400/3200]
Validation Loss: 0.1274, Accuracy: 77.62%, F1 Score: 0.7731
Adjusting learning rate of group 0 to 8.0139e-05.
Epoch 40/60
Loss: 0.0002  [0/3200]
Loss: 0.0038  [800/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0052  [2400/3200]
Validation Loss: 0.1464, Accuracy: 75.88%, F1 Score: 0.7577
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 41/60
Loss: 0.0021  [0/3200]
Loss: 0.0016  [800/3200]
Loss: 0.0062  [1600/3200]
Loss: 0.0012  [2400/3200]
Validation Loss: 0.1249, Accuracy: 78.12%, F1 Score: 0.7806
Adjusting learning rate of group 0 to 8.0154e-05.
Epoch 42/60
Loss: 0.0019  [0/3200]
Loss: 0.0019  [800/3200]
Loss: 0.0058  [1600/3200]
Loss: 0.0006  [2400/3200]
Validation Loss: 0.1337, Accuracy: 77.50%, F1 Score: 0.7739
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 43/60
Loss: 0.0017  [0/3200]
Loss: 0.0038  [800/3200]
Loss: 0.0026  [1600/3200]
Loss: 0.0020  [2400/3200]
Validation Loss: 0.1309, Accuracy: 78.50%, F1 Score: 0.7844
Adjusting learning rate of group 0 to 8.0169e-05.
Epoch 44/60
Loss: 0.0004  [0/3200]
Loss: 0.0015  [800/3200]
Loss: 0.0026  [1600/3200]
Loss: 0.0075  [2400/3200]
Validation Loss: 0.1443, Accuracy: 77.50%, F1 Score: 0.7740
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 45/60
Loss: 0.0068  [0/3200]
Loss: 0.0022  [800/3200]
Loss: 0.0010  [1600/3200]
Loss: 0.0019  [2400/3200]
Validation Loss: 0.1432, Accuracy: 77.88%, F1 Score: 0.7768
Adjusting learning rate of group 0 to 8.0185e-05.
Epoch 46/60
Loss: 0.0003  [0/3200]
Loss: 0.0017  [800/3200]
Loss: 0.0022  [1600/3200]
Loss: 0.0024  [2400/3200]
Validation Loss: 0.1408, Accuracy: 78.25%, F1 Score: 0.7802
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 47/60
Loss: 0.0011  [0/3200]
Loss: 0.0034  [800/3200]
Loss: 0.0050  [1600/3200]
Loss: 0.0026  [2400/3200]
Validation Loss: 0.1461, Accuracy: 77.75%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.0202e-05.
Epoch 48/60
Loss: 0.0007  [0/3200]
Loss: 0.0022  [800/3200]
Loss: 0.0009  [1600/3200]
Loss: 0.0008  [2400/3200]
Validation Loss: 0.1321, Accuracy: 77.75%, F1 Score: 0.7768
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 49/60
Loss: 0.0005  [0/3200]
Loss: 0.0026  [800/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0020  [2400/3200]
Validation Loss: 0.1448, Accuracy: 77.75%, F1 Score: 0.7752
Adjusting learning rate of group 0 to 8.0219e-05.
Epoch 50/60
Loss: 0.0003  [0/3200]
Loss: 0.0009  [800/3200]
Loss: 0.0008  [1600/3200]
Loss: 0.0009  [2400/3200]
Validation Loss: 0.1635, Accuracy: 76.25%, F1 Score: 0.7626
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 51/60
Loss: 0.0609  [0/3200]
Loss: 0.0076  [800/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0006  [2400/3200]
Validation Loss: 0.1480, Accuracy: 78.38%, F1 Score: 0.7821
Adjusting learning rate of group 0 to 8.0238e-05.
Epoch 52/60
Loss: 0.0006  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0005  [1600/3200]
Loss: 0.0006  [2400/3200]
Validation Loss: 0.1498, Accuracy: 77.88%, F1 Score: 0.7761
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 53/60
Loss: 0.0009  [0/3200]
Loss: 0.0018  [800/3200]
Loss: 0.0000  [1600/3200]
Loss: 0.0010  [2400/3200]
Validation Loss: 0.1517, Accuracy: 77.62%, F1 Score: 0.7746
Adjusting learning rate of group 0 to 8.0257e-05.
Epoch 54/60
Loss: 0.0002  [0/3200]
Loss: 0.0010  [800/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0008  [2400/3200]
Validation Loss: 0.1537, Accuracy: 78.00%, F1 Score: 0.7764
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 55/60
Loss: 0.0003  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0009  [2400/3200]
Validation Loss: 0.1573, Accuracy: 77.50%, F1 Score: 0.7726
Adjusting learning rate of group 0 to 8.0277e-05.
Epoch 56/60
Loss: 0.0004  [0/3200]
Loss: 0.0005  [800/3200]
Loss: 0.0003  [1600/3200]
Loss: 0.0094  [2400/3200]
Validation Loss: 0.1413, Accuracy: 79.62%, F1 Score: 0.7956
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 57/60
Loss: 0.0146  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0006  [1600/3200]
Loss: 0.0009  [2400/3200]
Validation Loss: 0.1553, Accuracy: 78.25%, F1 Score: 0.7808
Adjusting learning rate of group 0 to 8.0297e-05.
Epoch 58/60
Loss: 0.0017  [0/3200]
Loss: 0.0016  [800/3200]
Loss: 0.0004  [1600/3200]
Loss: 0.0003  [2400/3200]
Validation Loss: 0.1535, Accuracy: 78.50%, F1 Score: 0.7830
Adjusting learning rate of group 0 to 8.0307e-05.
Epoch 59/60
Loss: 0.0001  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0006  [1600/3200]
Loss: 0.0005  [2400/3200]
Validation Loss: 0.1602, Accuracy: 77.88%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0318e-05.
Epoch 60/60
Loss: 0.0005  [0/3200]
Loss: 0.0002  [800/3200]
Loss: 0.0002  [1600/3200]
Loss: 0.0006  [2400/3200]
Validation Loss: 0.1630, Accuracy: 78.50%, F1 Score: 0.7818
Adjusting learning rate of group 0 to 8.0329e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 16...
Epoch 1/60
Loss: 1.8130  [0/3200]

<ipython-input-42-fa3622509717>:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Loss: 1.0082  [1600/3200]
Validation Loss: 0.0518, Accuracy: 64.12%, F1 Score: 0.6167
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 0.8653  [0/3200]
Loss: 0.8663  [1600/3200]
Validation Loss: 0.0503, Accuracy: 64.62%, F1 Score: 0.6125
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.5948  [0/3200]
Loss: 0.5598  [1600/3200]
Validation Loss: 0.0416, Accuracy: 72.38%, F1 Score: 0.7242
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.5666  [0/3200]
Loss: 0.5504  [1600/3200]
Validation Loss: 0.0410, Accuracy: 75.25%, F1 Score: 0.7583
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.5307  [0/3200]
Loss: 0.6684  [1600/3200]
Validation Loss: 0.0378, Accuracy: 76.00%, F1 Score: 0.7492
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 1.0975  [0/3200]
Loss: 0.4646  [1600/3200]
Validation Loss: 0.0378, Accuracy: 77.00%, F1 Score: 0.7683
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5269  [0/3200]
Loss: 0.5878  [1600/3200]
Validation Loss: 0.0364, Accuracy: 79.75%, F1 Score: 0.7944
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3505  [0/3200]
Loss: 0.5043  [1600/3200]
Validation Loss: 0.0362, Accuracy: 77.88%, F1 Score: 0.7816
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.4426  [0/3200]
Loss: 0.4681  [1600/3200]
Validation Loss: 0.0340, Accuracy: 80.75%, F1 Score: 0.8049
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.3952  [0/3200]
Loss: 0.4945  [1600/3200]
Validation Loss: 0.0414, Accuracy: 75.75%, F1 Score: 0.7619
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.5319  [0/3200]
Loss: 0.1808  [1600/3200]
Validation Loss: 0.0371, Accuracy: 78.25%, F1 Score: 0.7773
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.3071  [0/3200]
Loss: 0.1682  [1600/3200]
Validation Loss: 0.0384, Accuracy: 77.75%, F1 Score: 0.7699
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3599  [0/3200]
Loss: 0.3353  [1600/3200]
Validation Loss: 0.0429, Accuracy: 74.88%, F1 Score: 0.7411
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.5915  [0/3200]
Loss: 0.2790  [1600/3200]
Validation Loss: 0.0481, Accuracy: 70.12%, F1 Score: 0.7041
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.5867  [0/3200]
Loss: 0.4311  [1600/3200]
Validation Loss: 0.0342, Accuracy: 80.00%, F1 Score: 0.8003
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.3286  [0/3200]
Loss: 0.4651  [1600/3200]
Validation Loss: 0.0345, Accuracy: 80.88%, F1 Score: 0.8062
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.2257  [0/3200]
Loss: 0.2394  [1600/3200]
Validation Loss: 0.0350, Accuracy: 80.62%, F1 Score: 0.8026
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.2997  [0/3200]
Loss: 0.1942  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.88%, F1 Score: 0.7999
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.2544  [0/3200]
Loss: 0.3014  [1600/3200]
Validation Loss: 0.0374, Accuracy: 80.75%, F1 Score: 0.8072
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.3391  [0/3200]
Loss: 0.0816  [1600/3200]
Validation Loss: 0.0353, Accuracy: 80.88%, F1 Score: 0.8080
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 21/60
Loss: 0.1816  [0/3200]
Loss: 0.4445  [1600/3200]
Validation Loss: 0.0420, Accuracy: 75.88%, F1 Score: 0.7416
Adjusting learning rate of group 0 to 8.0161e-05.
Epoch 22/60
Loss: 0.3777  [0/3200]
Loss: 0.4136  [1600/3200]
Validation Loss: 0.0412, Accuracy: 78.00%, F1 Score: 0.7810
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 23/60
Loss: 0.3346  [0/3200]
Loss: 0.0474  [1600/3200]
Validation Loss: 0.0397, Accuracy: 79.38%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0193e-05.
Epoch 24/60
Loss: 0.1614  [0/3200]
Loss: 0.4020  [1600/3200]
Validation Loss: 0.0398, Accuracy: 77.75%, F1 Score: 0.7743
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 25/60
Loss: 0.0967  [0/3200]
Loss: 0.2835  [1600/3200]
Validation Loss: 0.0405, Accuracy: 76.88%, F1 Score: 0.7666
Adjusting learning rate of group 0 to 8.0229e-05.
Epoch 26/60
Loss: 0.1596  [0/3200]
Loss: 0.0518  [1600/3200]
Validation Loss: 0.0471, Accuracy: 76.00%, F1 Score: 0.7525
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 27/60
Loss: 0.1848  [0/3200]
Loss: 0.2479  [1600/3200]
Validation Loss: 0.0466, Accuracy: 75.25%, F1 Score: 0.7485
Adjusting learning rate of group 0 to 8.0267e-05.
Epoch 28/60
Loss: 0.1496  [0/3200]
Loss: 0.1010  [1600/3200]
Validation Loss: 0.0454, Accuracy: 75.75%, F1 Score: 0.7541
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 29/60
Loss: 0.1436  [0/3200]
Loss: 0.0924  [1600/3200]
Validation Loss: 0.0575, Accuracy: 74.50%, F1 Score: 0.7271
Adjusting learning rate of group 0 to 8.0308e-05.
Epoch 30/60
Loss: 0.0935  [0/3200]
Loss: 0.0777  [1600/3200]
Validation Loss: 0.0439, Accuracy: 78.00%, F1 Score: 0.7784
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 31/60
Loss: 0.0857  [0/3200]
Loss: 0.1500  [1600/3200]
Validation Loss: 0.0549, Accuracy: 74.12%, F1 Score: 0.7282
Adjusting learning rate of group 0 to 8.0351e-05.
Epoch 32/60
Loss: 0.0549  [0/3200]
Loss: 0.0247  [1600/3200]
Validation Loss: 0.0556, Accuracy: 73.62%, F1 Score: 0.7313
Adjusting learning rate of group 0 to 8.0374e-05.
Epoch 33/60
Loss: 0.2466  [0/3200]
Loss: 0.0172  [1600/3200]
Validation Loss: 0.0520, Accuracy: 75.50%, F1 Score: 0.7480
Adjusting learning rate of group 0 to 8.0398e-05.
Epoch 34/60
Loss: 0.0358  [0/3200]
Loss: 0.1309  [1600/3200]
Validation Loss: 0.0462, Accuracy: 79.75%, F1 Score: 0.7976
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 35/60
Loss: 0.0632  [0/3200]
Loss: 0.0166  [1600/3200]
Validation Loss: 0.0552, Accuracy: 75.88%, F1 Score: 0.7490
Adjusting learning rate of group 0 to 8.0448e-05.
Epoch 36/60
Loss: 0.0631  [0/3200]
Loss: 0.0138  [1600/3200]
Validation Loss: 0.0515, Accuracy: 77.62%, F1 Score: 0.7703
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 37/60
Loss: 0.0553  [0/3200]
Loss: 0.0613  [1600/3200]
Validation Loss: 0.0538, Accuracy: 77.00%, F1 Score: 0.7612
Adjusting learning rate of group 0 to 8.0501e-05.
Epoch 38/60
Loss: 0.0045  [0/3200]
Loss: 0.0156  [1600/3200]
Validation Loss: 0.0510, Accuracy: 79.62%, F1 Score: 0.7932
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 39/60
Loss: 0.0198  [0/3200]
Loss: 0.0162  [1600/3200]
Validation Loss: 0.0525, Accuracy: 79.12%, F1 Score: 0.7903
Adjusting learning rate of group 0 to 8.0556e-05.
Epoch 40/60
Loss: 0.0079  [0/3200]
Loss: 0.0038  [1600/3200]
Validation Loss: 0.0609, Accuracy: 76.38%, F1 Score: 0.7559
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 41/60
Loss: 0.0418  [0/3200]
Loss: 0.0281  [1600/3200]
Validation Loss: 0.0549, Accuracy: 79.62%, F1 Score: 0.7902
Adjusting learning rate of group 0 to 8.0615e-05.
Epoch 42/60
Loss: 0.0109  [0/3200]
Loss: 0.0350  [1600/3200]
Validation Loss: 0.0577, Accuracy: 77.38%, F1 Score: 0.7713
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 43/60
Loss: 0.0231  [0/3200]
Loss: 0.0162  [1600/3200]
Validation Loss: 0.0603, Accuracy: 77.62%, F1 Score: 0.7701
Adjusting learning rate of group 0 to 8.0676e-05.
Epoch 44/60
Loss: 0.0020  [0/3200]
Loss: 0.0055  [1600/3200]
Validation Loss: 0.0570, Accuracy: 78.62%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 45/60
Loss: 0.0098  [0/3200]
Loss: 0.0038  [1600/3200]
Validation Loss: 0.0707, Accuracy: 75.12%, F1 Score: 0.7394
Adjusting learning rate of group 0 to 8.0741e-05.
Epoch 46/60
Loss: 0.0100  [0/3200]
Loss: 0.0068  [1600/3200]
Validation Loss: 0.0579, Accuracy: 78.62%, F1 Score: 0.7842
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 47/60
Loss: 0.0101  [0/3200]
Loss: 0.0185  [1600/3200]
Validation Loss: 0.0623, Accuracy: 78.12%, F1 Score: 0.7786
Adjusting learning rate of group 0 to 8.0808e-05.
Epoch 48/60
Loss: 0.0039  [0/3200]
Loss: 0.0034  [1600/3200]
Validation Loss: 0.0636, Accuracy: 77.50%, F1 Score: 0.7726
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 49/60
Loss: 0.0122  [0/3200]
Loss: 0.0027  [1600/3200]
Validation Loss: 0.0618, Accuracy: 78.50%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0878e-05.
Epoch 50/60
Loss: 0.0045  [0/3200]
Loss: 0.0036  [1600/3200]
Validation Loss: 0.0636, Accuracy: 77.88%, F1 Score: 0.7776
Adjusting learning rate of group 0 to 8.0914e-05.
Epoch 51/60
Loss: 0.0056  [0/3200]
Loss: 0.0048  [1600/3200]
Validation Loss: 0.0639, Accuracy: 78.25%, F1 Score: 0.7807
Adjusting learning rate of group 0 to 8.0951e-05.
Epoch 52/60
Loss: 0.0054  [0/3200]
Loss: 0.0034  [1600/3200]
Validation Loss: 0.0666, Accuracy: 77.88%, F1 Score: 0.7746
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 53/60
Loss: 0.0037  [0/3200]
Loss: 0.0005  [1600/3200]
Validation Loss: 0.0763, Accuracy: 75.62%, F1 Score: 0.7509
Adjusting learning rate of group 0 to 8.1027e-05.
Epoch 54/60
Loss: 0.0025  [0/3200]
Loss: 0.0038  [1600/3200]
Validation Loss: 0.0658, Accuracy: 78.62%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.1066e-05.
Epoch 55/60
Loss: 0.0024  [0/3200]
Loss: 0.0046  [1600/3200]
Validation Loss: 0.0746, Accuracy: 76.38%, F1 Score: 0.7576
Adjusting learning rate of group 0 to 8.1106e-05.
Epoch 56/60
Loss: 0.0020  [0/3200]
Loss: 0.0059  [1600/3200]
Validation Loss: 0.0738, Accuracy: 76.38%, F1 Score: 0.7592
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 57/60
Loss: 0.0155  [0/3200]
Loss: 0.0024  [1600/3200]
Validation Loss: 0.0697, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.1188e-05.
Epoch 58/60
Loss: 0.0025  [0/3200]
Loss: 0.0007  [1600/3200]
Validation Loss: 0.0703, Accuracy: 78.62%, F1 Score: 0.7825
Adjusting learning rate of group 0 to 8.1230e-05.
Epoch 59/60
Loss: 0.0011  [0/3200]
Loss: 0.0024  [1600/3200]
Validation Loss: 0.0749, Accuracy: 76.62%, F1 Score: 0.7640
Adjusting learning rate of group 0 to 8.1273e-05.
Epoch 60/60
Loss: 0.0029  [0/3200]
Loss: 0.0011  [1600/3200]
Validation Loss: 0.0722, Accuracy: 78.38%, F1 Score: 0.7794
Adjusting learning rate of group 0 to 8.1316e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 32...
Epoch 1/60
Loss: 1.7751  [0/3200]

<ipython-input-42-fa3622509717>:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Validation Loss: 0.0271, Accuracy: 63.88%, F1 Score: 0.6267
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/60
Loss: 0.8402  [0/3200]
Validation Loss: 0.0243, Accuracy: 68.75%, F1 Score: 0.6955
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/60
Loss: 0.8014  [0/3200]
Validation Loss: 0.0230, Accuracy: 69.62%, F1 Score: 0.6572
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/60
Loss: 0.7163  [0/3200]
Validation Loss: 0.0219, Accuracy: 70.62%, F1 Score: 0.6982
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/60
Loss: 0.5089  [0/3200]
Validation Loss: 0.0202, Accuracy: 74.75%, F1 Score: 0.7300
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/60
Loss: 0.8748  [0/3200]
Validation Loss: 0.0220, Accuracy: 72.25%, F1 Score: 0.7260
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/60
Loss: 0.6307  [0/3200]
Validation Loss: 0.0189, Accuracy: 75.75%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/60
Loss: 0.3423  [0/3200]
Validation Loss: 0.0193, Accuracy: 75.75%, F1 Score: 0.7445
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/60
Loss: 0.5447  [0/3200]
Validation Loss: 0.0187, Accuracy: 78.12%, F1 Score: 0.7828
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/60
Loss: 0.4167  [0/3200]
Validation Loss: 0.0225, Accuracy: 69.12%, F1 Score: 0.6876
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/60
Loss: 0.6092  [0/3200]
Validation Loss: 0.0192, Accuracy: 74.62%, F1 Score: 0.7342
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/60
Loss: 0.4271  [0/3200]
Validation Loss: 0.0179, Accuracy: 78.38%, F1 Score: 0.7755
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/60
Loss: 0.3851  [0/3200]
Validation Loss: 0.0179, Accuracy: 78.00%, F1 Score: 0.7786
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/60
Loss: 0.4125  [0/3200]
Validation Loss: 0.0170, Accuracy: 79.00%, F1 Score: 0.7923
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/60
Loss: 0.2866  [0/3200]
Validation Loss: 0.0170, Accuracy: 79.25%, F1 Score: 0.7947
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/60
Loss: 0.3484  [0/3200]
Validation Loss: 0.0192, Accuracy: 75.00%, F1 Score: 0.7372
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/60
Loss: 0.3203  [0/3200]
Validation Loss: 0.0171, Accuracy: 79.75%, F1 Score: 0.7971
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/60
Loss: 0.3165  [0/3200]
Validation Loss: 0.0175, Accuracy: 79.50%, F1 Score: 0.7913
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/60
Loss: 0.3024  [0/3200]
Validation Loss: 0.0186, Accuracy: 76.12%, F1 Score: 0.7571
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/60
Loss: 0.2813  [0/3200]
Validation Loss: 0.0203, Accuracy: 74.38%, F1 Score: 0.7292
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/60
Loss: 0.4285  [0/3200]
Validation Loss: 0.0204, Accuracy: 74.25%, F1 Score: 0.7270
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/60
Loss: 0.6483  [0/3200]
Validation Loss: 0.0177, Accuracy: 79.00%, F1 Score: 0.7935
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/60
Loss: 0.6675  [0/3200]
Validation Loss: 0.0174, Accuracy: 80.62%, F1 Score: 0.8043
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/60
Loss: 0.2092  [0/3200]
Validation Loss: 0.0201, Accuracy: 77.12%, F1 Score: 0.7657
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/60
Loss: 0.2151  [0/3200]
Validation Loss: 0.0175, Accuracy: 79.12%, F1 Score: 0.7905
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/60
Loss: 0.2833  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.62%, F1 Score: 0.7928
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/60
Loss: 0.2969  [0/3200]
Validation Loss: 0.0183, Accuracy: 79.25%, F1 Score: 0.7906
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/60
Loss: 0.1992  [0/3200]
Validation Loss: 0.0175, Accuracy: 80.75%, F1 Score: 0.8053
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/60
Loss: 0.1792  [0/3200]
Validation Loss: 0.0178, Accuracy: 80.88%, F1 Score: 0.8068
Adjusting learning rate of group 0 to 8.1231e-05.
Epoch 30/60
Loss: 0.2321  [0/3200]
Validation Loss: 0.0186, Accuracy: 79.00%, F1 Score: 0.7833
Adjusting learning rate of group 0 to 8.1317e-05.
Epoch 31/60
Loss: 0.2099  [0/3200]
Validation Loss: 0.0201, Accuracy: 75.88%, F1 Score: 0.7508
Adjusting learning rate of group 0 to 8.1406e-05.
Epoch 32/60
Loss: 0.0966  [0/3200]
Validation Loss: 0.0191, Accuracy: 77.88%, F1 Score: 0.7773
Adjusting learning rate of group 0 to 8.1499e-05.
Epoch 33/60
Loss: 0.2281  [0/3200]
Validation Loss: 0.0187, Accuracy: 79.62%, F1 Score: 0.7979
Adjusting learning rate of group 0 to 8.1594e-05.
Epoch 34/60
Loss: 0.1075  [0/3200]
Validation Loss: 0.0195, Accuracy: 78.88%, F1 Score: 0.7900
Adjusting learning rate of group 0 to 8.1692e-05.
Epoch 35/60
Loss: 0.1547  [0/3200]
Validation Loss: 0.0192, Accuracy: 80.25%, F1 Score: 0.8002
Adjusting learning rate of group 0 to 8.1793e-05.
Epoch 36/60
Loss: 0.1023  [0/3200]
Validation Loss: 0.0218, Accuracy: 75.62%, F1 Score: 0.7460
Adjusting learning rate of group 0 to 8.1896e-05.
Epoch 37/60
Loss: 0.2520  [0/3200]
Validation Loss: 0.0203, Accuracy: 79.50%, F1 Score: 0.7958
Adjusting learning rate of group 0 to 8.2003e-05.
Epoch 38/60
Loss: 0.0865  [0/3200]
Validation Loss: 0.0213, Accuracy: 77.50%, F1 Score: 0.7605
Adjusting learning rate of group 0 to 8.2113e-05.
Epoch 39/60
Loss: 0.1268  [0/3200]
Validation Loss: 0.0221, Accuracy: 76.38%, F1 Score: 0.7596
Adjusting learning rate of group 0 to 8.2226e-05.
Epoch 40/60
Loss: 0.0718  [0/3200]
Validation Loss: 0.0260, Accuracy: 74.12%, F1 Score: 0.7323
Adjusting learning rate of group 0 to 8.2341e-05.
Epoch 41/60
Loss: 0.1842  [0/3200]
Validation Loss: 0.0199, Accuracy: 79.88%, F1 Score: 0.7977
Adjusting learning rate of group 0 to 8.2460e-05.
Epoch 42/60
Loss: 0.1257  [0/3200]
Validation Loss: 0.0227, Accuracy: 77.00%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.2581e-05.
Epoch 43/60
Loss: 0.2332  [0/3200]
Validation Loss: 0.0221, Accuracy: 76.75%, F1 Score: 0.7651
Adjusting learning rate of group 0 to 8.2705e-05.
Epoch 44/60
Loss: 0.0481  [0/3200]
Validation Loss: 0.0212, Accuracy: 80.12%, F1 Score: 0.7956
Adjusting learning rate of group 0 to 8.2833e-05.
Epoch 45/60
Loss: 0.0686  [0/3200]
Validation Loss: 0.0206, Accuracy: 79.50%, F1 Score: 0.7961
Adjusting learning rate of group 0 to 8.2963e-05.
Epoch 46/60
Loss: 0.0769  [0/3200]
Validation Loss: 0.0226, Accuracy: 77.62%, F1 Score: 0.7710
Adjusting learning rate of group 0 to 8.3096e-05.
Epoch 47/60
Loss: 0.0807  [0/3200]
Validation Loss: 0.0218, Accuracy: 79.50%, F1 Score: 0.7892
Adjusting learning rate of group 0 to 8.3232e-05.
Epoch 48/60
Loss: 0.0684  [0/3200]
Validation Loss: 0.0232, Accuracy: 77.12%, F1 Score: 0.7700
Adjusting learning rate of group 0 to 8.3371e-05.
Epoch 49/60
Loss: 0.1120  [0/3200]
Validation Loss: 0.0242, Accuracy: 78.12%, F1 Score: 0.7763
Adjusting learning rate of group 0 to 8.3512e-05.
Epoch 50/60
Loss: 0.0591  [0/3200]
Validation Loss: 0.0240, Accuracy: 76.62%, F1 Score: 0.7652
Adjusting learning rate of group 0 to 8.3657e-05.
Epoch 51/60
Loss: 0.0417  [0/3200]
Validation Loss: 0.0238, Accuracy: 79.50%, F1 Score: 0.7959
Adjusting learning rate of group 0 to 8.3805e-05.
Epoch 52/60
Loss: 0.0402  [0/3200]
Validation Loss: 0.0232, Accuracy: 79.25%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 8.3955e-05.
Epoch 53/60
Loss: 0.0220  [0/3200]
Validation Loss: 0.0298, Accuracy: 74.62%, F1 Score: 0.7288
Adjusting learning rate of group 0 to 8.4109e-05.
Epoch 54/60
Loss: 0.1175  [0/3200]
Validation Loss: 0.0252, Accuracy: 77.50%, F1 Score: 0.7718
Adjusting learning rate of group 0 to 8.4265e-05.
Epoch 55/60
Loss: 0.0151  [0/3200]
Validation Loss: 0.0304, Accuracy: 74.75%, F1 Score: 0.7350
Adjusting learning rate of group 0 to 8.4425e-05.
Epoch 56/60
Loss: 0.0404  [0/3200]
Validation Loss: 0.0240, Accuracy: 79.50%, F1 Score: 0.7931
Adjusting learning rate of group 0 to 8.4587e-05.
Epoch 57/60
Loss: 0.0295  [0/3200]
Validation Loss: 0.0245, Accuracy: 79.62%, F1 Score: 0.7934
Adjusting learning rate of group 0 to 8.4752e-05.
Epoch 58/60
Loss: 0.0420  [0/3200]
Validation Loss: 0.0263, Accuracy: 78.75%, F1 Score: 0.7799
Adjusting learning rate of group 0 to 8.4920e-05.
Epoch 59/60
Loss: 0.0647  [0/3200]
Validation Loss: 0.0278, Accuracy: 76.62%, F1 Score: 0.7677
Adjusting learning rate of group 0 to 8.5091e-05.
Epoch 60/60
Loss: 0.0410  [0/3200]
Validation Loss: 0.0253, Accuracy: 79.00%, F1 Score: 0.7877
Adjusting learning rate of group 0 to 8.5265e-05.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 64...
Epoch 1/60
Loss: 1.7810  [0/3200]

<ipython-input-42-fa3622509717>:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Validation Loss: 0.0157, Accuracy: 61.50%, F1 Score: 0.5805
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 2/60
Loss: 1.0387  [0/3200]
Validation Loss: 0.0139, Accuracy: 67.62%, F1 Score: 0.6793
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 3/60
Loss: 0.8030  [0/3200]
Validation Loss: 0.0125, Accuracy: 69.12%, F1 Score: 0.6762
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 4/60
Loss: 0.7942  [0/3200]
Validation Loss: 0.0117, Accuracy: 71.25%, F1 Score: 0.7186
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 5/60
Loss: 0.5727  [0/3200]
Validation Loss: 0.0131, Accuracy: 68.12%, F1 Score: 0.6804
Adjusting learning rate of group 0 to 8.0147e-05.
Epoch 6/60
Loss: 0.9218  [0/3200]
Validation Loss: 0.0113, Accuracy: 72.62%, F1 Score: 0.7376
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 7/60
Loss: 0.6205  [0/3200]
Validation Loss: 0.0117, Accuracy: 70.50%, F1 Score: 0.6954
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 8/60
Loss: 0.5445  [0/3200]
Validation Loss: 0.0114, Accuracy: 66.75%, F1 Score: 0.6462
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 9/60
Loss: 0.7490  [0/3200]
Validation Loss: 0.0097, Accuracy: 76.75%, F1 Score: 0.7669
Adjusting learning rate of group 0 to 8.0475e-05.
Epoch 10/60
Loss: 0.5672  [0/3200]
Validation Loss: 0.0102, Accuracy: 76.00%, F1 Score: 0.7527
Adjusting learning rate of group 0 to 8.0586e-05.
Epoch 11/60
Loss: 0.7593  [0/3200]
Validation Loss: 0.0106, Accuracy: 75.38%, F1 Score: 0.7441
Adjusting learning rate of group 0 to 8.0709e-05.
Epoch 12/60
Loss: 0.5518  [0/3200]
Validation Loss: 0.0095, Accuracy: 78.50%, F1 Score: 0.7823
Adjusting learning rate of group 0 to 8.0844e-05.
Epoch 13/60
Loss: 0.5082  [0/3200]
Validation Loss: 0.0095, Accuracy: 77.62%, F1 Score: 0.7797
Adjusting learning rate of group 0 to 8.0990e-05.
Epoch 14/60
Loss: 0.4227  [0/3200]
Validation Loss: 0.0094, Accuracy: 76.50%, F1 Score: 0.7564
Adjusting learning rate of group 0 to 8.1149e-05.
Epoch 15/60
Loss: 0.4240  [0/3200]
Validation Loss: 0.0096, Accuracy: 76.88%, F1 Score: 0.7703
Adjusting learning rate of group 0 to 8.1319e-05.
Epoch 16/60
Loss: 0.4882  [0/3200]
Validation Loss: 0.0095, Accuracy: 77.00%, F1 Score: 0.7636
Adjusting learning rate of group 0 to 8.1500e-05.
Epoch 17/60
Loss: 0.4665  [0/3200]
Validation Loss: 0.0099, Accuracy: 76.75%, F1 Score: 0.7584
Adjusting learning rate of group 0 to 8.1694e-05.
Epoch 18/60
Loss: 0.5431  [0/3200]
Validation Loss: 0.0087, Accuracy: 80.12%, F1 Score: 0.8016
Adjusting learning rate of group 0 to 8.1899e-05.
Epoch 19/60
Loss: 0.4920  [0/3200]
Validation Loss: 0.0089, Accuracy: 78.50%, F1 Score: 0.7849
Adjusting learning rate of group 0 to 8.2115e-05.
Epoch 20/60
Loss: 0.5602  [0/3200]
Validation Loss: 0.0100, Accuracy: 73.25%, F1 Score: 0.7109
Adjusting learning rate of group 0 to 8.2344e-05.
Epoch 21/60
Loss: 0.4582  [0/3200]
Validation Loss: 0.0093, Accuracy: 78.88%, F1 Score: 0.7931
Adjusting learning rate of group 0 to 8.2584e-05.
Epoch 22/60
Loss: 0.4095  [0/3200]
Validation Loss: 0.0096, Accuracy: 76.88%, F1 Score: 0.7673
Adjusting learning rate of group 0 to 8.2836e-05.
Epoch 23/60
Loss: 0.5794  [0/3200]
Validation Loss: 0.0095, Accuracy: 77.75%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.3099e-05.
Epoch 24/60
Loss: 0.3990  [0/3200]
Validation Loss: 0.0088, Accuracy: 80.12%, F1 Score: 0.8019
Adjusting learning rate of group 0 to 8.3374e-05.
Epoch 25/60
Loss: 0.3909  [0/3200]
Validation Loss: 0.0088, Accuracy: 80.00%, F1 Score: 0.7977
Adjusting learning rate of group 0 to 8.3661e-05.
Epoch 26/60
Loss: 0.3785  [0/3200]
Validation Loss: 0.0092, Accuracy: 78.50%, F1 Score: 0.7856
Adjusting learning rate of group 0 to 8.3960e-05.
Epoch 27/60
Loss: 0.4027  [0/3200]
Validation Loss: 0.0099, Accuracy: 77.88%, F1 Score: 0.7859
Adjusting learning rate of group 0 to 8.4270e-05.
Epoch 28/60
Loss: 0.4461  [0/3200]
Validation Loss: 0.0088, Accuracy: 80.00%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.4592e-05.
Epoch 29/60
Loss: 0.3452  [0/3200]
Validation Loss: 0.0089, Accuracy: 80.50%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.4925e-05.
Epoch 30/60
Loss: 0.4422  [0/3200]
Validation Loss: 0.0097, Accuracy: 76.50%, F1 Score: 0.7514
Adjusting learning rate of group 0 to 8.5271e-05.
Epoch 31/60
Loss: 0.3494  [0/3200]
Validation Loss: 0.0088, Accuracy: 80.00%, F1 Score: 0.7995
Adjusting learning rate of group 0 to 8.5628e-05.
Epoch 32/60
Loss: 0.2260  [0/3200]
Validation Loss: 0.0104, Accuracy: 74.88%, F1 Score: 0.7413
Adjusting learning rate of group 0 to 8.5996e-05.
Epoch 33/60
Loss: 0.4278  [0/3200]
Validation Loss: 0.0090, Accuracy: 80.00%, F1 Score: 0.7994
Adjusting learning rate of group 0 to 8.6376e-05.
Epoch 34/60
Loss: 0.2461  [0/3200]
Validation Loss: 0.0088, Accuracy: 81.00%, F1 Score: 0.8066
Adjusting learning rate of group 0 to 8.6768e-05.
Epoch 35/60
Loss: 0.2510  [0/3200]
Validation Loss: 0.0097, Accuracy: 78.00%, F1 Score: 0.7746
Adjusting learning rate of group 0 to 8.7172e-05.
Epoch 36/60
Loss: 0.1857  [0/3200]
Validation Loss: 0.0096, Accuracy: 79.38%, F1 Score: 0.7876
Adjusting learning rate of group 0 to 8.7587e-05.
Epoch 37/60
Loss: 0.2919  [0/3200]
Validation Loss: 0.0087, Accuracy: 80.62%, F1 Score: 0.8045
Adjusting learning rate of group 0 to 8.8013e-05.
Epoch 38/60
Loss: 0.2006  [0/3200]
Validation Loss: 0.0103, Accuracy: 76.38%, F1 Score: 0.7650
Adjusting learning rate of group 0 to 8.8452e-05.
Epoch 39/60
Loss: 0.2616  [0/3200]
Validation Loss: 0.0108, Accuracy: 74.88%, F1 Score: 0.7485
Adjusting learning rate of group 0 to 8.8902e-05.
Epoch 40/60
Loss: 0.2947  [0/3200]
Validation Loss: 0.0109, Accuracy: 73.75%, F1 Score: 0.7349
Adjusting learning rate of group 0 to 8.9363e-05.
Epoch 41/60
Loss: 0.2466  [0/3200]
Validation Loss: 0.0091, Accuracy: 80.12%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.9837e-05.
Epoch 42/60
Loss: 0.2359  [0/3200]
Validation Loss: 0.0093, Accuracy: 80.00%, F1 Score: 0.7995
Adjusting learning rate of group 0 to 9.0321e-05.
Epoch 43/60
Loss: 0.2183  [0/3200]
Validation Loss: 0.0095, Accuracy: 79.88%, F1 Score: 0.7948
Adjusting learning rate of group 0 to 9.0818e-05.
Epoch 44/60
Loss: 0.2546  [0/3200]
Validation Loss: 0.0092, Accuracy: 79.88%, F1 Score: 0.7996
Adjusting learning rate of group 0 to 9.1326e-05.
Epoch 45/60
Loss: 0.2160  [0/3200]
Validation Loss: 0.0106, Accuracy: 77.50%, F1 Score: 0.7630
Adjusting learning rate of group 0 to 9.1845e-05.
Epoch 46/60
Loss: 0.2644  [0/3200]
Validation Loss: 0.0089, Accuracy: 80.62%, F1 Score: 0.8068
Adjusting learning rate of group 0 to 9.2377e-05.
Epoch 47/60
Loss: 0.2046  [0/3200]
Validation Loss: 0.0097, Accuracy: 79.12%, F1 Score: 0.7873
Adjusting learning rate of group 0 to 9.2919e-05.
Epoch 48/60
Loss: 0.2267  [0/3200]
Validation Loss: 0.0096, Accuracy: 80.00%, F1 Score: 0.8012
Adjusting learning rate of group 0 to 9.3474e-05.
Epoch 49/60
Loss: 0.2477  [0/3200]
Validation Loss: 0.0095, Accuracy: 81.12%, F1 Score: 0.8116
Adjusting learning rate of group 0 to 9.4040e-05.
Epoch 50/60
Loss: 0.2070  [0/3200]
Validation Loss: 0.0096, Accuracy: 80.25%, F1 Score: 0.8024
Adjusting learning rate of group 0 to 9.4617e-05.
Epoch 51/60
Loss: 0.1832  [0/3200]
Validation Loss: 0.0101, Accuracy: 78.50%, F1 Score: 0.7783
Adjusting learning rate of group 0 to 9.5206e-05.
Epoch 52/60
Loss: 0.1678  [0/3200]
Validation Loss: 0.0095, Accuracy: 79.88%, F1 Score: 0.7981
Adjusting learning rate of group 0 to 9.5806e-05.
Epoch 53/60
Loss: 0.2097  [0/3200]
Validation Loss: 0.0101, Accuracy: 78.88%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 9.6418e-05.
Epoch 54/60
Loss: 0.1574  [0/3200]
Validation Loss: 0.0108, Accuracy: 77.62%, F1 Score: 0.7681
Adjusting learning rate of group 0 to 9.7042e-05.
Epoch 55/60
Loss: 0.0879  [0/3200]
Validation Loss: 0.0104, Accuracy: 79.00%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 9.7677e-05.
Epoch 56/60
Loss: 0.0952  [0/3200]
Validation Loss: 0.0099, Accuracy: 80.88%, F1 Score: 0.8079
Adjusting learning rate of group 0 to 9.8324e-05.
Epoch 57/60
Loss: 0.1207  [0/3200]
Validation Loss: 0.0109, Accuracy: 78.88%, F1 Score: 0.7858
Adjusting learning rate of group 0 to 9.8982e-05.
Epoch 58/60
Loss: 0.1720  [0/3200]
Validation Loss: 0.0111, Accuracy: 76.50%, F1 Score: 0.7535
Adjusting learning rate of group 0 to 9.9651e-05.
Epoch 59/60
Loss: 0.2229  [0/3200]
Validation Loss: 0.0130, Accuracy: 75.12%, F1 Score: 0.7548
Adjusting learning rate of group 0 to 1.0033e-04.
Epoch 60/60
Loss: 0.1747  [0/3200]
Validation Loss: 0.0119, Accuracy: 77.62%, F1 Score: 0.7734
Adjusting learning rate of group 0 to 1.0102e-04.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 128...
Epoch 1/60
Loss: 1.7274  [0/3200]

<ipython-input-42-fa3622509717>:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,

Validation Loss: 0.0103, Accuracy: 56.62%, F1 Score: 0.4862
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 2/60
Loss: 1.1773  [0/3200]
Validation Loss: 0.0084, Accuracy: 62.25%, F1 Score: 0.5670
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 3/60
Loss: 1.0063  [0/3200]
Validation Loss: 0.0078, Accuracy: 65.88%, F1 Score: 0.6599
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 4/60
Loss: 0.8655  [0/3200]
Validation Loss: 0.0070, Accuracy: 67.50%, F1 Score: 0.6774
Adjusting learning rate of group 0 to 8.0376e-05.
Epoch 5/60
Loss: 0.7299  [0/3200]
Validation Loss: 0.0067, Accuracy: 68.25%, F1 Score: 0.6795
Adjusting learning rate of group 0 to 8.0587e-05.
Epoch 6/60
Loss: 0.8525  [0/3200]
Validation Loss: 0.0067, Accuracy: 68.25%, F1 Score: 0.6720
Adjusting learning rate of group 0 to 8.0846e-05.
Epoch 7/60
Loss: 0.7002  [0/3200]
Validation Loss: 0.0073, Accuracy: 65.00%, F1 Score: 0.6333
Adjusting learning rate of group 0 to 8.1151e-05.
Epoch 8/60
Loss: 0.6875  [0/3200]
Validation Loss: 0.0063, Accuracy: 71.00%, F1 Score: 0.7149
Adjusting learning rate of group 0 to 8.1504e-05.
Epoch 9/60
Loss: 0.6178  [0/3200]
Validation Loss: 0.0061, Accuracy: 73.88%, F1 Score: 0.7453
Adjusting learning rate of group 0 to 8.1903e-05.
Epoch 10/60
Loss: 0.6406  [0/3200]
Validation Loss: 0.0060, Accuracy: 74.38%, F1 Score: 0.7501
Adjusting learning rate of group 0 to 8.2349e-05.
Epoch 11/60
Loss: 0.7543  [0/3200]
Validation Loss: 0.0064, Accuracy: 69.88%, F1 Score: 0.7006
Adjusting learning rate of group 0 to 8.2842e-05.
Epoch 12/60
Loss: 0.5908  [0/3200]
Validation Loss: 0.0059, Accuracy: 71.25%, F1 Score: 0.6999
Adjusting learning rate of group 0 to 8.3382e-05.
Epoch 13/60
Loss: 0.5978  [0/3200]
Validation Loss: 0.0056, Accuracy: 73.62%, F1 Score: 0.7435
Adjusting learning rate of group 0 to 8.3969e-05.
Epoch 14/60
Loss: 0.5043  [0/3200]
Validation Loss: 0.0055, Accuracy: 74.38%, F1 Score: 0.7383
Adjusting learning rate of group 0 to 8.4602e-05.
Epoch 15/60
Loss: 0.4751  [0/3200]
Validation Loss: 0.0058, Accuracy: 73.50%, F1 Score: 0.7286
Adjusting learning rate of group 0 to 8.5282e-05.
Epoch 16/60
Loss: 0.6142  [0/3200]
Validation Loss: 0.0053, Accuracy: 76.50%, F1 Score: 0.7597
Adjusting learning rate of group 0 to 8.6009e-05.
Epoch 17/60
Loss: 0.5788  [0/3200]
Validation Loss: 0.0057, Accuracy: 75.00%, F1 Score: 0.7498
Adjusting learning rate of group 0 to 8.6783e-05.
Epoch 18/60
Loss: 0.5527  [0/3200]
Validation Loss: 0.0051, Accuracy: 75.38%, F1 Score: 0.7501
Adjusting learning rate of group 0 to 8.7604e-05.
Epoch 19/60
Loss: 0.5530  [0/3200]
Validation Loss: 0.0050, Accuracy: 76.38%, F1 Score: 0.7554
Adjusting learning rate of group 0 to 8.8471e-05.
Epoch 20/60
Loss: 0.5719  [0/3200]
Validation Loss: 0.0055, Accuracy: 73.50%, F1 Score: 0.7044
Adjusting learning rate of group 0 to 8.9384e-05.
Epoch 21/60
Loss: 0.5755  [0/3200]
Validation Loss: 0.0052, Accuracy: 78.38%, F1 Score: 0.7846
Adjusting learning rate of group 0 to 9.0344e-05.
Epoch 22/60
Loss: 0.5643  [0/3200]
Validation Loss: 0.0051, Accuracy: 79.00%, F1 Score: 0.7941
Adjusting learning rate of group 0 to 9.1351e-05.
Epoch 23/60
Loss: 0.6406  [0/3200]
Validation Loss: 0.0049, Accuracy: 78.00%, F1 Score: 0.7779
Adjusting learning rate of group 0 to 9.2404e-05.
Epoch 24/60
Loss: 0.4986  [0/3200]
Validation Loss: 0.0051, Accuracy: 78.38%, F1 Score: 0.7861
Adjusting learning rate of group 0 to 9.3504e-05.
Epoch 25/60
Loss: 0.4578  [0/3200]
Validation Loss: 0.0050, Accuracy: 77.38%, F1 Score: 0.7704
Adjusting learning rate of group 0 to 9.4649e-05.
Epoch 26/60
Loss: 0.5006  [0/3200]
Validation Loss: 0.0048, Accuracy: 78.50%, F1 Score: 0.7855
Adjusting learning rate of group 0 to 9.5842e-05.
Epoch 27/60
Loss: 0.4342  [0/3200]
Validation Loss: 0.0050, Accuracy: 79.38%, F1 Score: 0.7952
Adjusting learning rate of group 0 to 9.7080e-05.
Epoch 28/60
Loss: 0.4280  [0/3200]
Validation Loss: 0.0050, Accuracy: 80.50%, F1 Score: 0.8047
Adjusting learning rate of group 0 to 9.8364e-05.
Epoch 29/60
Loss: 0.5267  [0/3200]
Validation Loss: 0.0054, Accuracy: 75.25%, F1 Score: 0.7331
Adjusting learning rate of group 0 to 9.9695e-05.
Epoch 30/60
Loss: 0.5252  [0/3200]
Validation Loss: 0.0054, Accuracy: 79.75%, F1 Score: 0.7969
Adjusting learning rate of group 0 to 1.0107e-04.
Epoch 31/60
Loss: 0.5193  [0/3200]
Validation Loss: 0.0051, Accuracy: 79.12%, F1 Score: 0.7918
Adjusting learning rate of group 0 to 1.0249e-04.
Epoch 32/60
Loss: 0.3661  [0/3200]
Validation Loss: 0.0053, Accuracy: 76.25%, F1 Score: 0.7660
Adjusting learning rate of group 0 to 1.0396e-04.
Epoch 33/60
Loss: 0.4373  [0/3200]
Validation Loss: 0.0050, Accuracy: 79.88%, F1 Score: 0.7988
Adjusting learning rate of group 0 to 1.0548e-04.
Epoch 34/60
Loss: 0.4134  [0/3200]
Validation Loss: 0.0046, Accuracy: 79.25%, F1 Score: 0.7907
Adjusting learning rate of group 0 to 1.0704e-04.
Epoch 35/60
Loss: 0.4168  [0/3200]
Validation Loss: 0.0048, Accuracy: 79.50%, F1 Score: 0.7964
Adjusting learning rate of group 0 to 1.0864e-04.
Epoch 36/60
Loss: 0.3893  [0/3200]
Validation Loss: 0.0053, Accuracy: 76.12%, F1 Score: 0.7623
Adjusting learning rate of group 0 to 1.1029e-04.
Epoch 37/60
Loss: 0.4399  [0/3200]
Validation Loss: 0.0050, Accuracy: 78.25%, F1 Score: 0.7826
Adjusting learning rate of group 0 to 1.1199e-04.
Epoch 38/60
Loss: 0.3001  [0/3200]
Validation Loss: 0.0053, Accuracy: 78.12%, F1 Score: 0.7845
Adjusting learning rate of group 0 to 1.1373e-04.
Epoch 39/60
Loss: 0.4482  [0/3200]
Validation Loss: 0.0048, Accuracy: 79.25%, F1 Score: 0.7890
Adjusting learning rate of group 0 to 1.1552e-04.
Epoch 40/60
Loss: 0.3616  [0/3200]
Validation Loss: 0.0057, Accuracy: 74.12%, F1 Score: 0.7160
Adjusting learning rate of group 0 to 1.1735e-04.
Epoch 41/60
Loss: 0.4439  [0/3200]
Validation Loss: 0.0057, Accuracy: 76.38%, F1 Score: 0.7564
Adjusting learning rate of group 0 to 1.1923e-04.
Epoch 42/60
Loss: 0.3380  [0/3200]
Validation Loss: 0.0060, Accuracy: 74.00%, F1 Score: 0.7443
Adjusting learning rate of group 0 to 1.2115e-04.
Epoch 43/60
Loss: 0.5015  [0/3200]
Validation Loss: 0.0051, Accuracy: 77.12%, F1 Score: 0.7613
Adjusting learning rate of group 0 to 1.2312e-04.
Epoch 44/60
Loss: 0.3639  [0/3200]
Validation Loss: 0.0051, Accuracy: 77.88%, F1 Score: 0.7738
Adjusting learning rate of group 0 to 1.2514e-04.
Epoch 45/60
Loss: 0.3349  [0/3200]
Validation Loss: 0.0055, Accuracy: 79.12%, F1 Score: 0.7907
Adjusting learning rate of group 0 to 1.2719e-04.
Epoch 46/60
Loss: 0.4920  [0/3200]
Validation Loss: 0.0053, Accuracy: 77.50%, F1 Score: 0.7741
Adjusting learning rate of group 0 to 1.2930e-04.
Epoch 47/60
Loss: 0.5600  [0/3200]
Validation Loss: 0.0050, Accuracy: 80.50%, F1 Score: 0.8023
Adjusting learning rate of group 0 to 1.3144e-04.
Epoch 48/60
Loss: 0.3300  [0/3200]
Validation Loss: 0.0053, Accuracy: 75.88%, F1 Score: 0.7499
Adjusting learning rate of group 0 to 1.3363e-04.
Epoch 49/60
Loss: 0.3979  [0/3200]
Validation Loss: 0.0051, Accuracy: 79.62%, F1 Score: 0.7996
Adjusting learning rate of group 0 to 1.3587e-04.
Epoch 50/60
Loss: 0.4578  [0/3200]
Validation Loss: 0.0068, Accuracy: 70.25%, F1 Score: 0.6692
Adjusting learning rate of group 0 to 1.3815e-04.
Epoch 51/60
Loss: 0.3794  [0/3200]
Validation Loss: 0.0047, Accuracy: 81.75%, F1 Score: 0.8152
Adjusting learning rate of group 0 to 1.4047e-04.
Epoch 52/60
Loss: 0.3853  [0/3200]
Validation Loss: 0.0050, Accuracy: 79.00%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 1.4284e-04.
Epoch 53/60
Loss: 0.3577  [0/3200]
Validation Loss: 0.0059, Accuracy: 74.62%, F1 Score: 0.7409
Adjusting learning rate of group 0 to 1.4526e-04.
Epoch 54/60
Loss: 0.3014  [0/3200]
Validation Loss: 0.0049, Accuracy: 80.62%, F1 Score: 0.8070
Adjusting learning rate of group 0 to 1.4771e-04.
Epoch 55/60
Loss: 0.2272  [0/3200]
Validation Loss: 0.0052, Accuracy: 79.62%, F1 Score: 0.7910
Adjusting learning rate of group 0 to 1.5021e-04.
Epoch 56/60
Loss: 0.2038  [0/3200]
Validation Loss: 0.0049, Accuracy: 81.62%, F1 Score: 0.8158
Adjusting learning rate of group 0 to 1.5275e-04.
Epoch 57/60
Loss: 0.2200  [0/3200]
Validation Loss: 0.0064, Accuracy: 73.25%, F1 Score: 0.7285
Adjusting learning rate of group 0 to 1.5534e-04.
Epoch 58/60
Loss: 0.4051  [0/3200]
Validation Loss: 0.0051, Accuracy: 77.00%, F1 Score: 0.7613
Adjusting learning rate of group 0 to 1.5797e-04.
Epoch 59/60
Loss: 0.2819  [0/3200]
Validation Loss: 0.0052, Accuracy: 80.38%, F1 Score: 0.7998
Adjusting learning rate of group 0 to 1.6064e-04.
Epoch 60/60
Loss: 0.1721  [0/3200]
Validation Loss: 0.0052, Accuracy: 79.62%, F1 Score: 0.7973
Adjusting learning rate of group 0 to 1.6336e-04.

<ipython-input-42-fa3622509717>:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Batch Size': batch_size, 'Best Accuracy': best_accuracy,


print(results_df)
   Batch Size  Best Accuracy  Best F1 Score    Time (s)
0         2.0         81.625       0.811550  408.195627
1         4.0         80.250       0.801457  205.904454
2         8.0         81.000       0.808763  107.725860
3        16.0         81.625       0.818176   56.277556
4        32.0         82.125       0.820125   30.184809
5        64.0         81.625       0.816271   17.056365
6       128.0         80.875       0.806896    9.666589

Το batch size διαδραματίζει καθοριστικό ρόλο στο χρόνο εκπαίδευσης και ελέγχου του μοντέλου καθώς βλέπουμε τεράστιες ποσοστιαίες διαφορές, χωρίς αυτό ωστόσο να προσφέρει κάτι στην απόδοση ή στην f1 μετρική για τα συγκεκριμένα dataset Που χρησιμοποιούμε καθώς μπορούμε να επιτύχουμε υψηλότερα ποσοστά σε γρηγορότερους χρόνους με μεγάλύτερο ρυθμό εκπαίδευσης πχ με batch =32 (και για τα δύο μοντέλα)

Early stopping

Model CNN_pp_re for Adamax Optimizer, OneCycleLR LR scheduler, Mish activation function, Batch Normalization, Regularization με dropout στα linear=0.5 χωρίς weight decay και batch size=16
import time

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define the list of batch sizes
batch_sizes = [16]  # Only using batch size 16
patience_values = [3, 5, 7, 9]  # Different patience values to try

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=[ 'Patience', 'Epoch at Early Stopping', 'Early Stopping Accuracy', 'Early Stopping F1 Score', 'Best Accuracy', 'Best F1 Score', 'Time (s)'])

# Iterate over batch sizes and patience values
for batch_size in batch_sizes:
    for patience in patience_values:
        # Set the random seeds for reproducibility within the loop
        torch.manual_seed(SEED)
        random.seed(SEED)
        np.random.seed(SEED)
        torch.cuda.manual_seed_all(SEED)

        # Define the data and loaders with the current batch size
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

        # Initialize the model
        model = CNN_pp_re(output_dim=4, dropout_rate=0.5).to(device)
        model.apply(weights_init)  # Initialize the weights

        optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0)

        # Create the scheduler based on the scheduler name
        if scheduler_name == 'OneCycleLR':
            scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader), verbose=True)

        # Early stopping parameters
        early_stop_counter = 0
        best_accuracy = 0.0
        best_f1 = 0.0
        early_stopping_epoch = None
        early_stopping_accuracy = None
        early_stopping_f1 = None

        # Train the model
        print(f"Training with batch size {batch_size} and patience {patience}...")
        start_time = time.time()
        for epoch in range(epochs):
            print(f"Epoch {epoch+1}/{epochs}")
            train_loop(train_loader, model, loss_fn, optimizer)
            test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
            print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

            # Check if the accuracy has improved
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_f1 = f1
                early_stop_counter = 0  # Reset the counter
            else:
                early_stop_counter += 1

            # Check if early stopping condition is met
            if early_stop_counter >= patience:
                print(f"Early stopping triggered! No improvement in accuracy for {patience} epochs.")
                early_stopping_epoch = epoch + 1
                early_stopping_accuracy = accuracy
                early_stopping_f1 = f1
                break

            scheduler.step()  # Update the learning rate

        end_time = time.time()
        elapsed_time = end_time - start_time

        # Add the results to the DataFrame
        results_df = results_df.append({'Patience': patience,
                                        'Epoch at Early Stopping': early_stopping_epoch,
                                        'Early Stopping Accuracy': early_stopping_accuracy,
                                        'Early Stopping F1 Score': early_stopping_f1,
                                        'Best Accuracy': best_accuracy,
                                        'Best F1 Score': best_f1,
                                        'Time (s)': elapsed_time}, ignore_index=True)

Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 16 and patience 3...
Epoch 1/60
Loss: 1.4068  [0/3200]
Loss: 1.0340  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.38%, F1 Score: 0.6587
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0309  [0/3200]
Loss: 0.9789  [1600/3200]
Validation Loss: 0.0446, Accuracy: 69.38%, F1 Score: 0.6808
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7666  [0/3200]
Loss: 0.5071  [1600/3200]
Validation Loss: 0.0384, Accuracy: 76.00%, F1 Score: 0.7459
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.3333  [0/3200]
Loss: 0.2567  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8734  [0/3200]
Loss: 0.6187  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7106  [0/3200]
Loss: 0.3061  [1600/3200]
Validation Loss: 0.0371, Accuracy: 76.62%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5991  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7522
Early stopping triggered! No improvement in accuracy for 3 epochs.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 16 and patience 5...
Epoch 1/60
Loss: 1.4068  [0/3200]

<ipython-input-43-682125bcfd2c>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

Loss: 1.0340  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.38%, F1 Score: 0.6587
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0309  [0/3200]
Loss: 0.9789  [1600/3200]
Validation Loss: 0.0446, Accuracy: 69.38%, F1 Score: 0.6808
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7666  [0/3200]
Loss: 0.5071  [1600/3200]
Validation Loss: 0.0384, Accuracy: 76.00%, F1 Score: 0.7459
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.3333  [0/3200]
Loss: 0.2567  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8734  [0/3200]
Loss: 0.6187  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7106  [0/3200]
Loss: 0.3061  [1600/3200]
Validation Loss: 0.0371, Accuracy: 76.62%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5991  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7522
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3038  [0/3200]
Loss: 0.3593  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.12%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.5215  [0/3200]
Loss: 0.3155  [1600/3200]
Validation Loss: 0.0339, Accuracy: 80.00%, F1 Score: 0.7938
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2824  [0/3200]
Loss: 0.2885  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.3508  [0/3200]
Loss: 0.1472  [1600/3200]
Validation Loss: 0.0332, Accuracy: 80.50%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2190  [0/3200]
Loss: 0.2166  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.62%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3937  [0/3200]
Loss: 0.1673  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.12%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.2727  [0/3200]
Loss: 0.2061  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.3493  [0/3200]
Loss: 0.3084  [1600/3200]
Validation Loss: 0.0353, Accuracy: 79.12%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.2409  [0/3200]
Loss: 0.3674  [1600/3200]
Validation Loss: 0.0375, Accuracy: 78.88%, F1 Score: 0.7853
Early stopping triggered! No improvement in accuracy for 5 epochs.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 16 and patience 7...
Epoch 1/60
Loss: 1.4068  [0/3200]

<ipython-input-43-682125bcfd2c>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

Loss: 1.0340  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.38%, F1 Score: 0.6587
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0309  [0/3200]
Loss: 0.9789  [1600/3200]
Validation Loss: 0.0446, Accuracy: 69.38%, F1 Score: 0.6808
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7666  [0/3200]
Loss: 0.5071  [1600/3200]
Validation Loss: 0.0384, Accuracy: 76.00%, F1 Score: 0.7459
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.3333  [0/3200]
Loss: 0.2567  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8734  [0/3200]
Loss: 0.6187  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7106  [0/3200]
Loss: 0.3061  [1600/3200]
Validation Loss: 0.0371, Accuracy: 76.62%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5991  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7522
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3038  [0/3200]
Loss: 0.3593  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.12%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.5215  [0/3200]
Loss: 0.3155  [1600/3200]
Validation Loss: 0.0339, Accuracy: 80.00%, F1 Score: 0.7938
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2824  [0/3200]
Loss: 0.2885  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.3508  [0/3200]
Loss: 0.1472  [1600/3200]
Validation Loss: 0.0332, Accuracy: 80.50%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2190  [0/3200]
Loss: 0.2166  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.62%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3937  [0/3200]
Loss: 0.1673  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.12%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.2727  [0/3200]
Loss: 0.2061  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.3493  [0/3200]
Loss: 0.3084  [1600/3200]
Validation Loss: 0.0353, Accuracy: 79.12%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.2409  [0/3200]
Loss: 0.3674  [1600/3200]
Validation Loss: 0.0375, Accuracy: 78.88%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.1645  [0/3200]
Loss: 0.1782  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.75%, F1 Score: 0.7967
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.1921  [0/3200]
Loss: 0.1559  [1600/3200]
Validation Loss: 0.0377, Accuracy: 78.62%, F1 Score: 0.7821
Early stopping triggered! No improvement in accuracy for 7 epochs.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 16 and patience 9...
Epoch 1/60
Loss: 1.4068  [0/3200]

<ipython-input-43-682125bcfd2c>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

Loss: 1.0340  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.38%, F1 Score: 0.6587
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0309  [0/3200]
Loss: 0.9789  [1600/3200]
Validation Loss: 0.0446, Accuracy: 69.38%, F1 Score: 0.6808
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7666  [0/3200]
Loss: 0.5071  [1600/3200]
Validation Loss: 0.0384, Accuracy: 76.00%, F1 Score: 0.7459
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.3333  [0/3200]
Loss: 0.2567  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8734  [0/3200]
Loss: 0.6187  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7106  [0/3200]
Loss: 0.3061  [1600/3200]
Validation Loss: 0.0371, Accuracy: 76.62%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5991  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7522
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3038  [0/3200]
Loss: 0.3593  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.12%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.5215  [0/3200]
Loss: 0.3155  [1600/3200]
Validation Loss: 0.0339, Accuracy: 80.00%, F1 Score: 0.7938
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2824  [0/3200]
Loss: 0.2885  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.3508  [0/3200]
Loss: 0.1472  [1600/3200]
Validation Loss: 0.0332, Accuracy: 80.50%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2190  [0/3200]
Loss: 0.2166  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.62%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3937  [0/3200]
Loss: 0.1673  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.12%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.2727  [0/3200]
Loss: 0.2061  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.3493  [0/3200]
Loss: 0.3084  [1600/3200]
Validation Loss: 0.0353, Accuracy: 79.12%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.2409  [0/3200]
Loss: 0.3674  [1600/3200]
Validation Loss: 0.0375, Accuracy: 78.88%, F1 Score: 0.7853
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 17/60
Loss: 0.1645  [0/3200]
Loss: 0.1782  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.75%, F1 Score: 0.7967
Adjusting learning rate of group 0 to 8.0106e-05.
Epoch 18/60
Loss: 0.1921  [0/3200]
Loss: 0.1559  [1600/3200]
Validation Loss: 0.0377, Accuracy: 78.62%, F1 Score: 0.7821
Adjusting learning rate of group 0 to 8.0118e-05.
Epoch 19/60
Loss: 0.1434  [0/3200]
Loss: 0.3144  [1600/3200]
Validation Loss: 0.0373, Accuracy: 79.00%, F1 Score: 0.7884
Adjusting learning rate of group 0 to 8.0132e-05.
Epoch 20/60
Loss: 0.0890  [0/3200]
Loss: 0.0579  [1600/3200]
Validation Loss: 0.0399, Accuracy: 78.62%, F1 Score: 0.7871
Early stopping triggered! No improvement in accuracy for 9 epochs.

<ipython-input-43-682125bcfd2c>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

results_df

Model CNN_pp for Adamax Optimizer, OneCycleLR LR scheduler, Mish activation function, without Batch Normalization, without Regularization με default dropout στα linear=0.5 χωρίς weight decay και batch size=32
import time

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define the list of batch sizes
batch_sizes = [32]  # Only using batch size 16
patience_values = [3, 5, 7, 9]  # Different patience values to try

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=['Patience', 'Epoch at Early Stopping', 'Early Stopping Accuracy', 'Early Stopping F1 Score', 'Best Accuracy', 'Best F1 Score', 'Time (s)'])

# Iterate over batch sizes and patience values
for batch_size in batch_sizes:
    for patience in patience_values:
        # Set the random seeds for reproducibility within the loop
        torch.manual_seed(SEED)
        random.seed(SEED)
        np.random.seed(SEED)
        torch.cuda.manual_seed_all(SEED)

        # Define the data and loaders with the current batch size
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

        # Initialize the model
        model = CNN_pp2(output_dim=4, activation=nn.Mish(), dropout_rate=0.5).to(device)
        model.apply(weights_init)  # Initialize the weights

        optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0)

        # Create the scheduler based on the scheduler name
        if scheduler_name == 'OneCycleLR':
            scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader), verbose=True)

        # Early stopping parameters
        early_stop_counter = 0
        best_accuracy = 0.0
        best_f1 = 0.0
        early_stopping_epoch = None
        early_stopping_accuracy = None
        early_stopping_f1 = None

        # Train the model
        print(f"Training with batch size {batch_size} and patience {patience}...")
        start_time = time.time()
        for epoch in range(epochs):
            print(f"Epoch {epoch+1}/{epochs}")
            train_loop(train_loader, model, loss_fn, optimizer)
            test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
            print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

            # Check if the accuracy has improved
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_f1 = f1
                early_stop_counter = 0  # Reset the counter
            else:
                early_stop_counter += 1

            # Check if early stopping condition is met
            if early_stop_counter >= patience:
                print(f"Early stopping triggered! No improvement in accuracy for {patience} epochs.")
                early_stopping_epoch = epoch + 1
                early_stopping_accuracy = accuracy
                early_stopping_f1 = f1
                break

            scheduler.step()  # Update the learning rate

        end_time = time.time()
        elapsed_time = end_time - start_time

        # Add the results to the DataFrame
        results_df = results_df.append({'Patience': patience,
                                        'Epoch at Early Stopping': early_stopping_epoch,
                                        'Early Stopping Accuracy': early_stopping_accuracy,
                                        'Early Stopping F1 Score': early_stopping_f1,
                                        'Best Accuracy': best_accuracy,
                                        'Best F1 Score': best_f1,
                                        'Time (s)': elapsed_time}, ignore_index=True)

Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 32 and patience 3...
Epoch 1/60
Loss: 2.9772  [0/3200]
Validation Loss: 0.0428, Accuracy: 25.87%, F1 Score: 0.1176
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/60
Loss: 1.3745  [0/3200]
Validation Loss: 0.0399, Accuracy: 54.87%, F1 Score: 0.4732
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/60
Loss: 1.5195  [0/3200]
Validation Loss: 0.0358, Accuracy: 59.00%, F1 Score: 0.5253
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/60
Loss: 1.2554  [0/3200]
Validation Loss: 0.0306, Accuracy: 67.50%, F1 Score: 0.6712
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/60
Loss: 1.0440  [0/3200]
Validation Loss: 0.0270, Accuracy: 66.25%, F1 Score: 0.6559
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/60
Loss: 1.1116  [0/3200]
Validation Loss: 0.0250, Accuracy: 70.75%, F1 Score: 0.7082
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/60
Loss: 0.9562  [0/3200]
Validation Loss: 0.0252, Accuracy: 65.50%, F1 Score: 0.6146
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/60
Loss: 0.8322  [0/3200]
Validation Loss: 0.0223, Accuracy: 70.00%, F1 Score: 0.6750
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/60
Loss: 0.9166  [0/3200]
Validation Loss: 0.0212, Accuracy: 75.50%, F1 Score: 0.7453
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/60
Loss: 0.7715  [0/3200]
Validation Loss: 0.0211, Accuracy: 72.25%, F1 Score: 0.6980
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/60
Loss: 0.9843  [0/3200]
Validation Loss: 0.0198, Accuracy: 76.62%, F1 Score: 0.7564
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/60
Loss: 0.6400  [0/3200]
Validation Loss: 0.0191, Accuracy: 78.50%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/60
Loss: 0.7543  [0/3200]
Validation Loss: 0.0187, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/60
Loss: 0.5416  [0/3200]
Validation Loss: 0.0182, Accuracy: 77.88%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/60
Loss: 0.6312  [0/3200]
Validation Loss: 0.0177, Accuracy: 78.75%, F1 Score: 0.7857
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/60
Loss: 0.6728  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.00%, F1 Score: 0.7634
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/60
Loss: 0.4580  [0/3200]
Validation Loss: 0.0184, Accuracy: 75.62%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/60
Loss: 0.4547  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.25%, F1 Score: 0.7934
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/60
Loss: 0.6547  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.88%, F1 Score: 0.7720
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/60
Loss: 0.6676  [0/3200]
Validation Loss: 0.0186, Accuracy: 74.50%, F1 Score: 0.7229
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/60
Loss: 0.7095  [0/3200]
Validation Loss: 0.0172, Accuracy: 80.00%, F1 Score: 0.7984
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/60
Loss: 0.7561  [0/3200]
Validation Loss: 0.0178, Accuracy: 78.50%, F1 Score: 0.7820
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/60
Loss: 0.7263  [0/3200]
Validation Loss: 0.0184, Accuracy: 77.00%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/60
Loss: 0.5672  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.12%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/60
Loss: 0.5990  [0/3200]
Validation Loss: 0.0172, Accuracy: 78.75%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/60
Loss: 0.6398  [0/3200]
Validation Loss: 0.0175, Accuracy: 78.25%, F1 Score: 0.7754
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/60
Loss: 0.6231  [0/3200]
Validation Loss: 0.0163, Accuracy: 79.88%, F1 Score: 0.7985
Early stopping triggered! No improvement in accuracy for 3 epochs.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 32 and patience 5...
Epoch 1/60
Loss: 2.9772  [0/3200]

<ipython-input-45-84119924d6e8>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

Validation Loss: 0.0428, Accuracy: 25.87%, F1 Score: 0.1176
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/60
Loss: 1.3745  [0/3200]
Validation Loss: 0.0399, Accuracy: 54.87%, F1 Score: 0.4732
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/60
Loss: 1.5195  [0/3200]
Validation Loss: 0.0358, Accuracy: 59.00%, F1 Score: 0.5253
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/60
Loss: 1.2554  [0/3200]
Validation Loss: 0.0306, Accuracy: 67.50%, F1 Score: 0.6712
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/60
Loss: 1.0440  [0/3200]
Validation Loss: 0.0270, Accuracy: 66.25%, F1 Score: 0.6559
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/60
Loss: 1.1116  [0/3200]
Validation Loss: 0.0250, Accuracy: 70.75%, F1 Score: 0.7082
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/60
Loss: 0.9562  [0/3200]
Validation Loss: 0.0252, Accuracy: 65.50%, F1 Score: 0.6146
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/60
Loss: 0.8322  [0/3200]
Validation Loss: 0.0223, Accuracy: 70.00%, F1 Score: 0.6750
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/60
Loss: 0.9166  [0/3200]
Validation Loss: 0.0212, Accuracy: 75.50%, F1 Score: 0.7453
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/60
Loss: 0.7715  [0/3200]
Validation Loss: 0.0211, Accuracy: 72.25%, F1 Score: 0.6980
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/60
Loss: 0.9843  [0/3200]
Validation Loss: 0.0198, Accuracy: 76.62%, F1 Score: 0.7564
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/60
Loss: 0.6400  [0/3200]
Validation Loss: 0.0191, Accuracy: 78.50%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/60
Loss: 0.7543  [0/3200]
Validation Loss: 0.0187, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/60
Loss: 0.5416  [0/3200]
Validation Loss: 0.0182, Accuracy: 77.88%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/60
Loss: 0.6312  [0/3200]
Validation Loss: 0.0177, Accuracy: 78.75%, F1 Score: 0.7857
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/60
Loss: 0.6728  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.00%, F1 Score: 0.7634
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/60
Loss: 0.4580  [0/3200]
Validation Loss: 0.0184, Accuracy: 75.62%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/60
Loss: 0.4547  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.25%, F1 Score: 0.7934
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/60
Loss: 0.6547  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.88%, F1 Score: 0.7720
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/60
Loss: 0.6676  [0/3200]
Validation Loss: 0.0186, Accuracy: 74.50%, F1 Score: 0.7229
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/60
Loss: 0.7095  [0/3200]
Validation Loss: 0.0172, Accuracy: 80.00%, F1 Score: 0.7984
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/60
Loss: 0.7561  [0/3200]
Validation Loss: 0.0178, Accuracy: 78.50%, F1 Score: 0.7820
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/60
Loss: 0.7263  [0/3200]
Validation Loss: 0.0184, Accuracy: 77.00%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/60
Loss: 0.5672  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.12%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/60
Loss: 0.5990  [0/3200]
Validation Loss: 0.0172, Accuracy: 78.75%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/60
Loss: 0.6398  [0/3200]
Validation Loss: 0.0175, Accuracy: 78.25%, F1 Score: 0.7754
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/60
Loss: 0.6231  [0/3200]
Validation Loss: 0.0163, Accuracy: 79.88%, F1 Score: 0.7985
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/60
Loss: 0.6495  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.88%, F1 Score: 0.8004
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/60
Loss: 0.5707  [0/3200]
Validation Loss: 0.0169, Accuracy: 79.50%, F1 Score: 0.7904
Early stopping triggered! No improvement in accuracy for 5 epochs.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 32 and patience 7...
Epoch 1/60
Loss: 2.9772  [0/3200]

<ipython-input-45-84119924d6e8>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

Validation Loss: 0.0428, Accuracy: 25.87%, F1 Score: 0.1176
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/60
Loss: 1.3745  [0/3200]
Validation Loss: 0.0399, Accuracy: 54.87%, F1 Score: 0.4732
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/60
Loss: 1.5195  [0/3200]
Validation Loss: 0.0358, Accuracy: 59.00%, F1 Score: 0.5253
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/60
Loss: 1.2554  [0/3200]
Validation Loss: 0.0306, Accuracy: 67.50%, F1 Score: 0.6712
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/60
Loss: 1.0440  [0/3200]
Validation Loss: 0.0270, Accuracy: 66.25%, F1 Score: 0.6559
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/60
Loss: 1.1116  [0/3200]
Validation Loss: 0.0250, Accuracy: 70.75%, F1 Score: 0.7082
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/60
Loss: 0.9562  [0/3200]
Validation Loss: 0.0252, Accuracy: 65.50%, F1 Score: 0.6146
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/60
Loss: 0.8322  [0/3200]
Validation Loss: 0.0223, Accuracy: 70.00%, F1 Score: 0.6750
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/60
Loss: 0.9166  [0/3200]
Validation Loss: 0.0212, Accuracy: 75.50%, F1 Score: 0.7453
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/60
Loss: 0.7715  [0/3200]
Validation Loss: 0.0211, Accuracy: 72.25%, F1 Score: 0.6980
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/60
Loss: 0.9843  [0/3200]
Validation Loss: 0.0198, Accuracy: 76.62%, F1 Score: 0.7564
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/60
Loss: 0.6400  [0/3200]
Validation Loss: 0.0191, Accuracy: 78.50%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/60
Loss: 0.7543  [0/3200]
Validation Loss: 0.0187, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/60
Loss: 0.5416  [0/3200]
Validation Loss: 0.0182, Accuracy: 77.88%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/60
Loss: 0.6312  [0/3200]
Validation Loss: 0.0177, Accuracy: 78.75%, F1 Score: 0.7857
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/60
Loss: 0.6728  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.00%, F1 Score: 0.7634
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/60
Loss: 0.4580  [0/3200]
Validation Loss: 0.0184, Accuracy: 75.62%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/60
Loss: 0.4547  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.25%, F1 Score: 0.7934
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/60
Loss: 0.6547  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.88%, F1 Score: 0.7720
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/60
Loss: 0.6676  [0/3200]
Validation Loss: 0.0186, Accuracy: 74.50%, F1 Score: 0.7229
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/60
Loss: 0.7095  [0/3200]
Validation Loss: 0.0172, Accuracy: 80.00%, F1 Score: 0.7984
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/60
Loss: 0.7561  [0/3200]
Validation Loss: 0.0178, Accuracy: 78.50%, F1 Score: 0.7820
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/60
Loss: 0.7263  [0/3200]
Validation Loss: 0.0184, Accuracy: 77.00%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/60
Loss: 0.5672  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.12%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/60
Loss: 0.5990  [0/3200]
Validation Loss: 0.0172, Accuracy: 78.75%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/60
Loss: 0.6398  [0/3200]
Validation Loss: 0.0175, Accuracy: 78.25%, F1 Score: 0.7754
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/60
Loss: 0.6231  [0/3200]
Validation Loss: 0.0163, Accuracy: 79.88%, F1 Score: 0.7985
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/60
Loss: 0.6495  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.88%, F1 Score: 0.8004
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/60
Loss: 0.5707  [0/3200]
Validation Loss: 0.0169, Accuracy: 79.50%, F1 Score: 0.7904
Adjusting learning rate of group 0 to 8.1231e-05.
Epoch 30/60
Loss: 0.5551  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.75%, F1 Score: 0.8048
Adjusting learning rate of group 0 to 8.1317e-05.
Epoch 31/60
Loss: 0.7024  [0/3200]
Validation Loss: 0.0164, Accuracy: 80.75%, F1 Score: 0.8062
Adjusting learning rate of group 0 to 8.1406e-05.
Epoch 32/60
Loss: 0.3958  [0/3200]
Validation Loss: 0.0173, Accuracy: 78.75%, F1 Score: 0.7817
Adjusting learning rate of group 0 to 8.1499e-05.
Epoch 33/60
Loss: 0.5568  [0/3200]
Validation Loss: 0.0183, Accuracy: 77.50%, F1 Score: 0.7723
Adjusting learning rate of group 0 to 8.1594e-05.
Epoch 34/60
Loss: 0.4376  [0/3200]
Validation Loss: 0.0164, Accuracy: 80.62%, F1 Score: 0.8060
Adjusting learning rate of group 0 to 8.1692e-05.
Epoch 35/60
Loss: 0.4990  [0/3200]
Validation Loss: 0.0163, Accuracy: 80.75%, F1 Score: 0.8086
Adjusting learning rate of group 0 to 8.1793e-05.
Epoch 36/60
Loss: 0.4040  [0/3200]
Validation Loss: 0.0163, Accuracy: 80.25%, F1 Score: 0.8031
Adjusting learning rate of group 0 to 8.1896e-05.
Epoch 37/60
Loss: 0.5030  [0/3200]
Validation Loss: 0.0167, Accuracy: 80.50%, F1 Score: 0.8039
Early stopping triggered! No improvement in accuracy for 7 epochs.
Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 32 and patience 9...
Epoch 1/60
Loss: 2.9772  [0/3200]

<ipython-input-45-84119924d6e8>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

Validation Loss: 0.0428, Accuracy: 25.87%, F1 Score: 0.1176
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/60
Loss: 1.3745  [0/3200]
Validation Loss: 0.0399, Accuracy: 54.87%, F1 Score: 0.4732
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/60
Loss: 1.5195  [0/3200]
Validation Loss: 0.0358, Accuracy: 59.00%, F1 Score: 0.5253
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/60
Loss: 1.2554  [0/3200]
Validation Loss: 0.0306, Accuracy: 67.50%, F1 Score: 0.6712
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/60
Loss: 1.0440  [0/3200]
Validation Loss: 0.0270, Accuracy: 66.25%, F1 Score: 0.6559
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/60
Loss: 1.1116  [0/3200]
Validation Loss: 0.0250, Accuracy: 70.75%, F1 Score: 0.7082
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/60
Loss: 0.9562  [0/3200]
Validation Loss: 0.0252, Accuracy: 65.50%, F1 Score: 0.6146
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/60
Loss: 0.8322  [0/3200]
Validation Loss: 0.0223, Accuracy: 70.00%, F1 Score: 0.6750
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/60
Loss: 0.9166  [0/3200]
Validation Loss: 0.0212, Accuracy: 75.50%, F1 Score: 0.7453
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/60
Loss: 0.7715  [0/3200]
Validation Loss: 0.0211, Accuracy: 72.25%, F1 Score: 0.6980
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/60
Loss: 0.9843  [0/3200]
Validation Loss: 0.0198, Accuracy: 76.62%, F1 Score: 0.7564
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/60
Loss: 0.6400  [0/3200]
Validation Loss: 0.0191, Accuracy: 78.50%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/60
Loss: 0.7543  [0/3200]
Validation Loss: 0.0187, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/60
Loss: 0.5416  [0/3200]
Validation Loss: 0.0182, Accuracy: 77.88%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/60
Loss: 0.6312  [0/3200]
Validation Loss: 0.0177, Accuracy: 78.75%, F1 Score: 0.7857
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/60
Loss: 0.6728  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.00%, F1 Score: 0.7634
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/60
Loss: 0.4580  [0/3200]
Validation Loss: 0.0184, Accuracy: 75.62%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/60
Loss: 0.4547  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.25%, F1 Score: 0.7934
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/60
Loss: 0.6547  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.88%, F1 Score: 0.7720
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/60
Loss: 0.6676  [0/3200]
Validation Loss: 0.0186, Accuracy: 74.50%, F1 Score: 0.7229
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/60
Loss: 0.7095  [0/3200]
Validation Loss: 0.0172, Accuracy: 80.00%, F1 Score: 0.7984
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/60
Loss: 0.7561  [0/3200]
Validation Loss: 0.0178, Accuracy: 78.50%, F1 Score: 0.7820
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/60
Loss: 0.7263  [0/3200]
Validation Loss: 0.0184, Accuracy: 77.00%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/60
Loss: 0.5672  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.12%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/60
Loss: 0.5990  [0/3200]
Validation Loss: 0.0172, Accuracy: 78.75%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/60
Loss: 0.6398  [0/3200]
Validation Loss: 0.0175, Accuracy: 78.25%, F1 Score: 0.7754
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/60
Loss: 0.6231  [0/3200]
Validation Loss: 0.0163, Accuracy: 79.88%, F1 Score: 0.7985
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/60
Loss: 0.6495  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.88%, F1 Score: 0.8004
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/60
Loss: 0.5707  [0/3200]
Validation Loss: 0.0169, Accuracy: 79.50%, F1 Score: 0.7904
Adjusting learning rate of group 0 to 8.1231e-05.
Epoch 30/60
Loss: 0.5551  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.75%, F1 Score: 0.8048
Adjusting learning rate of group 0 to 8.1317e-05.
Epoch 31/60
Loss: 0.7024  [0/3200]
Validation Loss: 0.0164, Accuracy: 80.75%, F1 Score: 0.8062
Adjusting learning rate of group 0 to 8.1406e-05.
Epoch 32/60
Loss: 0.3958  [0/3200]
Validation Loss: 0.0173, Accuracy: 78.75%, F1 Score: 0.7817
Adjusting learning rate of group 0 to 8.1499e-05.
Epoch 33/60
Loss: 0.5568  [0/3200]
Validation Loss: 0.0183, Accuracy: 77.50%, F1 Score: 0.7723
Adjusting learning rate of group 0 to 8.1594e-05.
Epoch 34/60
Loss: 0.4376  [0/3200]
Validation Loss: 0.0164, Accuracy: 80.62%, F1 Score: 0.8060
Adjusting learning rate of group 0 to 8.1692e-05.
Epoch 35/60
Loss: 0.4990  [0/3200]
Validation Loss: 0.0163, Accuracy: 80.75%, F1 Score: 0.8086
Adjusting learning rate of group 0 to 8.1793e-05.
Epoch 36/60
Loss: 0.4040  [0/3200]
Validation Loss: 0.0163, Accuracy: 80.25%, F1 Score: 0.8031
Adjusting learning rate of group 0 to 8.1896e-05.
Epoch 37/60
Loss: 0.5030  [0/3200]
Validation Loss: 0.0167, Accuracy: 80.50%, F1 Score: 0.8039
Adjusting learning rate of group 0 to 8.2003e-05.
Epoch 38/60
Loss: 0.5003  [0/3200]
Validation Loss: 0.0169, Accuracy: 79.12%, F1 Score: 0.7892
Adjusting learning rate of group 0 to 8.2113e-05.
Epoch 39/60
Loss: 0.6461  [0/3200]
Validation Loss: 0.0194, Accuracy: 77.12%, F1 Score: 0.7636
Early stopping triggered! No improvement in accuracy for 9 epochs.

<ipython-input-45-84119924d6e8>:78: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  results_df = results_df.append({'Patience': patience,

results_df

Παρατηρούμε οτι το patience μπορεί επίσης να διαδραματήσει μεγάλο ρόλο (μικρότερο αναλογικά με το batch size) στους χρόνους εκτέλεσης οπου με τον τριπλασιασμο του patience παρατησούμε σχεδόν διπλασιασμό του χρόνου. Ωστόσο παρόλο που δεν παρατηρείται τεράστια διακύμανση στην απόδοση προφανώς για τα συγκεκριμένα δεδομένα αξίζει το tradeoff για να κερδίσουμε 3% στις μετρικές του accuracy & F1.

Storing the models

Fully optimized model
import time

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define the data and loaders with the specified batch size
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

# Initialize the model
model = CNN_pp_re(output_dim=4, dropout_rate=0.5).to(device)
model.apply(weights_init)  # Initialize the weights

optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0)

# Create the scheduler based on the scheduler name
if scheduler_name == 'OneCycleLR':
    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader), verbose=True)

# Early stopping parameters
early_stop_counter = 0
best_accuracy = 0.0
best_f1 = 0.0
early_stopping_epoch = None
early_stopping_accuracy = None
early_stopping_f1 = None

# Train the model
print(f"Training with batch size {batch_size} and patience 5...")
start_time = time.time()
for epoch in range(17):  # Stop training at epoch 17
    print(f"Epoch {epoch+1}/{epochs}")
    train_loop(train_loader, model, loss_fn, optimizer)
    test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
    print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

    # Check if the accuracy has improved
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_f1 = f1
        early_stop_counter = 0  # Reset the counter
    else:
        early_stop_counter += 1

    # Check if early stopping condition is met
    if early_stop_counter >= 5:
        print("Early stopping triggered! No improvement in accuracy for 5 epochs.")
        early_stopping_epoch = epoch + 1
        early_stopping_accuracy = accuracy
        early_stopping_f1 = f1
        break

    scheduler.step()  # Update the learning rate

end_time = time.time()
elapsed_time = end_time - start_time

# Create the final model
CNN_pp_re_final = model

# Save the final model
torch.save(CNN_pp_re_final, "/content/drive/MyDrive/Ν124/models/CNN_pp_re_final.pth")

# Print the results
print(f"\nTraining completed!\n")
print(f"Epoch at Early Stopping: {early_stopping_epoch}")
print(f"Early Stopping Accuracy: {early_stopping_accuracy:.2f}%")
print(f"Early Stopping F1 Score: {early_stopping_f1:.4f}")
print(f"Best Accuracy: {best_accuracy:.2f}%")
print(f"Best F1 Score: {best_f1:.4f}")
print(f"Total Time (s): {elapsed_time:.2f}")

Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 16 and patience 5...
Epoch 1/60
Loss: 1.4068  [0/3200]
Loss: 1.0340  [1600/3200]
Validation Loss: 0.0473, Accuracy: 67.38%, F1 Score: 0.6587
Adjusting learning rate of group 0 to 8.0000e-05.
Epoch 2/60
Loss: 1.0309  [0/3200]
Loss: 0.9789  [1600/3200]
Validation Loss: 0.0446, Accuracy: 69.38%, F1 Score: 0.6808
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 3/60
Loss: 0.7666  [0/3200]
Loss: 0.5071  [1600/3200]
Validation Loss: 0.0384, Accuracy: 76.00%, F1 Score: 0.7459
Adjusting learning rate of group 0 to 8.0003e-05.
Epoch 4/60
Loss: 0.3333  [0/3200]
Loss: 0.2567  [1600/3200]
Validation Loss: 0.0360, Accuracy: 79.12%, F1 Score: 0.7888
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 5/60
Loss: 0.8734  [0/3200]
Loss: 0.6187  [1600/3200]
Validation Loss: 0.0351, Accuracy: 78.12%, F1 Score: 0.7809
Adjusting learning rate of group 0 to 8.0009e-05.
Epoch 6/60
Loss: 0.7106  [0/3200]
Loss: 0.3061  [1600/3200]
Validation Loss: 0.0371, Accuracy: 76.62%, F1 Score: 0.7707
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 7/60
Loss: 0.5991  [0/3200]
Loss: 0.3975  [1600/3200]
Validation Loss: 0.0363, Accuracy: 76.12%, F1 Score: 0.7522
Adjusting learning rate of group 0 to 8.0018e-05.
Epoch 8/60
Loss: 0.3038  [0/3200]
Loss: 0.3593  [1600/3200]
Validation Loss: 0.0350, Accuracy: 78.12%, F1 Score: 0.7732
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 9/60
Loss: 0.5215  [0/3200]
Loss: 0.3155  [1600/3200]
Validation Loss: 0.0339, Accuracy: 80.00%, F1 Score: 0.7938
Adjusting learning rate of group 0 to 8.0030e-05.
Epoch 10/60
Loss: 0.2824  [0/3200]
Loss: 0.2885  [1600/3200]
Validation Loss: 0.0335, Accuracy: 78.75%, F1 Score: 0.7850
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 11/60
Loss: 0.3508  [0/3200]
Loss: 0.1472  [1600/3200]
Validation Loss: 0.0332, Accuracy: 80.50%, F1 Score: 0.8008
Adjusting learning rate of group 0 to 8.0044e-05.
Epoch 12/60
Loss: 0.2190  [0/3200]
Loss: 0.2166  [1600/3200]
Validation Loss: 0.0351, Accuracy: 79.62%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 13/60
Loss: 0.3937  [0/3200]
Loss: 0.1673  [1600/3200]
Validation Loss: 0.0365, Accuracy: 79.12%, F1 Score: 0.7835
Adjusting learning rate of group 0 to 8.0062e-05.
Epoch 14/60
Loss: 0.2727  [0/3200]
Loss: 0.2061  [1600/3200]
Validation Loss: 0.0355, Accuracy: 79.38%, F1 Score: 0.7893
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 15/60
Loss: 0.3493  [0/3200]
Loss: 0.3084  [1600/3200]
Validation Loss: 0.0353, Accuracy: 79.12%, F1 Score: 0.7880
Adjusting learning rate of group 0 to 8.0082e-05.
Epoch 16/60
Loss: 0.2409  [0/3200]
Loss: 0.3674  [1600/3200]
Validation Loss: 0.0375, Accuracy: 78.88%, F1 Score: 0.7853
Early stopping triggered! No improvement in accuracy for 5 epochs.

Training completed!

Epoch at Early Stopping: 16
Early Stopping Accuracy: 78.88%
Early Stopping F1 Score: 0.7853
Best Accuracy: 80.50%
Best F1 Score: 0.8008
Total Time (s): 19.58

Optimal model
import time

# Set the random seeds for reproducibility
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)
SEED = 12345
# Define the data and loaders with the specified batch size
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

# Initialize the model
model = CNN_pp2(output_dim=4, activation=nn.Mish(), dropout_rate=0.5).to(device)
model.apply(weights_init)  # Initialize the weights

optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0)

# Create the scheduler based on the scheduler name
if scheduler_name == 'OneCycleLR':
    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader), verbose=True)

# Early stopping parameters
early_stop_counter = 0
best_accuracy = 0.0
best_f1 = 0.0
early_stopping_epoch = None
early_stopping_accuracy = None
early_stopping_f1 = None

# Train the model
print(f"Training with batch size {batch_size} and patience 9...")
start_time = time.time()
for epoch in range(30):  # Stop training at epoch 30
    print(f"Epoch {epoch+1}/{epochs}")
    train_loop(train_loader, model, loss_fn, optimizer)
    test_loss, accuracy, f1 = test_loop(val_loader, model, loss_fn)
    print(f"Validation Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}")

    # Check if the accuracy has improved
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_f1 = f1
        early_stop_counter = 0  # Reset the counter
    else:
        early_stop_counter += 1

    # Check if early stopping condition is met
    if early_stop_counter >= 9:
        print("Early stopping triggered! No improvement in accuracy for 9 epochs.")
        early_stopping_epoch = epoch + 1
        early_stopping_accuracy = accuracy
        early_stopping_f1 = f1
        break

    scheduler.step()  # Update the learning rate

end_time = time.time()
elapsed_time = end_time - start_time

# Create the final model
CNN_pp2_final = model

# Save the final model
torch.save(CNN_pp2_final, "/content/drive/MyDrive/Ν124/models/CNN_pp_final.pth")

# Print the results
print(f"\nTraining completed!\n")
print(f"Epoch at Early Stopping: {early_stopping_epoch}")
print(f"Early Stopping Accuracy: {early_stopping_accuracy:.2f}%")
print(f"Early Stopping F1 Score: {early_stopping_f1:.4f}")
print(f"Best Accuracy: {best_accuracy:.2f}%")
print(f"Best F1 Score: {best_f1:.4f}")
print(f"Total Time (s): {elapsed_time:.2f}")

Adjusting learning rate of group 0 to 8.0000e-05.
Training with batch size 32 and patience 9...
Epoch 1/60
Loss: 2.9772  [0/3200]
Validation Loss: 0.0428, Accuracy: 25.87%, F1 Score: 0.1176
Adjusting learning rate of group 0 to 8.0001e-05.
Epoch 2/60
Loss: 1.3745  [0/3200]
Validation Loss: 0.0399, Accuracy: 54.87%, F1 Score: 0.4732
Adjusting learning rate of group 0 to 8.0006e-05.
Epoch 3/60
Loss: 1.5195  [0/3200]
Validation Loss: 0.0358, Accuracy: 59.00%, F1 Score: 0.5253
Adjusting learning rate of group 0 to 8.0013e-05.
Epoch 4/60
Loss: 1.2554  [0/3200]
Validation Loss: 0.0306, Accuracy: 67.50%, F1 Score: 0.6712
Adjusting learning rate of group 0 to 8.0023e-05.
Epoch 5/60
Loss: 1.0440  [0/3200]
Validation Loss: 0.0270, Accuracy: 66.25%, F1 Score: 0.6559
Adjusting learning rate of group 0 to 8.0037e-05.
Epoch 6/60
Loss: 1.1116  [0/3200]
Validation Loss: 0.0250, Accuracy: 70.75%, F1 Score: 0.7082
Adjusting learning rate of group 0 to 8.0053e-05.
Epoch 7/60
Loss: 0.9562  [0/3200]
Validation Loss: 0.0252, Accuracy: 65.50%, F1 Score: 0.6146
Adjusting learning rate of group 0 to 8.0072e-05.
Epoch 8/60
Loss: 0.8322  [0/3200]
Validation Loss: 0.0223, Accuracy: 70.00%, F1 Score: 0.6750
Adjusting learning rate of group 0 to 8.0094e-05.
Epoch 9/60
Loss: 0.9166  [0/3200]
Validation Loss: 0.0212, Accuracy: 75.50%, F1 Score: 0.7453
Adjusting learning rate of group 0 to 8.0119e-05.
Epoch 10/60
Loss: 0.7715  [0/3200]
Validation Loss: 0.0211, Accuracy: 72.25%, F1 Score: 0.6980
Adjusting learning rate of group 0 to 8.0146e-05.
Epoch 11/60
Loss: 0.9843  [0/3200]
Validation Loss: 0.0198, Accuracy: 76.62%, F1 Score: 0.7564
Adjusting learning rate of group 0 to 8.0177e-05.
Epoch 12/60
Loss: 0.6400  [0/3200]
Validation Loss: 0.0191, Accuracy: 78.50%, F1 Score: 0.7803
Adjusting learning rate of group 0 to 8.0211e-05.
Epoch 13/60
Loss: 0.7543  [0/3200]
Validation Loss: 0.0187, Accuracy: 77.88%, F1 Score: 0.7770
Adjusting learning rate of group 0 to 8.0247e-05.
Epoch 14/60
Loss: 0.5416  [0/3200]
Validation Loss: 0.0182, Accuracy: 77.88%, F1 Score: 0.7772
Adjusting learning rate of group 0 to 8.0287e-05.
Epoch 15/60
Loss: 0.6312  [0/3200]
Validation Loss: 0.0177, Accuracy: 78.75%, F1 Score: 0.7857
Adjusting learning rate of group 0 to 8.0329e-05.
Epoch 16/60
Loss: 0.6728  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.00%, F1 Score: 0.7634
Adjusting learning rate of group 0 to 8.0375e-05.
Epoch 17/60
Loss: 0.4580  [0/3200]
Validation Loss: 0.0184, Accuracy: 75.62%, F1 Score: 0.7482
Adjusting learning rate of group 0 to 8.0423e-05.
Epoch 18/60
Loss: 0.4547  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.25%, F1 Score: 0.7934
Adjusting learning rate of group 0 to 8.0474e-05.
Epoch 19/60
Loss: 0.6547  [0/3200]
Validation Loss: 0.0176, Accuracy: 77.88%, F1 Score: 0.7720
Adjusting learning rate of group 0 to 8.0528e-05.
Epoch 20/60
Loss: 0.6676  [0/3200]
Validation Loss: 0.0186, Accuracy: 74.50%, F1 Score: 0.7229
Adjusting learning rate of group 0 to 8.0585e-05.
Epoch 21/60
Loss: 0.7095  [0/3200]
Validation Loss: 0.0172, Accuracy: 80.00%, F1 Score: 0.7984
Adjusting learning rate of group 0 to 8.0645e-05.
Epoch 22/60
Loss: 0.7561  [0/3200]
Validation Loss: 0.0178, Accuracy: 78.50%, F1 Score: 0.7820
Adjusting learning rate of group 0 to 8.0708e-05.
Epoch 23/60
Loss: 0.7263  [0/3200]
Validation Loss: 0.0184, Accuracy: 77.00%, F1 Score: 0.7708
Adjusting learning rate of group 0 to 8.0774e-05.
Epoch 24/60
Loss: 0.5672  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.12%, F1 Score: 0.8015
Adjusting learning rate of group 0 to 8.0843e-05.
Epoch 25/60
Loss: 0.5990  [0/3200]
Validation Loss: 0.0172, Accuracy: 78.75%, F1 Score: 0.7867
Adjusting learning rate of group 0 to 8.0915e-05.
Epoch 26/60
Loss: 0.6398  [0/3200]
Validation Loss: 0.0175, Accuracy: 78.25%, F1 Score: 0.7754
Adjusting learning rate of group 0 to 8.0989e-05.
Epoch 27/60
Loss: 0.6231  [0/3200]
Validation Loss: 0.0163, Accuracy: 79.88%, F1 Score: 0.7985
Adjusting learning rate of group 0 to 8.1067e-05.
Epoch 28/60
Loss: 0.6495  [0/3200]
Validation Loss: 0.0173, Accuracy: 79.88%, F1 Score: 0.8004
Adjusting learning rate of group 0 to 8.1147e-05.
Epoch 29/60
Loss: 0.5707  [0/3200]
Validation Loss: 0.0169, Accuracy: 79.50%, F1 Score: 0.7904
Adjusting learning rate of group 0 to 8.1231e-05.
Epoch 30/60
Loss: 0.5551  [0/3200]
Validation Loss: 0.0166, Accuracy: 80.75%, F1 Score: 0.8048
Adjusting learning rate of group 0 to 8.1317e-05.

Training completed!

Epoch at Early Stopping: None

---------------------------------------------------------------------------
TypeError Traceback (most recent call last)
<ipython-input-48-96e811cef8ad> in <cell line: 71>() 69 print(f"\nTraining completed!\n") 70 print(f"Epoch at Early Stopping: {early_stopping_epoch}") ---> 71 print(f"Early Stopping Accuracy: {early_stopping_accuracy:.2f}%") 72 print(f"Early Stopping F1 Score: {early_stopping_f1:.4f}") 73 print(f"Best Accuracy: {best_accuracy:.2f}%")
TypeError: unsupported format string passed to NoneType.__format__
Ερώτημα 4: Testing
Με την αξιολόγηση του ταξινομητή στο test set προσπαθούμε να αποκτήσουμε μια εικόνα της ικανότητας γενίκευσής του σε δεδομένα που δεν έχει χρησιμοποιήσει κατά την εκπαίδευση. Για να δούμε πόσο κοντά είναι αυτή η εικόνα στην πραγματικότητα, θα παράξουμε προβλέψεις για δεδομένα από τον πραγματικό κόσμο (youtube videos).

Βήμα 1: Inference

Εδώ θα χρειαστεί να φτιαχτεί μία συνάρτηση που θα παίρνει ένα σύνολο δεδομένων (dataloader με shuffle=False) και ένα εκπαιδευμένο Συνελικτικό Δίκτυο και θα επιστρέφει μία λίστα με τις προβλέψεις του μοντέλου

# Set the random seeds for reproducibility
SEED = 12345
torch.manual_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define the function for predicting genres
def predict_genre(data_loader, model):
    predictions = []
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for inputs in data_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            predictions.extend(predicted.tolist())
    return predictions
Βήμα 2: Κατέβασμα μουσικής

Εγκαταστήστε το youtube-dl στο colab χρησιμοποιώντας την παρακάτω ακολουθία εντολών:

!sudo apt-get update

!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl

!sudo chmod a+rx /usr/local/bin/youtube-dl

Το αρχείο youtube.py που σας έχει δοθεί μαζί με τα δεδομένα περιέχει συναρτήσεις οι οποίες, δεδομένου ενός youtube url, κατεβάζουν τον ήχο και υπολογίζουν μία ακολουθία από mel spectrograms (ένα για κάθε δευτερόλεπτο). Η συνάρτηση youtube_to_melgram αποθηκεύει στο αρχείο melgrams.npy την ακολουθία melgram ενός δεδομένου url.

Χρησιμοποιήστε την με τουλάχιστον 1 url απο κάθε μουσικό είδος που περιέχεται στο σύνολο δεδομένων μας. Είστε ελεύθεροι να χρησιμοποιήσετε όποιο url θέλετε. Σας δίνονται ενδεικτικά:

• κλασσική μουσική:https://www.youtube.com/watch?v=9E6b3swbnWg

• ποπ: https://www.youtube.com/watch?v=EDwb9jOVRtU

• ροκ: https://www.youtube.com/watch?v=OMaycNcPsHI

• μπλουζ: https://www.youtube.com/watch?v=l45f28PzfCI

!sudo apt-get update
!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
!sudo chmod a+rx /usr/local/bin/youtube-dl
!sudo pip install --upgrade --force-reinstall "git+https://github.com/ytdl-org/youtube-dl.git"

0% [Working]
            
Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease

0% [Connecting to security.ubuntu.com (91.189.91.39)] [Connected to cloud.r-pro
                                                                               
Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]

                                                                               
Get:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]

                                                                               
Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]

0% [2 InRelease 15.6 kB/114 kB 14%] [Connecting to security.ubuntu.com (91.189.
                                                                               
Hit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease

0% [2 InRelease 47.5 kB/114 kB 42%] [Connecting to security.ubuntu.com (91.189.
0% [Connecting to security.ubuntu.com (91.189.91.39)] [Waiting for headers] [Wa
                                                                               
Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease

                                                                               
0% [Waiting for headers] [Waiting for headers] [Waiting for headers]
                                                                    
Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]

0% [7 InRelease 6,947 B/108 kB 6%] [Waiting for headers] [Connecting to ppa.lau
                                                                               
Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]

0% [7 InRelease 28.7 kB/108 kB 26%] [Waiting for headers] [Connecting to ppa.la
0% [7 InRelease 28.7 kB/108 kB 26%] [Waiting for headers] [Connecting to ppa.la
                                                                               
0% [Waiting for headers] [Waiting for headers]
                                              
Get:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease [24.3 kB]
Hit:10 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease
Get:11 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,604 kB]
Get:13 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [81.0 kB]
Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,229 kB]
Get:15 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,664 kB]
Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,369 kB]
Get:17 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [40.3 kB]
Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,342 kB]
Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,079 kB]
Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal/main amd64 Packages [44.1 kB]
Get:21 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,858 kB]
Get:22 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]
Get:23 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,482 kB]
Get:24 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,069 kB]
Fetched 19.3 MB in 2s (9,364 kB/s)
Reading package lists... Done
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     3  100     3    0     0     29      0 --:--:-- --:--:-- --:--:--    29
100     3  100     3    0     0     26      0 --:--:-- --:--:-- --:--:--    26
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 1794k  100 1794k    0     0  3810k      0 --:--:-- --:--:-- --:--:-- 3810k
Collecting git+https://github.com/ytdl-org/youtube-dl.git
  Cloning https://github.com/ytdl-org/youtube-dl.git to /tmp/pip-req-build-nus0_0gp
  Running command git clone --filter=blob:none --quiet https://github.com/ytdl-org/youtube-dl.git /tmp/pip-req-build-nus0_0gp
  Resolved https://github.com/ytdl-org/youtube-dl.git to commit fa7f0effbe4e14fcf70e1dc4496371c9862b64b9
  Preparing metadata (setup.py) ... ?25l?25hdone
Building wheels for collected packages: youtube-dl
  Building wheel for youtube-dl (setup.py) ... ?25l?25hdone
  Created wheel for youtube-dl: filename=youtube_dl-2021.12.17-py2.py3-none-any.whl size=1922071 sha256=cdec8abb1df31aba1013f806047478bb1d7d3bfcd967474f9b30c3a4adca4ca2
  Stored in directory: /tmp/pip-ephem-wheel-cache-dkw71b5c/wheels/64/8e/40/866e846163e3e4859bbe820ff8847ec97f4320864f1525aa9a
Successfully built youtube-dl
Installing collected packages: youtube-dl
Successfully installed youtube-dl-2021.12.17

import os
import sys

print(os.path.abspath(os.curdir))
os.chdir("/content/drive/MyDrive/Ν124/")
print(os.path.abspath(os.curdir))
/content
/content/drive/MyDrive/Ν124

import os
import librosa
import numpy as np

window_length = (50 * 1e-3)
hop_length = (50 * 1e-3)
mel_time_size = 21


def download_youtube(url):
    command = f'youtube-dl --extract-audio --audio-format wav --output temp.wav --postprocessor-args "-ar 8000" ' + url + " --quiet"
    os.system(command)


def load_wav(filename):
    """Rea audio file and return audio signal and sampling frequency"""
    if not os.path.exists(filename):
        raise FileNotFoundError
    # Load file using librosa
    x, fs = librosa.load(filename, sr=None)
    return x, fs


def melspectrogram(x=None, fs=None, n_fft=None, hop_length=None,
                   fuse=False):
    """Returns a mel spectrogram."""

    if x is None:
        return None
    # Set some values
    if n_fft is None:
        n_fft = int(window_length * fs)
    if hop_length is None:
        hop_length = int(hop_length * fs)
    # Get spectrogram
    spectrogram = librosa.feature.melspectrogram(y=x, sr=fs, n_fft=n_fft,
                                                 hop_length=hop_length)
    # Convert to MEL-Scale
    spectrogram_dB = librosa.power_to_db(spectrogram, ref=np.max)  # (n_mel,t)

    if fuse:
        chroma = librosa.feature.chroma_stft(y=x, sr=fs, n_fft=n_fft,
                                             hop_length=hop_length)
        chroma_dB = librosa.power_to_db(chroma)
        out = np.concatenate((spectrogram_dB.T, chroma_dB.T), axis=1)
    else:
        # Transpose to return (time,n_mel)
        out = spectrogram_dB.T
    return out


def get_melgrams(file):
    signal, fs = load_wav(file)

    segment_length = int((mel_time_size - 1) * window_length * fs)
    sequence_length = signal.shape[0]
    progress = 0
    segments = []
    while progress < sequence_length:
        if progress + segment_length > sequence_length:
            fill_data = sequence_length - progress
            empty_data = segment_length - fill_data
            feature = melspectrogram(
                np.pad(signal[progress:], (0, empty_data), 'constant'),
                fs=fs, n_fft=int(window_length * fs), hop_length=int(hop_length * fs))
            segments.append(feature)
        else:
            feature = melspectrogram(
                signal[progress:progress + segment_length],
                fs=fs, n_fft=int(window_length * fs), hop_length=int(hop_length * fs))

            segments.append(feature)
        progress += segment_length

    return segments


def youtube_to_melgram(url):
    download_youtube(url)
    melgrams = get_melgrams("temp.wav")
    np.save(f"youtube_melgrams_{label}.npy", melgrams)


batch_one_urls = [
  ("https://www.youtube.com/watch?v=9E6b3swbnWg", "classical"),
  ("https://www.youtube.com/watch?v=EDwb9jOVRtU", "pop"),
  ("https://www.youtube.com/watch?v=OMaycNcPsHI", "rock"),
  ("https://www.youtube.com/watch?v=l45f28PzfCI", "blues"),
]

batch_two_urls = [
  ("https://www.youtube.com/watch?v=WMLoBXgPil8", "classical"),
  ("https://www.youtube.com/watch?v=7-x3uD5z1bQ", "pop"),
  ("https://www.youtube.com/watch?v=hTWKbfoikeg", "rock"),
  ("https://www.youtube.com/watch?v=yDjNyK0KqmE", "blues")
]
for url, label in batch_one_urls:
    youtube_to_melgram(url)

# Load the saved melgrams and predict for each label
predictions_dict_model1 = {}
predictions_dict_model2 = {}

<ipython-input-96-f2799f214de8>:20: UserWarning: PySoundFile failed. Trying audioread instead.
  x, fs = librosa.load(filename, sr=None)
/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load
	Deprecated as of librosa version 0.10.0.
	It will be removed in librosa version 1.0.
  y, sr_native = __audioread_load(path, offset, duration, dtype)
<ipython-input-96-f2799f214de8>:20: UserWarning: PySoundFile failed. Trying audioread instead.
  x, fs = librosa.load(filename, sr=None)
/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load
	Deprecated as of librosa version 0.10.0.
	It will be removed in librosa version 1.0.
  y, sr_native = __audioread_load(path, offset, duration, dtype)
<ipython-input-96-f2799f214de8>:20: UserWarning: PySoundFile failed. Trying audioread instead.
  x, fs = librosa.load(filename, sr=None)
/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load
	Deprecated as of librosa version 0.10.0.
	It will be removed in librosa version 1.0.
  y, sr_native = __audioread_load(path, offset, duration, dtype)
<ipython-input-96-f2799f214de8>:20: UserWarning: PySoundFile failed. Trying audioread instead.
  x, fs = librosa.load(filename, sr=None)
/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load
	Deprecated as of librosa version 0.10.0.
	It will be removed in librosa version 1.0.
  y, sr_native = __audioread_load(path, offset, duration, dtype)

Βήμα 3: Προβλέψεις

Για κάθε ένα από τα μουσικά είδη χρησιμοποιήστε την συνάρτηση του βήματος 1 ώστε να παράξετε προβλέψεις. Τυπώστε ένα διάγραμμα όπου στον κατακόρυφο άξονα θα βρίσκονται οι μουσικές κλάσεις και στον οριζόντιο τα timestamps (που αντιστοιχούν σε δευτερόλεπτα). Βγάζουν τα αποτελέσματά σας νόημα αν τα συγκρίνετε με τον ήχο των youtube videos; Εποπτικά σχολιάστε πόσο κοντά είναι η απόδοση του ταξινομητή σας στα youtube videos (σε προβλέψεις τους ενός δευτερολέπτου) σε σχέση με την απόδοση στο test set.

# Load the trained models

model1 = CNN_pp_re(output_dim=4, dropout_rate=0.5)
model1.to(device)
model1 = torch.load("/content/drive/MyDrive/Ν124/models/CNN_pp_re_final.pth", map_location=device)

model2 = CNN_pp2(output_dim=4, activation=nn.Mish(), dropout_rate=0.5)
model2.to(device)
model2 = torch.load("/content/drive/MyDrive/Ν124/models/CNN_pp_final.pth", map_location=device)

def predict_genre(data_loader, model):
    predictions = []
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for batch in data_loader:
            inputs = batch[0].to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            predictions.extend(predicted.tolist())
    return predictions


predictions_dict_model1 = {}
predictions_dict_model2 = {}

for url, label in batch_one_urls:
    # Load the melgrams for the label
    melgrams = np.load(f"/content/drive/MyDrive/Ν124/youtube_melgrams_{label}.npy")

    # Create tensor for testing data
    x_test_tensor = torch.from_numpy(melgrams).unsqueeze(1).float()

    # Create a dataset and data loader
    test_dataset = TensorDataset(x_test_tensor)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

    # Classify the melgrams using the first model
    predictions_model1 = predict_genre(test_loader, model1)
    predictions_dict_model1[url] = predictions_model1

    # Classify the melgrams using the second model
    predictions_model2 = predict_genre(test_loader, model2)
    predictions_dict_model2[url] = predictions_model2
First Batch
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
encoded_genre_labels = label_encoder.fit_transform(y_tr)
genre_labels = label_encoder.classes_

for url, label in batch_one_urls:
    predictions_model1 = predictions_dict_model1[url]
    predictions_model2 = predictions_dict_model2[url]

    # Create a dictionary to store the predicted timestamps for each genre
    timestamps_dict_model1 = {encoded_genre: [] for encoded_genre in encoded_genre_labels}
    timestamps_dict_model2 = {encoded_genre: [] for encoded_genre in encoded_genre_labels}

    # Iterate over the predictions from model 1 and store the timestamps for each genre
    for i, prediction in enumerate(predictions_model1):
        timestamps_dict_model1[prediction].append(i)

    # Iterate over the predictions from model 2 and store the timestamps for each genre
    for i, prediction in enumerate(predictions_model2):
        timestamps_dict_model2[prediction].append(i)

    # Plot the genre predictions for the current label and models
    plt.figure(figsize=(12, 6))
    plt.subplot(121)
    for encoded_genre, timestamps in timestamps_dict_model1.items():
        genre = genre_labels[encoded_genre]
        plt.scatter(timestamps, [genre] * len(timestamps), label=genre)
    plt.xlabel('Timestamp (in seconds)')
    plt.ylabel('Genre')
    plt.title(f'Model 1: {label}')
    plt.legend()

    plt.subplot(122)
    for encoded_genre, timestamps in timestamps_dict_model2.items():
        genre = genre_labels[encoded_genre]
        plt.scatter(timestamps, [genre] * len(timestamps), label=genre)
    plt.xlabel('Timestamp (in seconds)')
    plt.ylabel('Genre')
    plt.title(f'Model 2: {label}')
    plt.legend()

    plt.tight_layout()
    plt.show()


# Print the predictions
print("Model 1 predictions:", predictions_dict_model1)
print("Model 2 predictions:", predictions_dict_model2)
Model 1 predictions: {'https://www.youtube.com/watch?v=9E6b3swbnWg': [1, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3], 'https://www.youtube.com/watch?v=EDwb9jOVRtU': [1, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3], 'https://www.youtube.com/watch?v=OMaycNcPsHI': [1, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3], 'https://www.youtube.com/watch?v=l45f28PzfCI': [1, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3]}
Model 2 predictions: {'https://www.youtube.com/watch?v=9E6b3swbnWg': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], 'https://www.youtube.com/watch?v=EDwb9jOVRtU': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], 'https://www.youtube.com/watch?v=OMaycNcPsHI': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], 'https://www.youtube.com/watch?v=l45f28PzfCI': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]}

Σημείωση: Επειδή αντιμετώπιζα προβλήματα με το colab καθώς δεν είμαι pro user με αποτέλεσμα να με πετάσει ανα χρόνικά διαστήματα, καθώς επίσης έκανα το λάθος να μην αποθηκεύω τα μοντέλα μου κάπου ακολούθησα την εξείς προσέγγιση: Επιδίωξα να έχω δύο μοντέλα τουλάχιστον, το μεν να είναι το βέλτιστο απο τη διαδικασία μέχρι στιγμής και το δεύτερο να ακολουθεί βήμα προς βήμα τη συνολική διαδικασία με την πιθανότητα να είναι εν τέλει το βέλτιστο με το συνδιασμό μεθόδων.

Ωστόσο είχα δυσκολίες καθώς το colab με πετούσε και έπρεπε να ξανατρέξω κάποια μοντέλα τα οποία μου έβγαζαν διαφορετικά αποτελέσματα, πχ διαφορετικό optimizer ή καλύτερη απόδοση σε άλλον sceduler.

Λόγω πεορισμένου χρόνου δεν μπορούσα να ξανατρέξω κάθε φορά τα μοντέλα μου για τις διαφορετικές παραμέτρους και διαφορετικά συμπεράσματα. Ελπίζω να με συγχωρέσετε που έχω κρατήσει αποτελέσματα των μοντέλων των συμπερασμάτων ενώ μπορέι να έχω τρέξει το μοντέλο σε διαφορετικό runtime και συνεπώς τα τρέχοντα αποτελέσματα απο πάνω να μην ταυτίζονται με τα results.

Προφανώς αυτό μπορεί να έχει επίπτωση και στο τελευταίο ερώτημα στην πρόβλεψη των test sets, ωστόσο λόγω χρόνου και πίεσης απο άλλα μαθήματα δεν κατάφερα να κάνω το βέλτιστο, καθώς όλα τα plots δείχνουν το ίδιο αποτέλεσμα με ίδια διάρκεια οπότε κάτι μπορεί να γίνεται με το plotting. Θα συνεχίσω ωστόσο να το δουλεύω και έπειτα απο τη παράδοση της εργασίας.

Ευχαριστώ για το χρόνο σας και για το υλικό που μας προσφέρατε μέσα στο εξάμηνο.

Καλό καλοκαίρι.
